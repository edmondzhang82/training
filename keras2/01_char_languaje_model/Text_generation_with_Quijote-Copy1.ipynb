{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a RNN model to text generation\n",
    "- RNN model at character level\n",
    "    - Input: n character previous\n",
    "    - Output: next character\n",
    "    - Model LSTM\n",
    "- Use 'El Quijote' to train the generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Header\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "print('Keras version: ', keras.__version__)\n",
    "\n",
    "# GPU devices visible by python\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "# Limit memory usage\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "path = '/home/ubuntu/data/training/keras/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and generate sequences\n",
    "\n",
    "Download quijote from guttenberg project\n",
    "\n",
    "wget http://www.gutenberg.org/cache/epub/2000/pg2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2117498\n",
      "Chars list:  ['\\n', ' ', '!', '\"', '#', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '«', '»', '¿', 'à', 'á', 'é', 'í', 'ï', 'ñ', 'ó', 'ù', 'ú', 'ü', '\\ufeff']\n",
      "total chars: 72\n"
     ]
    }
   ],
   "source": [
    "#Read book\n",
    "text = open(path + \"pg2000.txt\").read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('Chars list: ', chars)\n",
    "print('total chars:', len(chars))\n",
    "\n",
    "#Dictionaries to convert char to num & num to char\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplify text to improve the semantic capacities of the model.\n",
    "replace_dir={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 705726\n",
      "tregará a medea; si  - d\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "# One sentence of length 20 for each 3 characters\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(3000, len(text) - maxlen, step): #Start in character 3000 to exclude Gutenberg header.\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print(sentences[4996], '-', next_chars[4996])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "X shape:  (705726, 20, 72)\n",
      "y shape:  (705726, 72)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X: One row by sentence\n",
    "    in each row a matrix of bool 0/1 of dim length_sentence x num_chars coding the sentence. Dummy variables\n",
    "y: One row by sentence\n",
    "    in each row a vector of bool of lengt num_chars with 1 in the next char position\n",
    "'''\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('y shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prev (InputLayer)            (None, 20, 72)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 1024)          4493312   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1024)              8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 72)                73800     \n",
      "=================================================================\n",
      "Total params: 12,959,816.0\n",
      "Trainable params: 12,959,816.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: 2 stacked LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM\n",
    "from keras import optimizers\n",
    "\n",
    "print('Build model 1')\n",
    "seq_prev_input = Input(shape=(maxlen, len(chars)), name='prev') \n",
    "                \n",
    "# apply forwards LSTM\n",
    "forwards1 = LSTM(1024, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)(seq_prev_input)\n",
    "\n",
    "forwards2 = LSTM(1024, return_sequences=False, dropout=0.3, recurrent_dropout=0.3)(forwards1)\n",
    "\n",
    "output = Dense(len(chars), activation='softmax')(forwards2)\n",
    "\n",
    "model = Model(inputs=seq_prev_input, outputs=output)\n",
    "model.summary()\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "Nadam = optimizers.Nadam(lr=0.0002, schedule_decay=0.00005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Nadam, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 116.00 264.00\" width=\"116pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 112,-260 112,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139922444747440 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139922444747440</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 108,-255.5 108,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-233.8\">prev: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139926023581032 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139926023581032</title>\n",
       "<polygon fill=\"none\" points=\"5,-146.5 5,-182.5 103,-182.5 103,-146.5 5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-160.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 139922444747440&#45;&gt;139926023581032 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139922444747440-&gt;139926023581032</title>\n",
       "<path d=\"M54,-219.313C54,-211.289 54,-201.547 54,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"57.5001,-192.529 54,-182.529 50.5001,-192.529 57.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139926081984160 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139926081984160</title>\n",
       "<polygon fill=\"none\" points=\"5,-73.5 5,-109.5 103,-109.5 103,-73.5 5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-87.8\">lstm_2: LSTM</text>\n",
       "</g>\n",
       "<!-- 139926023581032&#45;&gt;139926081984160 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139926023581032-&gt;139926081984160</title>\n",
       "<path d=\"M54,-146.313C54,-138.289 54,-128.547 54,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"57.5001,-119.529 54,-109.529 50.5001,-119.529 57.5001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139922444748224 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139922444748224</title>\n",
       "<polygon fill=\"none\" points=\"3,-0.5 3,-36.5 105,-36.5 105,-0.5 3,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 139926081984160&#45;&gt;139922444748224 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139926081984160-&gt;139922444748224</title>\n",
       "<path d=\"M54,-73.3129C54,-65.2895 54,-55.5475 54,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"57.5001,-46.5288 54,-36.5288 50.5001,-46.5289 57.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot the model graph\n",
    "from IPython.display import SVG\n",
    "from keras.utils import vis_utils\n",
    "\n",
    "SVG(vis_utils.model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600000 samples, validate on 105726 samples\n",
      "Epoch 1/50\n",
      "600000/600000 [==============================] - 306s - loss: 2.2753 - acc: 0.3206 - val_loss: 1.9601 - val_acc: 0.4098\n",
      "Epoch 2/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.7904 - acc: 0.4468 - val_loss: 1.7279 - val_acc: 0.4898\n",
      "Epoch 3/50\n",
      "600000/600000 [==============================] - 307s - loss: 1.5911 - acc: 0.5049 - val_loss: 1.6220 - val_acc: 0.5230\n",
      "Epoch 4/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.4794 - acc: 0.5368 - val_loss: 1.5782 - val_acc: 0.5406\n",
      "Epoch 5/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.4100 - acc: 0.5563 - val_loss: 1.5412 - val_acc: 0.5512\n",
      "Epoch 6/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.3598 - acc: 0.5698 - val_loss: 1.5241 - val_acc: 0.5570\n",
      "Epoch 7/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.3214 - acc: 0.5795 - val_loss: 1.5053 - val_acc: 0.5627\n",
      "Epoch 8/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.2883 - acc: 0.5890 - val_loss: 1.4993 - val_acc: 0.5642\n",
      "Epoch 9/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.2618 - acc: 0.5955 - val_loss: 1.4897 - val_acc: 0.5701\n",
      "Epoch 10/50\n",
      "600000/600000 [==============================] - 305s - loss: 1.2379 - acc: 0.6020 - val_loss: 1.4956 - val_acc: 0.5719\n",
      "Epoch 11/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.2186 - acc: 0.6076 - val_loss: 1.4853 - val_acc: 0.5738\n",
      "Epoch 12/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.1996 - acc: 0.6124 - val_loss: 1.4857 - val_acc: 0.5753\n",
      "Epoch 13/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.1818 - acc: 0.6171 - val_loss: 1.4902 - val_acc: 0.5765\n",
      "Epoch 14/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.1669 - acc: 0.6215 - val_loss: 1.4925 - val_acc: 0.5760\n",
      "Epoch 15/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.1526 - acc: 0.6256 - val_loss: 1.4951 - val_acc: 0.5775\n",
      "Epoch 16/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.1399 - acc: 0.6288 - val_loss: 1.4990 - val_acc: 0.5769\n",
      "Epoch 17/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.1266 - acc: 0.6327 - val_loss: 1.5054 - val_acc: 0.5765\n",
      "Epoch 18/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.1155 - acc: 0.6353 - val_loss: 1.5042 - val_acc: 0.5775\n",
      "Epoch 19/50\n",
      "600000/600000 [==============================] - 305s - loss: 1.1035 - acc: 0.6387 - val_loss: 1.5046 - val_acc: 0.5783\n",
      "Epoch 20/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.0940 - acc: 0.6413 - val_loss: 1.5074 - val_acc: 0.5798\n",
      "Epoch 21/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.0850 - acc: 0.6439 - val_loss: 1.5181 - val_acc: 0.5768\n",
      "Epoch 22/50\n",
      "600000/600000 [==============================] - 306s - loss: 1.0745 - acc: 0.6470 - val_loss: 1.5288 - val_acc: 0.5781\n",
      "Epoch 23/50\n",
      "600000/600000 [==============================] - 305s - loss: 1.0661 - acc: 0.6492 - val_loss: 1.5295 - val_acc: 0.5783\n",
      "Epoch 24/50\n",
      "600000/600000 [==============================] - 305s - loss: 1.0572 - acc: 0.6521 - val_loss: 1.5346 - val_acc: 0.5788\n",
      "Epoch 25/50\n",
      "600000/600000 [==============================] - 305s - loss: 1.0470 - acc: 0.6549 - val_loss: 1.5403 - val_acc: 0.5786\n",
      "Epoch 26/50\n",
      "600000/600000 [==============================] - 305s - loss: 1.0400 - acc: 0.6567 - val_loss: 1.5483 - val_acc: 0.5767\n",
      "Epoch 27/50\n",
      "600000/600000 [==============================] - 305s - loss: 1.0334 - acc: 0.6583 - val_loss: 1.5547 - val_acc: 0.5759\n",
      "Epoch 28/50\n",
      "600000/600000 [==============================] - 305s - loss: 1.0251 - acc: 0.6611 - val_loss: 1.5649 - val_acc: 0.5756\n",
      "Epoch 29/50\n",
      "600000/600000 [==============================] - 308s - loss: 1.0180 - acc: 0.6628 - val_loss: 1.5707 - val_acc: 0.5748\n",
      "Epoch 30/50\n",
      "600000/600000 [==============================] - 312s - loss: 1.0122 - acc: 0.6643 - val_loss: 1.5702 - val_acc: 0.5765\n",
      "Epoch 31/50\n",
      "600000/600000 [==============================] - 313s - loss: 1.0061 - acc: 0.6665 - val_loss: 1.5767 - val_acc: 0.5751\n",
      "Epoch 32/50\n",
      "528896/600000 [=========================>....] - ETA: 34s - loss: 0.9948 - acc: 0.6703"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ca6efe5a3d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(X[:600000], y[:600000], batch_size=256, epochs=50,\n\u001b[0;32m----> 3\u001b[0;31m            validation_data=(X[600000:], y[600000:]))\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2075\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "history = model.fit(X[:600000], y[:600000], batch_size=256, epochs=50,\n",
    "           validation_data=(X[600000:], y[600000:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train on 600000 samples, validate on 132869 samples\n",
    "Epoch 1/30\n",
    "600000/600000 [==============================] - 523s - loss: 2.2971 - acc: 0.3154 - val_loss: 1.9305 - val_acc: 0.4111\n",
    "Epoch 2/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.8107 - acc: 0.4357 - val_loss: 1.7215 - val_acc: 0.4805\n",
    "Epoch 3/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.6351 - acc: 0.4875 - val_loss: 1.6226 - val_acc: 0.5123\n",
    "Epoch 4/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.5340 - acc: 0.5168 - val_loss: 1.5604 - val_acc: 0.5306\n",
    "Epoch 5/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4689 - acc: 0.5361 - val_loss: 1.5244 - val_acc: 0.5443\n",
    "Epoch 6/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4232 - acc: 0.5494 - val_loss: 1.5053 - val_acc: 0.5508\n",
    "Epoch 7/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3890 - acc: 0.5596 - val_loss: 1.4823 - val_acc: 0.5579\n",
    "Epoch 8/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3618 - acc: 0.5672 - val_loss: 1.4643 - val_acc: 0.5625\n",
    "Epoch 9/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3409 - acc: 0.5730 - val_loss: 1.4587 - val_acc: 0.5667\n",
    "Epoch 10/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.3231 - acc: 0.5784 - val_loss: 1.4466 - val_acc: 0.5690\n",
    "Epoch 11/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3084 - acc: 0.5819 - val_loss: 1.4398 - val_acc: 0.5736\n",
    "Epoch 12/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2954 - acc: 0.5864 - val_loss: 1.4360 - val_acc: 0.5740\n",
    "Epoch 13/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2839 - acc: 0.5885 - val_loss: 1.4296 - val_acc: 0.5776\n",
    "Epoch 14/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2751 - acc: 0.5913 - val_loss: 1.4275 - val_acc: 0.5785\n",
    "Epoch 15/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2645 - acc: 0.5944 - val_loss: 1.4230 - val_acc: 0.5794\n",
    "Epoch 16/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2578 - acc: 0.5963 - val_loss: 1.4182 - val_acc: 0.5816\n",
    "Epoch 17/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2508 - acc: 0.5988 - val_loss: 1.4202 - val_acc: 0.5818\n",
    "Epoch 18/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2455 - acc: 0.6005 - val_loss: 1.4151 - val_acc: 0.5812\n",
    "Epoch 19/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2394 - acc: 0.6017 - val_loss: 1.4133 - val_acc: 0.5836\n",
    "Epoch 20/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2332 - acc: 0.6037 - val_loss: 1.4099 - val_acc: 0.5848\n",
    "Epoch 21/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2288 - acc: 0.6051 - val_loss: 1.4164 - val_acc: 0.5837\n",
    "Epoch 22/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2247 - acc: 0.6057 - val_loss: 1.4102 - val_acc: 0.5861\n",
    "Epoch 23/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2215 - acc: 0.6067 - val_loss: 1.4080 - val_acc: 0.5873\n",
    "Epoch 24/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2167 - acc: 0.6078 - val_loss: 1.4099 - val_acc: 0.5855\n",
    "Epoch 25/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2124 - acc: 0.6097 - val_loss: 1.4050 - val_acc: 0.5876\n",
    "Epoch 26/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2111 - acc: 0.6101 - val_loss: 1.4105 - val_acc: 0.5868\n",
    "Epoch 27/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2059 - acc: 0.6116 - val_loss: 1.4027 - val_acc: 0.5879\n",
    "Epoch 28/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2026 - acc: 0.6122 - val_loss: 1.4048 - val_acc: 0.5868\n",
    "Epoch 29/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.1990 - acc: 0.6138 - val_loss: 1.4090 - val_acc: 0.5887\n",
    "Epoch 30/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.1969 - acc: 0.6139 - val_loss: 1.4053 - val_acc: 0.5889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAHVCAYAAADcnaM7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c80a7ceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "\n",
    "save_model(model, path + 'models/text_generation_model1024.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model1 = load_model(path + 'models/text_generation_model1024.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 20\n",
    "\n",
    "\n",
    "def sample(a, diversity=1.0):\n",
    "    '''\n",
    "    helper function to sample an index from a probability array\n",
    "    - Diversity control the level of randomless\n",
    "    '''\n",
    "    a = np.log(a) / diversity\n",
    "    a = np.exp(a) / np.sum(np.exp(a), axis=0)\n",
    "    a /= np.sum(a+0.0000001) #Precission error\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "\n",
    "def generate_text(sentence, diversity, current_model, num_char=400):\n",
    "    sentence_init = sentence\n",
    "    generated = ''\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = current_model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    print()\n",
    "    print('DIVERSITY: ',diversity)\n",
    "    print(sentence_init + generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/ipykernel/__main__.py:9: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIVERSITY:  0.5\n",
      "mire vuestra merced se ha de ser el voluntad, como tengo dicho, que ya soy tan industria que todas las manos, que el carro de la mancha, y que no la estaba hecho, y de despecho della, porque si no ha de dejar de contar los antiguos habían de ser fuerza y tuviera mala señora dulcinea, no hay poder dejar de ser los moros, y que no se ha de ser su escudero.\n",
      "\n",
      "-yo no pudo saber que no la hallaba con un capitán de su amo, \n"
     ]
    }
   ],
   "source": [
    "sentence = 'mire vuestra merced '\n",
    "generate_text(sentence, 0.5, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/ipykernel/__main__.py:9: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIVERSITY:  0.2\n",
      "mire vuestra merced que se le ofreció a su casa, y a las más de la caballería, y que es tan buena gala de la mancha, y que el cura y el barbero se le ofreció a la mesa, y que el caballero andante caballero andante caballero andante, que no la hallaba en la cabeza, y que se llamaba el de la caballería andante caballero andante.\n",
      "\n",
      "-eso no es que es menester ser en el mundo, y que el caballero de la mancha, con la cual l\n",
      "\n",
      "DIVERSITY:  0.5\n",
      "mire vuestra merced que fue mejor de la salud de la ingeniosa de la infanta, como se\n",
      "entretenía en el suelo, porque le falta a la del agravio que de su caballo andante caballero andante, y que no puede ver la cabeza, y todo el tiempo que no era posible que se le puso a la luna y estas y todas las cosas de la estada de las armas, y el caballero de la caballería, y ellos como el mesmo desdichado que el cura y la histor\n",
      "\n",
      "DIVERSITY:  1\n",
      "mire vuestra merced pude, preguntó\n",
      "lo que oyer todas las costumbres lo sin padecer, y así se muestra tal del careño, en\n",
      "el cual se su ternera volvió: a su nombre, y alote para ellos pensáis a cada paso,\n",
      "llevarse della, que ya soy caza, y luscinda deslía de\n",
      "la mujer quieres acomodarme para volver a este pastor no comenzado sirvieron cacido con ella y a la\n",
      "incletro de modo de pajecio\n",
      "de la mesma hija, hasta que se está\n",
      "\n",
      "DIVERSITY:  1.2\n",
      "mire vuestra merced se engaño y dar una noche, le mandó,\n",
      "cristiando le sucedió a mediodumbre que debe sabéis de ser que las ecesivas dellas, según él vuestra merced tuviere, y aun por ventura, como de libro, se representó con los\n",
      "zuntos; el libro apisábansele,\n",
      "   os, sino lo cual la he\n",
      "pasado una ligereza, que se\n",
      "vuesa merced si desdeneaza y recogieron los\n",
      "caballeros andantes, se volvió, a\n",
      "su buen gobierno de la acee\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = 'mire vuestra merced '\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "sentence = 'mi señora dulcinea '\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "sentence = 'el caballero andant'\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "mire vuestra merced decís, y que se le pareció que en la cabeza de la cabeza, y el caballero del caballero de la cabeza, y los demás de los demás de los demás de los demás de la mano de la mujer de la mano de la mano, y aun a su señora dulcinea del toboso, y el cura que el caballero de la mano que le pareció que estaba en la misma cosa que en la mitad del caballero de la mano, se le dijo:\n",
    "\n",
    "-no sé -respondi\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "mire vuestra merced que no está en la cabeza. pero, en efeto, pues todo aquello que está en el mundo que cada uno debía\n",
    "de ser con la misma sierra parte de la muerte de la mano, con todo el mundo me la conoció, que tenía por el primero que están el rostro en las manos, y al señor don quijote -respondió sancho-, porque en la puerta de los sucesos del buen estado de la desencantada de los ojos.\n",
    "\n",
    "-¿qué mal \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "mire vuestra merced que, aunque, desdicho y tiempo viene, a\n",
    "cuya cabeza destos tres, los informaciones que dejara de serle, que me fueron? ¿admirado,\n",
    "\n",
    "y, creyendo que me va y enfermo. y esto que tienen entonces las requiebros que algunos limpios en un ampeoso como si improvisentes en sus insimulables y en los míos al que el honesto, en la mano de mucho premio y don quijote fingióno los dos o sabidores, y,\n",
    "acom\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "mire vuestra merced paso que no sa cluero-. subió, señor,\n",
    "le hubiera vuelto el duque, que pica, yo fue pose�ría de guardar cierto. para los demás, esperando la misma tragua debe de haberlas hallado su santa hijo ni debían para mí, porque yo hay de platar pre subir tú turba -dijo el deleitable-; otras porturas, sino como alabanzas y\n",
    "comedimientos puciese\n",
    "los días de lo que von mirado después de\n",
    "visión de\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de lo que sucedió a la mano de la mano de la mano de los ojos de la mano de la mano de la mano, y aun más que se le dijese la mano de la mano y en la mitad del caballero del caballero de la mano de la mano, y que el parte de la mano, y que en la mitad del caballero de la mano, sino a la puerta de la mano de la mano de la mano de los demás de la cabeza, y el cura y la cabeza de la mano y en la cabeza de la cabeza, \n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de lo que sucedió a camila, por ser tan alta de la mano y en voz brazo? ¿qué es lo que pudiera, señor don quijote que en las fermosuras que después que le sacarán con don quijote había de ser la duquesa, la cual no le había de ser muy buena como gausa, y si este deseo de ser mejor que en la cabeza está a los dos de la mano, y su amo no le puede dar a su casa por los manos de mano, diciendo:\n",
    "\n",
    "-pues, ¿qué \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de lo que sucedió a las más faltas cuentan haciendo:\n",
    "\n",
    "-�qcorrían por esto, bueno -respondió cómo está aquel mano traía furia por ella estajo\n",
    "más que andaba y fingión de sus apartieron de modo que no\n",
    "hay vasto qué hizo con galdáis que soy sin duda, duque ni en la más\n",
    "buena gran señora dulcinea; y así lo han don quijote y no bastaba junto a nuestra\n",
    "señora dulcinea, ahora\n",
    "la entienda, el aposento a\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de lo que sucedió al\n",
    "admiración, dijo:\n",
    "\n",
    "-eso me esplevo en razón encantada, y su venimo tiempo, que pasaba tan hirtoria. es\n",
    "\n",
    "don quijote, le dijo:\n",
    "\n",
    "y cuando jamás:\n",
    "  si ya entienden en mi padre desde aquí vean fuerza, como yo\n",
    "costa yo he oído decir, el rey\n",
    "\n",
    "vención que ésto, que lo més comenzó a vuestra merced colgaré por la orden y parece que\n",
    "pudieron semplarse. el cual, si de la mono.\n",
    "acudi�\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de allí a poco comenzó a su caballero andante, que en la más hermosa de la mano, y el cura que el mundo tenía con el cura y el de la mano de la cabeza de la mano de la mano de la mano de la mano de la cabeza, y a los demás de los cuatro de la mano de la mano de la mano y en la mitad del caballero de la cabeza, y le dijo:\n",
    "\n",
    "-¡oh sancho -dijo el cura-, que no se le habían de ser manos de los demás de los de la\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de allí a poco comenzó a la vida de la mano de\n",
    "arriba al mundo. pero, con todo eso, ha de ser el rostro de las linajes de su escudero. por el rey en el mundo de la industria que en su caballero andante; y, aunque se volvió a camila y de la mano de la muerte de mi cabeza? y así, como el tal caballero andante, que los demás juramentos, y con la mitad del toboso, y así, por decir que os son de allí a los deseos \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de allí a poco comenzó\n",
    "a dos crazos de entender que tratar la nueva al desde nuevas andantes de solos con los detros donde me hubieran\n",
    "de subir la sumiera y rabia la duquesa\n",
    "   a un hacer con un gato, que le\n",
    "dijo juntar la locura para su\n",
    "amo, para que quisieron decir alguna, vino\n",
    "todas aquellos demás caballeros. pero, sancho, tanto, viendo camila la primero tiene: prodición\n",
    "que yo le dio caminar las belloz\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de allí a poco comer. otros días se le pidía hablar de cerra\n",
    "   que el gate predice\n",
    "y otras puntas del cordel bestia.\n",
    "\n",
    "-yo no por eso, decaría sobre la venta, porque él saslos demás sin él que\n",
    "llevase a buscar de la suma, sancho milático, cuando posían los zogados. y si así, mi rendido a\n",
    "cuerpo, ni en llopar salió el baece-, que el hábísimo que viene por\n",
    "los tratas y tontos que sean, y el de los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras2_py36]",
   "language": "python",
   "name": "conda-env-keras2_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
