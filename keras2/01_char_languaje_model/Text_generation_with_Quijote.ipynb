{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a RNN model to text generation\n",
    "- RNN model at character level\n",
    "    - Input: n character previous\n",
    "    - Output: next character\n",
    "    - Model LSTM\n",
    "- Use 'El Quijote' to train the generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Header\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "print('Keras version: ', keras.__version__)\n",
    "\n",
    "# GPU devices visible by python\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# Limit memory usage\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "path = '/home/ubuntu/data/training/keras/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and generate sequences\n",
    "\n",
    "Download quijote from guttenberg project\n",
    "\n",
    "wget http://www.gutenberg.org/cache/epub/2000/pg2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2117498\n",
      "Chars list:  ['\\n', ' ', '!', '\"', '#', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '«', '»', '¿', 'à', 'á', 'é', 'í', 'ï', 'ñ', 'ó', 'ù', 'ú', 'ü', '\\ufeff']\n",
      "total chars: 72\n"
     ]
    }
   ],
   "source": [
    "#Read book\n",
    "text = open(path + \"pg2000.txt\").read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('Chars list: ', chars)\n",
    "print('total chars:', len(chars))\n",
    "\n",
    "#Dictionaries to convert char to num & num to char\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', con los hechos del\\nemperador, compuestos por don luis de ávila, que, sin duda, debían de estar\\nentre los que quedaban; y quizá, si el cura los viera, no pasaran por tan\\nrigurosa sentencia.\\n\\ncuando llegaron a don quijote, ya él estaba levantado de la cama, y\\nproseguía en sus voces y en sus desatinos, dando cuchilladas y reveses a\\ntodas partes, estando tan despierto como si nunca hubiera dormido.\\nabrazáronse con él, y por fuerza le volvieron al lecho; y, después que hubo\\nsosegado un poco, volviéndose a hablar con el cura, le dijo:\\n\\n-por cierto, señor arzobispo turpín, que es gran mengua de los que nos\\nllamamos doce pares dejar, tan sin más ni más, llevar la vitoria deste\\ntorneo a los caballeros cortesanos, habiendo nosotros los aventureros\\nganado el prez en los tres días antecedentes.\\n\\n-calle vuestra merced, señor compadre -dijo el cura-, que dios será servido\\nque la suerte se mude, y que lo que hoy se pierde se gane mañana; y atienda\\nvuestra merced a su salud por agora, que me parece que debe de estar\\ndemasiadamente cansado, si ya no es que está malferido.\\n\\n-ferido no -dijo don quijote-, pero molido y quebrantado, no hay duda en\\nello; porque aquel bastardo de don roldán me ha molido a palos con el\\ntronco de una encina, y todo de envidia, porque ve que yo solo soy el\\nopuesto de sus valentías. mas no me llamaría yo reinaldos de montalbán si,\\nen levantándome deste lecho, no me lo pagare, a pesar de todos sus\\nencantamentos; y, por agora, tráiganme de yantar, que sé que es lo que más\\nme hará al caso, y quédese lo del vengarme a mi cargo.\\n\\nhiciéronlo ansí: diéronle de comer, y quedóse otra vez dormido, y ellos,\\nadmirados de su locura.\\n\\naquella noche quemó y abrasó el ama cuantos libros había en el corral y en\\ntoda la casa, y tales debieron de arder que merecían guardarse en perpetuos\\narchivos; mas no lo permitió su suerte y la pereza del escrutiñador; y así,\\nse cumplió el refrán en ellos de que pagan a las veces justos por\\npecadores.\\n\\nuno de los remedios que el cura y el barbero dieron, por entonces, para el\\nmal de su amigo, fue que le murasen y tapiasen el aposento de los libros,\\nporque cuando se levantase no los hallase -quizá quitando la causa, cesaría\\nel efeto-, y que dijesen que un encantador se los había llevado, y el\\naposento y todo; y así fue hecho con mucha presteza. de allí a dos días se\\nlevantó don quijote, y lo primero que hizo fue ir a\\n\\nver sus libros; y, como no hallaba el aposento donde le había dejado,\\nandaba de una en otra parte buscándole. llegaba adonde solía tener la\\npuerta, y tentábala con las manos, y volvía y revolvía los ojos por todo,\\nsin decir palabra; pero, al cabo de una buena pieza, preguntó a su ama que\\nhacia qué parte estaba el aposento de sus libros. el ama, que ya estaba\\nbien advertida de lo que había de responder, le dijo:\\n\\n-¿qué aposento, o qué nada, busca vuestra merced? ya no hay aposento ni\\nlibros en esta casa, porque todo se lo llevó el mesmo diablo.\\n\\n-no era diablo -replicó la sobrina-, sino un encantador que vi'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[100000:103000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 705726\n",
      "tregará a medea; si  - d\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "# One sentence of length 20 for each 3 characters\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(300, len(text) - maxlen, step): #Start in line 30 to exclude Gutenberg header.\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print(sentences[4996], '-', next_chars[4996])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "X shape:  (705726, 20, 72)\n",
      "y shape:  (705726, 72)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X: One row by sentence\n",
    "    in each row a matrix of bool 0/1 of dim length_sentence x num_chars coding the sentence. Dummy variables\n",
    "y: One row by sentence\n",
    "    in each row a vector of bool of lengt num_chars with 1 in the next char position\n",
    "'''\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('y shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prev (InputLayer)            (None, 20, 72)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 512)           1198080   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 72)                36936     \n",
      "=================================================================\n",
      "Total params: 3,334,216.0\n",
      "Trainable params: 3,334,216.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: 2 stacked LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM\n",
    "from keras import optimizers\n",
    "\n",
    "print('Build model 1')\n",
    "seq_prev_input = Input(shape=(maxlen, len(chars)), name='prev') \n",
    "                \n",
    "# apply forwards LSTM\n",
    "forwards1 = LSTM(512, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)(seq_prev_input)\n",
    "\n",
    "forwards2 = LSTM(512, return_sequences=False, dropout=0.3, recurrent_dropout=0.3)(forwards1)\n",
    "\n",
    "output = Dense(len(chars), activation='softmax')(forwards2)\n",
    "\n",
    "model = Model(inputs=seq_prev_input, outputs=output)\n",
    "model.summary()\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "Nadam = optimizers.Nadam(lr=0.0005, schedule_decay=0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Nadam, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 116.00 264.00\" width=\"116pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 112,-260 112,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139688086405920 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139688086405920</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 108,-255.5 108,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-233.8\">prev: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139691673714080 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139691673714080</title>\n",
       "<polygon fill=\"none\" points=\"5,-146.5 5,-182.5 103,-182.5 103,-146.5 5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-160.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 139688086405920&#45;&gt;139691673714080 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139688086405920-&gt;139691673714080</title>\n",
       "<path d=\"M54,-219.313C54,-211.289 54,-201.547 54,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"57.5001,-192.529 54,-182.529 50.5001,-192.529 57.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139691732121192 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139691732121192</title>\n",
       "<polygon fill=\"none\" points=\"5,-73.5 5,-109.5 103,-109.5 103,-73.5 5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-87.8\">lstm_2: LSTM</text>\n",
       "</g>\n",
       "<!-- 139691673714080&#45;&gt;139691732121192 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139691673714080-&gt;139691732121192</title>\n",
       "<path d=\"M54,-146.313C54,-138.289 54,-128.547 54,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"57.5001,-119.529 54,-109.529 50.5001,-119.529 57.5001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139688086406536 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139688086406536</title>\n",
       "<polygon fill=\"none\" points=\"3,-0.5 3,-36.5 105,-36.5 105,-0.5 3,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 139691732121192&#45;&gt;139688086406536 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139691732121192-&gt;139688086406536</title>\n",
       "<path d=\"M54,-73.3129C54,-65.2895 54,-55.5475 54,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"57.5001,-46.5288 54,-36.5288 50.5001,-46.5289 57.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot the model graph\n",
    "from IPython.display import SVG\n",
    "from keras.utils import vis_utils\n",
    "\n",
    "SVG(vis_utils.model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600000 samples, validate on 105726 samples\n",
      "Epoch 1/50\n",
      "600000/600000 [==============================] - 204s - loss: 2.3121 - acc: 0.3097 - val_loss: 2.0576 - val_acc: 0.3802\n",
      "Epoch 2/50\n",
      "600000/600000 [==============================] - 205s - loss: 1.9436 - acc: 0.4006 - val_loss: 1.8804 - val_acc: 0.4385\n",
      "Epoch 3/50\n",
      "600000/600000 [==============================] - 204s - loss: 1.7805 - acc: 0.4495 - val_loss: 1.7690 - val_acc: 0.4736\n",
      "Epoch 4/50\n",
      "600000/600000 [==============================] - 205s - loss: 1.6715 - acc: 0.4802 - val_loss: 1.6986 - val_acc: 0.4976\n",
      "Epoch 5/50\n",
      "600000/600000 [==============================] - 205s - loss: 1.5947 - acc: 0.5027 - val_loss: 1.6493 - val_acc: 0.5143\n",
      "Epoch 6/50\n",
      "600000/600000 [==============================] - 200s - loss: 1.5389 - acc: 0.5187 - val_loss: 1.6182 - val_acc: 0.5250\n",
      "Epoch 7/50\n",
      "600000/600000 [==============================] - 201s - loss: 1.4978 - acc: 0.5299 - val_loss: 1.5967 - val_acc: 0.5339\n",
      "Epoch 8/50\n",
      "600000/600000 [==============================] - 205s - loss: 1.4646 - acc: 0.5399 - val_loss: 1.5756 - val_acc: 0.5391\n",
      "Epoch 9/50\n",
      "600000/600000 [==============================] - 205s - loss: 1.4377 - acc: 0.5468 - val_loss: 1.5617 - val_acc: 0.5430\n",
      "Epoch 10/50\n",
      "600000/600000 [==============================] - 204s - loss: 1.4160 - acc: 0.5531 - val_loss: 1.5501 - val_acc: 0.5480\n",
      "Epoch 11/50\n",
      "600000/600000 [==============================] - 205s - loss: 1.3966 - acc: 0.5579 - val_loss: 1.5452 - val_acc: 0.5510\n",
      "Epoch 12/50\n",
      "600000/600000 [==============================] - 205s - loss: 1.3798 - acc: 0.5627 - val_loss: 1.5322 - val_acc: 0.5536\n",
      "Epoch 13/50\n",
      "600000/600000 [==============================] - 205s - loss: 1.3659 - acc: 0.5664 - val_loss: 1.5270 - val_acc: 0.5552\n",
      "Epoch 14/50\n",
      "600000/600000 [==============================] - 204s - loss: 1.3511 - acc: 0.5708 - val_loss: 1.5186 - val_acc: 0.5590\n",
      "Epoch 15/50\n",
      "600000/600000 [==============================] - 204s - loss: 1.3405 - acc: 0.5733 - val_loss: 1.5131 - val_acc: 0.5588\n",
      "Epoch 16/50\n",
      "600000/600000 [==============================] - 201s - loss: 1.3302 - acc: 0.5765 - val_loss: 1.5104 - val_acc: 0.5605\n",
      "Epoch 17/50\n",
      "600000/600000 [==============================] - 201s - loss: 1.3203 - acc: 0.5790 - val_loss: 1.5068 - val_acc: 0.5627\n",
      "Epoch 18/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.3121 - acc: 0.5807 - val_loss: 1.5018 - val_acc: 0.5640\n",
      "Epoch 19/50\n",
      "600000/600000 [==============================] - 206s - loss: 1.3027 - acc: 0.5831 - val_loss: 1.5000 - val_acc: 0.5653\n",
      "Epoch 20/50\n",
      "600000/600000 [==============================] - 206s - loss: 1.2954 - acc: 0.5856 - val_loss: 1.4953 - val_acc: 0.5655\n",
      "Epoch 21/50\n",
      "600000/600000 [==============================] - 207s - loss: 1.2886 - acc: 0.5871 - val_loss: 1.4956 - val_acc: 0.5670\n",
      "Epoch 22/50\n",
      "600000/600000 [==============================] - 206s - loss: 1.2823 - acc: 0.5885 - val_loss: 1.4962 - val_acc: 0.5672\n",
      "Epoch 23/50\n",
      "600000/600000 [==============================] - 206s - loss: 1.2760 - acc: 0.5899 - val_loss: 1.4923 - val_acc: 0.5688\n",
      "Epoch 24/50\n",
      "600000/600000 [==============================] - 204s - loss: 1.2694 - acc: 0.5926 - val_loss: 1.4878 - val_acc: 0.5698\n",
      "Epoch 25/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.2639 - acc: 0.5938 - val_loss: 1.4882 - val_acc: 0.5706\n",
      "Epoch 26/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.2599 - acc: 0.5947 - val_loss: 1.4855 - val_acc: 0.5727\n",
      "Epoch 27/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.2544 - acc: 0.5962 - val_loss: 1.4864 - val_acc: 0.5714\n",
      "Epoch 28/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.2497 - acc: 0.5977 - val_loss: 1.4857 - val_acc: 0.5721\n",
      "Epoch 29/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.2451 - acc: 0.5983 - val_loss: 1.4824 - val_acc: 0.5727\n",
      "Epoch 30/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.2415 - acc: 0.5995 - val_loss: 1.4830 - val_acc: 0.5736\n",
      "Epoch 31/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.2362 - acc: 0.6012 - val_loss: 1.4827 - val_acc: 0.5740\n",
      "Epoch 32/50\n",
      "600000/600000 [==============================] - 206s - loss: 1.2339 - acc: 0.6014 - val_loss: 1.4829 - val_acc: 0.5747\n",
      "Epoch 33/50\n",
      "600000/600000 [==============================] - 207s - loss: 1.2280 - acc: 0.6029 - val_loss: 1.4786 - val_acc: 0.5742\n",
      "Epoch 34/50\n",
      "600000/600000 [==============================] - 206s - loss: 1.2250 - acc: 0.6044 - val_loss: 1.4792 - val_acc: 0.5749\n",
      "Epoch 35/50\n",
      "600000/600000 [==============================] - 206s - loss: 1.2224 - acc: 0.6047 - val_loss: 1.4805 - val_acc: 0.5760\n",
      "Epoch 36/50\n",
      "600000/600000 [==============================] - 206s - loss: 1.2180 - acc: 0.6063 - val_loss: 1.4814 - val_acc: 0.5759\n",
      "Epoch 37/50\n",
      "600000/600000 [==============================] - 206s - loss: 1.2149 - acc: 0.6063 - val_loss: 1.4800 - val_acc: 0.5750\n",
      "Epoch 38/50\n",
      "600000/600000 [==============================] - 207s - loss: 1.2135 - acc: 0.6070 - val_loss: 1.4796 - val_acc: 0.5758\n",
      "Epoch 39/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.2100 - acc: 0.6083 - val_loss: 1.4843 - val_acc: 0.5752\n",
      "Epoch 40/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.2060 - acc: 0.6090 - val_loss: 1.4792 - val_acc: 0.5769\n",
      "Epoch 41/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.2038 - acc: 0.6101 - val_loss: 1.4830 - val_acc: 0.5778\n",
      "Epoch 42/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.2012 - acc: 0.6100 - val_loss: 1.4812 - val_acc: 0.5786\n",
      "Epoch 43/50\n",
      "600000/600000 [==============================] - 201s - loss: 1.1980 - acc: 0.6112 - val_loss: 1.4807 - val_acc: 0.5780\n",
      "Epoch 44/50\n",
      "600000/600000 [==============================] - 198s - loss: 1.1962 - acc: 0.6113 - val_loss: 1.4827 - val_acc: 0.5778\n",
      "Epoch 45/50\n",
      "600000/600000 [==============================] - 196s - loss: 1.1925 - acc: 0.6129 - val_loss: 1.4815 - val_acc: 0.5786\n",
      "Epoch 46/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.1906 - acc: 0.6134 - val_loss: 1.4823 - val_acc: 0.5792\n",
      "Epoch 47/50\n",
      "600000/600000 [==============================] - 202s - loss: 1.1874 - acc: 0.6141 - val_loss: 1.4807 - val_acc: 0.5792\n",
      "Epoch 48/50\n",
      "600000/600000 [==============================] - 202s - loss: 1.1855 - acc: 0.6149 - val_loss: 1.4842 - val_acc: 0.5788\n",
      "Epoch 49/50\n",
      "600000/600000 [==============================] - 202s - loss: 1.1827 - acc: 0.6154 - val_loss: 1.4823 - val_acc: 0.5801\n",
      "Epoch 50/50\n",
      "600000/600000 [==============================] - 203s - loss: 1.1811 - acc: 0.6162 - val_loss: 1.4830 - val_acc: 0.5798\n"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "history = model.fit(X[:600000], y[:600000], batch_size=256, epochs=50,\n",
    "           validation_data=(X[600000:], y[600000:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 600000 samples, validate on 105726 samples\n",
    "Epoch 1/50\n",
    "600000/600000 [==============================] - 96s - loss: 2.4896 - acc: 0.2734 - val_loss: 2.1208 - val_acc: 0.3573\n",
    "Epoch 2/50\n",
    "600000/600000 [==============================] - 98s - loss: 2.0241 - acc: 0.3775 - val_loss: 1.9248 - val_acc: 0.4214\n",
    "Epoch 3/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.8431 - acc: 0.4304 - val_loss: 1.7974 - val_acc: 0.4645\n",
    "Epoch 4/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.7160 - acc: 0.4674 - val_loss: 1.7106 - val_acc: 0.4908\n",
    "Epoch 5/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.9464 - acc: 0.4114 - val_loss: 2.3071 - val_acc: 0.3025\n",
    "Epoch 6/50\n",
    "600000/600000 [==============================] - 96s - loss: 2.1581 - acc: 0.3424 - val_loss: 2.0364 - val_acc: 0.3888\n",
    "Epoch 7/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.9694 - acc: 0.3941 - val_loss: 1.9155 - val_acc: 0.4266\n",
    "Epoch 8/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.8541 - acc: 0.4282 - val_loss: 1.8315 - val_acc: 0.4557\n",
    "Epoch 9/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.7714 - acc: 0.4514 - val_loss: 1.7699 - val_acc: 0.4758\n",
    "Epoch 10/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.7077 - acc: 0.4693 - val_loss: 1.7328 - val_acc: 0.4897\n",
    "Epoch 11/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.6551 - acc: 0.4846 - val_loss: 1.6905 - val_acc: 0.5043\n",
    "Epoch 12/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.6137 - acc: 0.4961 - val_loss: 1.6674 - val_acc: 0.5102\n",
    "Epoch 13/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.5794 - acc: 0.5058 - val_loss: 1.6454 - val_acc: 0.5168\n",
    "Epoch 14/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.5499 - acc: 0.5141 - val_loss: 1.6272 - val_acc: 0.5257\n",
    "Epoch 15/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.5224 - acc: 0.5214 - val_loss: 1.6116 - val_acc: 0.5277\n",
    "Epoch 16/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.5012 - acc: 0.5281 - val_loss: 1.5944 - val_acc: 0.5330\n",
    "Epoch 17/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.4810 - acc: 0.5333 - val_loss: 1.5863 - val_acc: 0.5339\n",
    "Epoch 18/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.4620 - acc: 0.5388 - val_loss: 1.5734 - val_acc: 0.5398\n",
    "Epoch 19/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.4468 - acc: 0.5433 - val_loss: 1.5649 - val_acc: 0.5425\n",
    "Epoch 20/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.4318 - acc: 0.5477 - val_loss: 1.5570 - val_acc: 0.5453\n",
    "Epoch 21/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.4196 - acc: 0.5508 - val_loss: 1.5493 - val_acc: 0.5475\n",
    "Epoch 22/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.4055 - acc: 0.5544 - val_loss: 1.5436 - val_acc: 0.5515\n",
    "Epoch 23/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3941 - acc: 0.5574 - val_loss: 1.5424 - val_acc: 0.5515\n",
    "Epoch 24/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3840 - acc: 0.5599 - val_loss: 1.5342 - val_acc: 0.5523\n",
    "Epoch 25/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3754 - acc: 0.5633 - val_loss: 1.5301 - val_acc: 0.5546\n",
    "Epoch 26/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3643 - acc: 0.5664 - val_loss: 1.5272 - val_acc: 0.5566\n",
    "Epoch 27/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3556 - acc: 0.5680 - val_loss: 1.5248 - val_acc: 0.5587\n",
    "Epoch 28/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3482 - acc: 0.5701 - val_loss: 1.5176 - val_acc: 0.5590\n",
    "Epoch 29/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3419 - acc: 0.5719 - val_loss: 1.5156 - val_acc: 0.5609\n",
    "Epoch 30/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3337 - acc: 0.5739 - val_loss: 1.5126 - val_acc: 0.5608\n",
    "Epoch 31/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3271 - acc: 0.5760 - val_loss: 1.5107 - val_acc: 0.5623\n",
    "Epoch 32/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3202 - acc: 0.5780 - val_loss: 1.5065 - val_acc: 0.5637\n",
    "Epoch 33/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3145 - acc: 0.5792 - val_loss: 1.5059 - val_acc: 0.5648\n",
    "Epoch 34/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3090 - acc: 0.5810 - val_loss: 1.5047 - val_acc: 0.5638\n",
    "Epoch 35/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.3031 - acc: 0.5827 - val_loss: 1.5042 - val_acc: 0.5650\n",
    "Epoch 36/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.2987 - acc: 0.5836 - val_loss: 1.4984 - val_acc: 0.5671\n",
    "Epoch 37/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.2936 - acc: 0.5852 - val_loss: 1.4981 - val_acc: 0.5670\n",
    "Epoch 38/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.2870 - acc: 0.5875 - val_loss: 1.4989 - val_acc: 0.5678\n",
    "Epoch 39/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.2828 - acc: 0.5884 - val_loss: 1.4959 - val_acc: 0.5692\n",
    "Epoch 40/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.2787 - acc: 0.5897 - val_loss: 1.4948 - val_acc: 0.5676\n",
    "Epoch 41/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.2746 - acc: 0.5908 - val_loss: 1.4946 - val_acc: 0.5692\n",
    "Epoch 42/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.2689 - acc: 0.5921 - val_loss: 1.4941 - val_acc: 0.5701\n",
    "Epoch 43/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.2656 - acc: 0.5924 - val_loss: 1.4905 - val_acc: 0.5711\n",
    "Epoch 44/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.2619 - acc: 0.5934 - val_loss: 1.4903 - val_acc: 0.5708\n",
    "Epoch 45/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.2585 - acc: 0.5947 - val_loss: 1.4896 - val_acc: 0.5718\n",
    "Epoch 46/50\n",
    "600000/600000 [==============================] - 96s - loss: 1.2544 - acc: 0.5948 - val_loss: 1.4890 - val_acc: 0.5722\n",
    "Epoch 47/50\n",
    "600000/600000 [==============================] - 95s - loss: 1.2501 - acc: 0.5966 - val_loss: 1.4903 - val_acc: 0.5728\n",
    "Epoch 48/50\n",
    "600000/600000 [==============================] - 95s - loss: 1.2469 - acc: 0.5975 - val_loss: 1.4901 - val_acc: 0.5733\n",
    "Epoch 49/50\n",
    "600000/600000 [==============================] - 95s - loss: 1.2443 - acc: 0.5980 - val_loss: 1.4905 - val_acc: 0.5739\n",
    "Epoch 50/50\n",
    "600000/600000 [==============================] - 95s - loss: 1.2396 - acc: 0.5997 - val_loss: 1.4882 - val_acc: 0.5729"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train on 600000 samples, validate on 132869 samples\n",
    "Epoch 1/30\n",
    "600000/600000 [==============================] - 523s - loss: 2.2971 - acc: 0.3154 - val_loss: 1.9305 - val_acc: 0.4111\n",
    "Epoch 2/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.8107 - acc: 0.4357 - val_loss: 1.7215 - val_acc: 0.4805\n",
    "Epoch 3/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.6351 - acc: 0.4875 - val_loss: 1.6226 - val_acc: 0.5123\n",
    "Epoch 4/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.5340 - acc: 0.5168 - val_loss: 1.5604 - val_acc: 0.5306\n",
    "Epoch 5/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4689 - acc: 0.5361 - val_loss: 1.5244 - val_acc: 0.5443\n",
    "Epoch 6/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4232 - acc: 0.5494 - val_loss: 1.5053 - val_acc: 0.5508\n",
    "Epoch 7/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3890 - acc: 0.5596 - val_loss: 1.4823 - val_acc: 0.5579\n",
    "Epoch 8/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3618 - acc: 0.5672 - val_loss: 1.4643 - val_acc: 0.5625\n",
    "Epoch 9/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3409 - acc: 0.5730 - val_loss: 1.4587 - val_acc: 0.5667\n",
    "Epoch 10/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.3231 - acc: 0.5784 - val_loss: 1.4466 - val_acc: 0.5690\n",
    "Epoch 11/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3084 - acc: 0.5819 - val_loss: 1.4398 - val_acc: 0.5736\n",
    "Epoch 12/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2954 - acc: 0.5864 - val_loss: 1.4360 - val_acc: 0.5740\n",
    "Epoch 13/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2839 - acc: 0.5885 - val_loss: 1.4296 - val_acc: 0.5776\n",
    "Epoch 14/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2751 - acc: 0.5913 - val_loss: 1.4275 - val_acc: 0.5785\n",
    "Epoch 15/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2645 - acc: 0.5944 - val_loss: 1.4230 - val_acc: 0.5794\n",
    "Epoch 16/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2578 - acc: 0.5963 - val_loss: 1.4182 - val_acc: 0.5816\n",
    "Epoch 17/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2508 - acc: 0.5988 - val_loss: 1.4202 - val_acc: 0.5818\n",
    "Epoch 18/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2455 - acc: 0.6005 - val_loss: 1.4151 - val_acc: 0.5812\n",
    "Epoch 19/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2394 - acc: 0.6017 - val_loss: 1.4133 - val_acc: 0.5836\n",
    "Epoch 20/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2332 - acc: 0.6037 - val_loss: 1.4099 - val_acc: 0.5848\n",
    "Epoch 21/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2288 - acc: 0.6051 - val_loss: 1.4164 - val_acc: 0.5837\n",
    "Epoch 22/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2247 - acc: 0.6057 - val_loss: 1.4102 - val_acc: 0.5861\n",
    "Epoch 23/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2215 - acc: 0.6067 - val_loss: 1.4080 - val_acc: 0.5873\n",
    "Epoch 24/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2167 - acc: 0.6078 - val_loss: 1.4099 - val_acc: 0.5855\n",
    "Epoch 25/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2124 - acc: 0.6097 - val_loss: 1.4050 - val_acc: 0.5876\n",
    "Epoch 26/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2111 - acc: 0.6101 - val_loss: 1.4105 - val_acc: 0.5868\n",
    "Epoch 27/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2059 - acc: 0.6116 - val_loss: 1.4027 - val_acc: 0.5879\n",
    "Epoch 28/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2026 - acc: 0.6122 - val_loss: 1.4048 - val_acc: 0.5868\n",
    "Epoch 29/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.1990 - acc: 0.6138 - val_loss: 1.4090 - val_acc: 0.5887\n",
    "Epoch 30/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.1969 - acc: 0.6139 - val_loss: 1.4053 - val_acc: 0.5889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAHVCAYAAADcnaM7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c80a7ceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "\n",
    "save_model(model, path + 'models/text_generation_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model1 = load_model(path + 'models/text_generation_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 20\n",
    "\n",
    "\n",
    "def sample(a, diversity=1.0):\n",
    "    '''\n",
    "    helper function to sample an index from a probability array\n",
    "    - Diversity control the level of randomless\n",
    "    '''\n",
    "    a = np.log(a) / diversity\n",
    "    a = np.exp(a) / np.sum(np.exp(a), axis=0)\n",
    "    a /= np.sum(a+0.0000001) #Precission error\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "\n",
    "def generate_text(sentence, diversity, current_model, num_char=400):\n",
    "    sentence_init = sentence\n",
    "    generated = ''\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = current_model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    print()\n",
    "    print('DIVERSITY: ',diversity)\n",
    "    print(sentence_init + generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/ipykernel/__main__.py:9: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIVERSITY:  0.5\n",
      " mi señora dulcinea del toboso\n",
      "   de donde pasó más de las manos en la mitad del camino que se le había de parar todas aquellas que de la profunda se lo dicen, no pudo ningún fin de los caballeros andantes, que el barbero se consigo, y aun las representan cada uno de los que pudiera ser estado, pues si la sobrina\n",
      "\n",
      "\n",
      "-pues está en mi profesión y más alto y el mesmo que hacer en el cuerpo de la mancha, y el cual se le c\n"
     ]
    }
   ],
   "source": [
    "sentence = ' mi señora dulcinea '\n",
    "generate_text(sentence, 0.5, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/ipykernel/__main__.py:9: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIVERSITY:  0.2\n",
      "mire vuestra merced se le ofreció de la mancha, y que no se ha de ser tan buena gana de la mancha, y que el amor de la mancha, y que no se puede ser tan sabio que le pareció a decir que es mucha presencia del toboso, que es tan deseo de ser que el cura y el amo no le hallaba en la cabeza, y de la mancha, y que no se puede ser alguna peligrosa aventura, y el cura y el caballero del señor don quijote, y de la mancha, y\n",
      "\n",
      "DIVERSITY:  0.5\n",
      "mire vuestra merced que se considera que le había de estar otra cosa.\n",
      "\n",
      "-no se ha de dejar de esperar a una carta de su caballería, y\n",
      "que le dijo que se estaba despertar puesto el camino que tanto se quiero decir que la suerte, y no se ha de parar aquel pasado en la cabeza, en la tierra y del cielo\n",
      "que el cura y el barbero andante caballero andante.\n",
      "\n",
      "-eso no quiero decir que quisiera que deseaba de ser cosa de don fer\n",
      "\n",
      "DIVERSITY:  1\n",
      "mire vuestra merced basta que no temiese por el gigante de caballerías quisiera hacer lo que encaminaba vara un sobre otro reino en el sueño a la había de represente a las obra no hay en el mundo, se fueron poeta respuestas de los que ella\n",
      "sino una mancha, un plático y gesta, nunca\n",
      "mi señora portuna había conocido) y el que se trujó\n",
      "a todas las cuatro señas criatianas; y si había\n",
      "hallado el retor debajo\n",
      "de hierro, si\n",
      "\n",
      "DIVERSITY:  1.2\n",
      "mire vuestra merced que él me alcanza.\n",
      "\n",
      "el otro cuento y de ora cómo el mundo; que\n",
      "cuando quisiera reciber o\n",
      "ha-, ¿qué es -dijo el cura-, señora parte, hasta que, aunque cuando toca\n",
      "y alboroto, ni está en el\n",
      "mismo manera, y también se aglo en la mano, pero los dos estuvieron\n",
      "los más de por la armas, confirmó\n",
      "de rifre que aqué; que, habiendo venido azote el bofa donde se has contado en las más jinés, sino\n",
      "vuesa empres\n",
      "\n",
      "DIVERSITY:  0.2\n",
      " mi señora dulcinea del toboso, y que el caballero andante caballero andante caballero andante.\n",
      "\n",
      "-eso no -dijo el cura-, que el caballero andante caballero andante, que es un parecer que le había de ser tan sabio que el cura y el barbero se le había de probar de su casa, y que esto es el mundo. y así, le dijo:\n",
      "\n",
      "-señor caballero, que es un poco más admirado de la caballería andante.\n",
      "\n",
      "-así es -dijo el cura-, que es men\n",
      "\n",
      "DIVERSITY:  0.5\n",
      " mi señora dulcinea del toboso, que ya se consintió que no es muy a quien yo soy el remedio y se me da a la sobrina.\n",
      "\n",
      "-eso no -dijo sancho-, y ¿qué decir \"dijo el cura-, que yo tengo de probar sino el mundo. y, pues, en esto, se fue a descubrirme en la tal y de la corazón con la cueva de la memoria de la sobrina, sin que decís para ella me dejaron; y\n",
      "así, le dijo:\n",
      "\n",
      "-señora mía, que el trabajo que tenía de la mitad de\n",
      "\n",
      "DIVERSITY:  1\n",
      " mi señora dulcinea del toboso,\n",
      "acudió al doctor ni de las\n",
      "incompañías y esperanzas, y venga este primero le mostrón de hablarlas, y en aquel año, que no os dará admirado de su nombre, fue para no verle en él se puso. más cuando fue infinito iquería haberme\n",
      "comido del mundo! si no es vida, pues la ventera del castillo, y con el\n",
      "temor de sancho panza y que lo puede haber\n",
      "andado y la llanta me doy\n",
      "afligidos, que estaba\n",
      "\n",
      "DIVERSITY:  1.2\n",
      " mi señora dulcinea del toboso, y es que las demasifas se estraba captiva rála, pues que\n",
      "lleva de su fuego, y ¡huela creer\n",
      "don fernándome espara un riquecís, más capitán se\n",
      "ofrecieron del dueño donde vama su madre, se acompañason a\n",
      "ellas.\n",
      "¡las batallas que llegasen y hacer estos mugícadores que nos dejé de\n",
      "haber tan cortado que no\n",
      "lo trujese a hacer).\n",
      "''yo, pues, vernadorlas,\n",
      "don quijote sancho acertíque es cosa se h\n",
      "\n",
      "DIVERSITY:  0.2\n",
      "el caballero andant3 caballero andante caballero andante, que es tan buena gana que el caballero del bosque y de la mancha, y a la mano en el suelo, y que el cura y el ama de la caballería andante, que no hay que decir que si es mucha presteza que ella se había de ser tan alta se entretenía a la cabeza, y que no es muy bien que este mismo historiador de su caballería andante caballero andante caballero andante caball\n",
      "\n",
      "DIVERSITY:  0.5\n",
      "el caballero andant3 caballero andante, y a los que si le había de ser un mozo me diera a camila a las más\n",
      "pareceres de sus amores, y los dos cortesanos del camino de la caballería andante, que le había de ser tener alguna peligro en la mitad del mayordomo\n",
      "de los mejores de la caballería andante caballero andante.\n",
      "\n",
      "-eso no digo -respondió el cura-, que le dijo a la mancha, y no se entregase con la mitad del corazón, \n",
      "\n",
      "DIVERSITY:  1\n",
      "el caballero andant3, haciéndola de caballerías que desde los encantadores.\n",
      "\n",
      "-ha venido cierto que llegue della a vuestra merced que, del doctorrelle refrás, o por aquí\n",
      "en el suyo, y en aquí lo sea,\n",
      "a lo que a estas partes de grande ampara.\n",
      "\n",
      "siendo mi ventura, que cuando llegó ante la carta a la ánima que allí se\n",
      "pareció de allí a mi bestia, y cuando los temaros, luego con la cumplida.\n",
      "\n",
      "-señora, me dije -respondió el\n",
      "\n",
      "DIVERSITY:  1.2\n",
      "el caballero andant3 caballero, y de todo eso, he oído raso respondie:\n",
      "\n",
      "-señor, sancho -dijo dan la gana, sin enamorados grandes vaisas.\n",
      "-como a él volveré al poele,\n",
      "porque sabe, los gobiernos de la cortesía.\n",
      "despió de huya las barbas a una corte\n",
      "por no uniz cred, mi profundo los albabrías, una lanza\n",
      "insoledida, imagino que allí al buen bosque pasaje al\n",
      "rey; y, por ahora, ¡oh refriegado que puede empientarse y mohími\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = 'mire vuestra merced '\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "sentence = ' mi señora dulcinea '\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "sentence = 'el caballero andant3'\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "mire vuestra merced decís, y que se le pareció que en la cabeza de la cabeza, y el caballero del caballero de la cabeza, y los demás de los demás de los demás de los demás de la mano de la mujer de la mano de la mano, y aun a su señora dulcinea del toboso, y el cura que el caballero de la mano que le pareció que estaba en la misma cosa que en la mitad del caballero de la mano, se le dijo:\n",
    "\n",
    "-no sé -respondi\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "mire vuestra merced que no está en la cabeza. pero, en efeto, pues todo aquello que está en el mundo que cada uno debía\n",
    "de ser con la misma sierra parte de la muerte de la mano, con todo el mundo me la conoció, que tenía por el primero que están el rostro en las manos, y al señor don quijote -respondió sancho-, porque en la puerta de los sucesos del buen estado de la desencantada de los ojos.\n",
    "\n",
    "-¿qué mal \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "mire vuestra merced que, aunque, desdicho y tiempo viene, a\n",
    "cuya cabeza destos tres, los informaciones que dejara de serle, que me fueron? ¿admirado,\n",
    "\n",
    "y, creyendo que me va y enfermo. y esto que tienen entonces las requiebros que algunos limpios en un ampeoso como si improvisentes en sus insimulables y en los míos al que el honesto, en la mano de mucho premio y don quijote fingióno los dos o sabidores, y,\n",
    "acom\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "mire vuestra merced paso que no sa cluero-. subió, señor,\n",
    "le hubiera vuelto el duque, que pica, yo fue pose�ría de guardar cierto. para los demás, esperando la misma tragua debe de haberlas hallado su santa hijo ni debían para mí, porque yo hay de platar pre subir tú turba -dijo el deleitable-; otras porturas, sino como alabanzas y\n",
    "comedimientos puciese\n",
    "los días de lo que von mirado después de\n",
    "visión de\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de lo que sucedió a la mano de la mano de la mano de los ojos de la mano de la mano de la mano, y aun más que se le dijese la mano de la mano y en la mitad del caballero del caballero de la mano de la mano, y que el parte de la mano, y que en la mitad del caballero de la mano, sino a la puerta de la mano de la mano de la mano de los demás de la cabeza, y el cura y la cabeza de la mano y en la cabeza de la cabeza, \n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de lo que sucedió a camila, por ser tan alta de la mano y en voz brazo? ¿qué es lo que pudiera, señor don quijote que en las fermosuras que después que le sacarán con don quijote había de ser la duquesa, la cual no le había de ser muy buena como gausa, y si este deseo de ser mejor que en la cabeza está a los dos de la mano, y su amo no le puede dar a su casa por los manos de mano, diciendo:\n",
    "\n",
    "-pues, ¿qué \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de lo que sucedió a las más faltas cuentan haciendo:\n",
    "\n",
    "-�qcorrían por esto, bueno -respondió cómo está aquel mano traía furia por ella estajo\n",
    "más que andaba y fingión de sus apartieron de modo que no\n",
    "hay vasto qué hizo con galdáis que soy sin duda, duque ni en la más\n",
    "buena gran señora dulcinea; y así lo han don quijote y no bastaba junto a nuestra\n",
    "señora dulcinea, ahora\n",
    "la entienda, el aposento a\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de lo que sucedió al\n",
    "admiración, dijo:\n",
    "\n",
    "-eso me esplevo en razón encantada, y su venimo tiempo, que pasaba tan hirtoria. es\n",
    "\n",
    "don quijote, le dijo:\n",
    "\n",
    "y cuando jamás:\n",
    "  si ya entienden en mi padre desde aquí vean fuerza, como yo\n",
    "costa yo he oído decir, el rey\n",
    "\n",
    "vención que ésto, que lo més comenzó a vuestra merced colgaré por la orden y parece que\n",
    "pudieron semplarse. el cual, si de la mono.\n",
    "acudi�\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de allí a poco comenzó a su caballero andante, que en la más hermosa de la mano, y el cura que el mundo tenía con el cura y el de la mano de la cabeza de la mano de la mano de la mano de la mano de la cabeza, y a los demás de los cuatro de la mano de la mano de la mano y en la mitad del caballero de la cabeza, y le dijo:\n",
    "\n",
    "-¡oh sancho -dijo el cura-, que no se le habían de ser manos de los demás de los de la\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de allí a poco comenzó a la vida de la mano de\n",
    "arriba al mundo. pero, con todo eso, ha de ser el rostro de las linajes de su escudero. por el rey en el mundo de la industria que en su caballero andante; y, aunque se volvió a camila y de la mano de la muerte de mi cabeza? y así, como el tal caballero andante, que los demás juramentos, y con la mitad del toboso, y así, por decir que os son de allí a los deseos \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de allí a poco comenzó\n",
    "a dos crazos de entender que tratar la nueva al desde nuevas andantes de solos con los detros donde me hubieran\n",
    "de subir la sumiera y rabia la duquesa\n",
    "   a un hacer con un gato, que le\n",
    "dijo juntar la locura para su\n",
    "amo, para que quisieron decir alguna, vino\n",
    "todas aquellos demás caballeros. pero, sancho, tanto, viendo camila la primero tiene: prodición\n",
    "que yo le dio caminar las belloz\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de allí a poco comer. otros días se le pidía hablar de cerra\n",
    "   que el gate predice\n",
    "y otras puntas del cordel bestia.\n",
    "\n",
    "-yo no por eso, decaría sobre la venta, porque él saslos demás sin él que\n",
    "llevase a buscar de la suma, sancho milático, cuando posían los zogados. y si así, mi rendido a\n",
    "cuerpo, ni en llopar salió el baece-, que el hábísimo que viene por\n",
    "los tratas y tontos que sean, y el de los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras2_py36]",
   "language": "python",
   "name": "conda-env-keras2_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
