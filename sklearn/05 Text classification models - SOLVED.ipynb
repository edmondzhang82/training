{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn version: 0.18.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "print('Sklearn version:', sklearn_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sklearn.datasets.twenty_newsgroups\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                 remove=('headers', 'footers', 'quotes'),\n",
    "                 categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "---------------\n",
      "Target:  1\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(twenty_train.data[0])\n",
    "print('---------------')\n",
    "print('Target: ', twenty_train.target[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 5000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text preprocessing, tokenizing and filtering of stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=5000,\n",
    "                                stop_words='english')\n",
    "X_train_counts = tf_vectorizer.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2866)\t1\n",
      "  (0, 238)\t1\n",
      "  (0, 4522)\t1\n",
      "  (0, 2058)\t1\n",
      "  (0, 1123)\t1\n",
      "  (0, 3867)\t1\n",
      "  (0, 1543)\t1\n",
      "  (0, 3385)\t1\n",
      "  (0, 2197)\t1\n",
      "  (0, 1094)\t1\n",
      "  (0, 2643)\t1\n",
      "  (0, 1865)\t1\n",
      "  (0, 2237)\t1\n",
      "  (0, 1795)\t2\n",
      "  (0, 4520)\t1\n",
      "  (0, 2251)\t1\n",
      "  (0, 1090)\t1\n",
      "  (0, 4744)\t1\n",
      "  (0, 3276)\t1\n",
      "  (0, 357)\t1\n",
      "  (0, 3273)\t1\n",
      "  (0, 4299)\t1\n",
      "  (0, 4869)\t1\n",
      "  (0, 2014)\t1\n",
      "  (0, 2550)\t1\n",
      "  (0, 1445)\t1\n",
      "  (232, 0)\t2\n",
      "  (272, 0)\t1\n",
      "  (282, 0)\t1\n",
      "  (400, 0)\t1\n",
      "  (433, 0)\t2\n",
      "  (581, 0)\t2\n",
      "  (588, 0)\t1\n",
      "  (766, 0)\t1\n",
      "  (768, 0)\t2\n",
      "  (837, 0)\t3\n",
      "  (844, 0)\t1\n",
      "  (859, 0)\t1\n",
      "  (880, 0)\t1\n",
      "  (1030, 0)\t1\n",
      "  (1056, 0)\t6\n",
      "  (1057, 0)\t2\n",
      "  (1263, 0)\t1\n",
      "  (1475, 0)\t1\n",
      "  (1665, 0)\t16\n",
      "  (1795, 0)\t1\n",
      "  (1802, 0)\t1\n",
      "  (1833, 0)\t1\n",
      "  (1890, 0)\t2\n",
      "  (2069, 0)\t1\n",
      "  (2144, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts[0,:])\n",
    "print(X_train_counts[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From occurrences to frequencies\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tf = tfidf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1445)\t0.0998496101737\n",
      "  (0, 2550)\t0.0920875619201\n",
      "  (0, 2014)\t0.10905059472\n",
      "  (0, 4869)\t0.112409159775\n",
      "  (0, 4299)\t0.172232378831\n",
      "  (0, 3273)\t0.189497984618\n",
      "  (0, 357)\t0.196147304589\n",
      "  (0, 3276)\t0.239358101611\n",
      "  (0, 4744)\t0.242697172074\n",
      "  (0, 1090)\t0.185367646905\n",
      "  (0, 2251)\t0.281517460204\n",
      "  (0, 4520)\t0.239358101611\n",
      "  (0, 1795)\t0.326673936513\n",
      "  (0, 2237)\t0.217882788689\n",
      "  (0, 1865)\t0.182356290661\n",
      "  (0, 2643)\t0.0944312658437\n",
      "  (0, 1094)\t0.250397930473\n",
      "  (0, 2197)\t0.225991796704\n",
      "  (0, 3385)\t0.272954303671\n",
      "  (0, 1543)\t0.163780615995\n",
      "  (0, 3867)\t0.165608347231\n",
      "  (0, 1123)\t0.157610927262\n",
      "  (0, 2058)\t0.144807482284\n",
      "  (0, 4522)\t0.126533637604\n",
      "  (0, 238)\t0.170069829145\n",
      "  (0, 2866)\t0.190380209723\n",
      "  (232, 0)\t0.162673301572\n",
      "  (272, 0)\t0.0396045882998\n",
      "  (282, 0)\t0.0830143471237\n",
      "  (400, 0)\t0.00527736458963\n",
      "  (433, 0)\t0.00596499373539\n",
      "  (581, 0)\t0.150704657006\n",
      "  (588, 0)\t0.154296833105\n",
      "  (766, 0)\t0.126956846998\n",
      "  (768, 0)\t0.0117078298784\n",
      "  (837, 0)\t0.334685959895\n",
      "  (844, 0)\t0.207167396703\n",
      "  (859, 0)\t0.216506134034\n",
      "  (880, 0)\t0.0133502362916\n",
      "  (1030, 0)\t0.278741714593\n",
      "  (1056, 0)\t0.212612262278\n",
      "  (1057, 0)\t0.139865527738\n",
      "  (1263, 0)\t0.0889107355711\n",
      "  (1475, 0)\t0.275148224162\n",
      "  (1665, 0)\t0.22386057664\n",
      "  (1795, 0)\t0.0911799104224\n",
      "  (1802, 0)\t0.0165319212251\n",
      "  (1833, 0)\t0.11551299395\n",
      "  (1890, 0)\t0.0079616312192\n",
      "  (2069, 0)\t0.10844175005\n",
      "  (2144, 0)\t0.114983457384\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tf[0,:])\n",
    "print(X_train_tf[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First basic model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define and fit in one line\n",
    "clf = MultinomialNB().fit(X_train_tf, twenty_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test:  0.798934753662\n"
     ]
    }
   ],
   "source": [
    "#Score test data\n",
    "\n",
    "# Read test data\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                 remove=('headers', 'footers', 'quotes'),\n",
    "                 categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Transform text to counts\n",
    "X_test_counts = tf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tf-idf transformation\n",
    "X_test_tf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "# Prediction\n",
    "predicted = clf.predict(X_test_tf)\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy test: ', accuracy_score(twenty_test.target, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# Score 2 new docs\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "\n",
    "X_new_counts = tf_vectorizer.transform(docs_new)\n",
    "\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.95, max_features=5000, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "    ...False,\n",
       "         use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define the pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "# Fit all the pipeline\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79893475366178424"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate test data\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                    remove=('headers', 'footers', 'quotes'),\n",
    "                    categories=categories, \n",
    "                    shuffle=True, random_state=42)\n",
    "\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted == twenty_test.target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change classifier in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80692410119840208"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42)),\n",
    "                    ])\n",
    "#Fit\n",
    "_ = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# Predict\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "\n",
    "# Evaluate accuracy\n",
    "np.mean(predicted == twenty_test.target)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80892143808255657"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', svm.LinearSVC()),\n",
    "                    ])\n",
    "\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define estimator. No parameters of the search\n",
    "clf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', svm.LinearSVC()),\n",
    "                ])\n",
    "\n",
    "# Specify parameters and distributions to sample from\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_dist = {\"vect__max_features\": [1000, 2500, 5000, 7500, 10000, None], \n",
    "              \"vect__stop_words\": ['english', None], \n",
    "              \"clf__C\": [.1, .5, 1., 1.5, 2.]}\n",
    "\n",
    "# Define randomized search\n",
    "n_iter_search = 10\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search)\n",
    "\n",
    "# Run the randomized search\n",
    "random_search.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>param_vect__stop_words</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.385353</td>\n",
       "      <td>0.161182</td>\n",
       "      <td>0.865751</td>\n",
       "      <td>0.982057</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7500</td>\n",
       "      <td>english</td>\n",
       "      <td>{u'clf__C': 0.5, u'vect__max_features': 7500, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.881806</td>\n",
       "      <td>0.982048</td>\n",
       "      <td>0.855246</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.860186</td>\n",
       "      <td>0.980080</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.008654</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.001618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.394397</td>\n",
       "      <td>0.162430</td>\n",
       "      <td>0.818343</td>\n",
       "      <td>0.982499</td>\n",
       "      <td>2</td>\n",
       "      <td>2500</td>\n",
       "      <td>english</td>\n",
       "      <td>{u'clf__C': 2.0, u'vect__max_features': 2500, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.819389</td>\n",
       "      <td>0.983378</td>\n",
       "      <td>0.816733</td>\n",
       "      <td>0.982713</td>\n",
       "      <td>0.818908</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.009286</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.000818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.413017</td>\n",
       "      <td>0.162230</td>\n",
       "      <td>0.865308</td>\n",
       "      <td>0.982499</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 1.0, u'vect__max_features': 10000,...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.875166</td>\n",
       "      <td>0.982048</td>\n",
       "      <td>0.853918</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.866844</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>0.009257</td>\n",
       "      <td>0.008585</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>0.001122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.384515</td>\n",
       "      <td>0.162045</td>\n",
       "      <td>0.828977</td>\n",
       "      <td>0.951043</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 0.1, u'vect__max_features': None, ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.843293</td>\n",
       "      <td>0.954122</td>\n",
       "      <td>0.816733</td>\n",
       "      <td>0.951463</td>\n",
       "      <td>0.826897</td>\n",
       "      <td>0.947543</td>\n",
       "      <td>0.007866</td>\n",
       "      <td>0.009316</td>\n",
       "      <td>0.010947</td>\n",
       "      <td>0.002702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.383914</td>\n",
       "      <td>0.163052</td>\n",
       "      <td>0.826318</td>\n",
       "      <td>0.947499</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7500</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 0.1, u'vect__max_features': 7500, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.837981</td>\n",
       "      <td>0.952128</td>\n",
       "      <td>0.815405</td>\n",
       "      <td>0.947473</td>\n",
       "      <td>0.825566</td>\n",
       "      <td>0.942895</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>0.007929</td>\n",
       "      <td>0.009236</td>\n",
       "      <td>0.003769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.391355</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>0.853345</td>\n",
       "      <td>0.980505</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 0.5, u'vect__max_features': 5000, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.861886</td>\n",
       "      <td>0.979388</td>\n",
       "      <td>0.841965</td>\n",
       "      <td>0.982048</td>\n",
       "      <td>0.856192</td>\n",
       "      <td>0.980080</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>0.008330</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.001127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.437481</td>\n",
       "      <td>0.162024</td>\n",
       "      <td>0.858219</td>\n",
       "      <td>0.983164</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 2.0, u'vect__max_features': None, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.864542</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.848606</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.861518</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.008307</td>\n",
       "      <td>0.006913</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.375346</td>\n",
       "      <td>0.163447</td>\n",
       "      <td>0.816128</td>\n",
       "      <td>0.932875</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2500</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 0.1, u'vect__max_features': 2500, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.833997</td>\n",
       "      <td>0.932846</td>\n",
       "      <td>0.800797</td>\n",
       "      <td>0.931516</td>\n",
       "      <td>0.813582</td>\n",
       "      <td>0.934263</td>\n",
       "      <td>0.008025</td>\n",
       "      <td>0.008995</td>\n",
       "      <td>0.013679</td>\n",
       "      <td>0.001122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.413922</td>\n",
       "      <td>0.159846</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>english</td>\n",
       "      <td>{u'clf__C': 2.0, u'vect__max_features': 10000,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.873838</td>\n",
       "      <td>0.984707</td>\n",
       "      <td>0.852590</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.868176</td>\n",
       "      <td>0.982072</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>0.008988</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.395163</td>\n",
       "      <td>0.159460</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.983164</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>english</td>\n",
       "      <td>{u'clf__C': 1.0, u'vect__max_features': 10000,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880478</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.853918</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.872170</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>0.008901</td>\n",
       "      <td>0.008428</td>\n",
       "      <td>0.011098</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.385353         0.161182         0.865751          0.982057   \n",
       "1       0.394397         0.162430         0.818343          0.982499   \n",
       "2       0.413017         0.162230         0.865308          0.982499   \n",
       "3       0.384515         0.162045         0.828977          0.951043   \n",
       "4       0.383914         0.163052         0.826318          0.947499   \n",
       "5       0.391355         0.161800         0.853345          0.980505   \n",
       "6       0.437481         0.162024         0.858219          0.983164   \n",
       "7       0.375346         0.163447         0.816128          0.932875   \n",
       "8       0.413922         0.159846         0.864865          0.983607   \n",
       "9       0.395163         0.159460         0.868852          0.983164   \n",
       "\n",
       "  param_clf__C param_vect__max_features param_vect__stop_words  \\\n",
       "0          0.5                     7500                english   \n",
       "1            2                     2500                english   \n",
       "2            1                    10000                   None   \n",
       "3          0.1                     None                   None   \n",
       "4          0.1                     7500                   None   \n",
       "5          0.5                     5000                   None   \n",
       "6            2                     None                   None   \n",
       "7          0.1                     2500                   None   \n",
       "8            2                    10000                english   \n",
       "9            1                    10000                english   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {u'clf__C': 0.5, u'vect__max_features': 7500, ...                2   \n",
       "1  {u'clf__C': 2.0, u'vect__max_features': 2500, ...                9   \n",
       "2  {u'clf__C': 1.0, u'vect__max_features': 10000,...                3   \n",
       "3  {u'clf__C': 0.1, u'vect__max_features': None, ...                7   \n",
       "4  {u'clf__C': 0.1, u'vect__max_features': 7500, ...                8   \n",
       "5  {u'clf__C': 0.5, u'vect__max_features': 5000, ...                6   \n",
       "6  {u'clf__C': 2.0, u'vect__max_features': None, ...                5   \n",
       "7  {u'clf__C': 0.1, u'vect__max_features': 2500, ...               10   \n",
       "8  {u'clf__C': 2.0, u'vect__max_features': 10000,...                4   \n",
       "9  {u'clf__C': 1.0, u'vect__max_features': 10000,...                1   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.881806            0.982048           0.855246   \n",
       "1           0.819389            0.983378           0.816733   \n",
       "2           0.875166            0.982048           0.853918   \n",
       "3           0.843293            0.954122           0.816733   \n",
       "4           0.837981            0.952128           0.815405   \n",
       "5           0.861886            0.979388           0.841965   \n",
       "6           0.864542            0.984043           0.848606   \n",
       "7           0.833997            0.932846           0.800797   \n",
       "8           0.873838            0.984707           0.852590   \n",
       "9           0.880478            0.984043           0.853918   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.984043           0.860186            0.980080      0.007650   \n",
       "1            0.982713           0.818908            0.981408      0.003227   \n",
       "2            0.984043           0.866844            0.981408      0.009257   \n",
       "3            0.951463           0.826897            0.947543      0.007866   \n",
       "4            0.947473           0.825566            0.942895      0.008919   \n",
       "5            0.982048           0.856192            0.980080      0.007668   \n",
       "6            0.984043           0.861518            0.981408      0.003648   \n",
       "7            0.931516           0.813582            0.934263      0.008025   \n",
       "8            0.984043           0.868176            0.982072      0.007230   \n",
       "9            0.984043           0.872170            0.981408      0.008901   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.008654        0.011538         0.001618  \n",
       "1        0.009286        0.001156         0.000818  \n",
       "2        0.008585        0.008746         0.001122  \n",
       "3        0.009316        0.010947         0.002702  \n",
       "4        0.007929        0.009236         0.003769  \n",
       "5        0.008330        0.008381         0.001127  \n",
       "6        0.008307        0.006913         0.001242  \n",
       "7        0.008995        0.013679         0.001122  \n",
       "8        0.008104        0.008988         0.001119  \n",
       "9        0.008428        0.011098         0.001242  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dictionary of search results to a Pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df_cv_results = pd.DataFrame.from_dict(random_search.cv_results_)\n",
    "df_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81424766977363516"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score & evaluate test data using the best estimator\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=10000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', svm.LinearSVC(C=1.5)),\n",
    "                    ])\n",
    "\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional metrics for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.76      0.61      0.68       319\n",
      "         comp.graphics       0.82      0.92      0.87       389\n",
      "               sci.med       0.88      0.85      0.86       396\n",
      "soc.religion.christian       0.78      0.84      0.81       398\n",
      "\n",
      "           avg / total       0.81      0.81      0.81      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(twenty_test.target, \n",
    "                                    predicted,\n",
    "                                    target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[196,  22,  24,  77],\n",
       "       [ 16, 356,  14,   3],\n",
       "       [ 14,  32, 337,  13],\n",
       "       [ 32,  22,  10, 334]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
