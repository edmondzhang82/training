{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn version: 0.18.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "print('Sklearn version:', sklearn_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                 remove=('headers', 'footers', 'quotes'),\n",
    "                 categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "---------------\n",
      "Target:  1\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(twenty_train.data[0])\n",
    "print('---------------')\n",
    "print('Target: ', twenty_train.target[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 5000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text preprocessing, tokenizing and filtering of stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=5000,\n",
    "                                stop_words='english')\n",
    "X_train_counts = tf_vectorizer.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2866)\t1\n",
      "  (0, 238)\t1\n",
      "  (0, 4522)\t1\n",
      "  (0, 2058)\t1\n",
      "  (0, 1123)\t1\n",
      "  (0, 3867)\t1\n",
      "  (0, 1543)\t1\n",
      "  (0, 3385)\t1\n",
      "  (0, 2197)\t1\n",
      "  (0, 1094)\t1\n",
      "  (0, 2643)\t1\n",
      "  (0, 1865)\t1\n",
      "  (0, 2237)\t1\n",
      "  (0, 1795)\t2\n",
      "  (0, 4520)\t1\n",
      "  (0, 2251)\t1\n",
      "  (0, 1090)\t1\n",
      "  (0, 4744)\t1\n",
      "  (0, 3276)\t1\n",
      "  (0, 357)\t1\n",
      "  (0, 3273)\t1\n",
      "  (0, 4299)\t1\n",
      "  (0, 4869)\t1\n",
      "  (0, 2014)\t1\n",
      "  (0, 2550)\t1\n",
      "  (0, 1445)\t1\n",
      "  (232, 0)\t2\n",
      "  (272, 0)\t1\n",
      "  (282, 0)\t1\n",
      "  (400, 0)\t1\n",
      "  (433, 0)\t2\n",
      "  (581, 0)\t2\n",
      "  (588, 0)\t1\n",
      "  (766, 0)\t1\n",
      "  (768, 0)\t2\n",
      "  (837, 0)\t3\n",
      "  (844, 0)\t1\n",
      "  (859, 0)\t1\n",
      "  (880, 0)\t1\n",
      "  (1030, 0)\t1\n",
      "  (1056, 0)\t6\n",
      "  (1057, 0)\t2\n",
      "  (1263, 0)\t1\n",
      "  (1475, 0)\t1\n",
      "  (1665, 0)\t16\n",
      "  (1795, 0)\t1\n",
      "  (1802, 0)\t1\n",
      "  (1833, 0)\t1\n",
      "  (1890, 0)\t2\n",
      "  (2069, 0)\t1\n",
      "  (2144, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts[0,:])\n",
    "print(X_train_counts[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From occurrences to frequencies\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tf = tfidf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1445)\t0.0998496101737\n",
      "  (0, 2550)\t0.0920875619201\n",
      "  (0, 2014)\t0.10905059472\n",
      "  (0, 4869)\t0.112409159775\n",
      "  (0, 4299)\t0.172232378831\n",
      "  (0, 3273)\t0.189497984618\n",
      "  (0, 357)\t0.196147304589\n",
      "  (0, 3276)\t0.239358101611\n",
      "  (0, 4744)\t0.242697172074\n",
      "  (0, 1090)\t0.185367646905\n",
      "  (0, 2251)\t0.281517460204\n",
      "  (0, 4520)\t0.239358101611\n",
      "  (0, 1795)\t0.326673936513\n",
      "  (0, 2237)\t0.217882788689\n",
      "  (0, 1865)\t0.182356290661\n",
      "  (0, 2643)\t0.0944312658437\n",
      "  (0, 1094)\t0.250397930473\n",
      "  (0, 2197)\t0.225991796704\n",
      "  (0, 3385)\t0.272954303671\n",
      "  (0, 1543)\t0.163780615995\n",
      "  (0, 3867)\t0.165608347231\n",
      "  (0, 1123)\t0.157610927262\n",
      "  (0, 2058)\t0.144807482284\n",
      "  (0, 4522)\t0.126533637604\n",
      "  (0, 238)\t0.170069829145\n",
      "  :\t:\n",
      "  (9, 1041)\t0.0576401552277\n",
      "  (9, 1780)\t0.115995106978\n",
      "  (9, 1492)\t0.0533385546095\n",
      "  (9, 2755)\t0.0583670692289\n",
      "  (9, 1446)\t0.0372652083697\n",
      "  (9, 3264)\t0.102003115359\n",
      "  (9, 4551)\t0.0847112103101\n",
      "  (9, 423)\t0.0486076782268\n",
      "  (9, 703)\t0.0613781399834\n",
      "  (9, 2365)\t0.147719409349\n",
      "  (9, 3160)\t0.0711204602374\n",
      "  (9, 2266)\t0.0699720653426\n",
      "  (9, 2388)\t0.0432569176033\n",
      "  (9, 2603)\t0.0599847983667\n",
      "  (9, 1944)\t0.0513908730075\n",
      "  (9, 4239)\t0.0471714365306\n",
      "  (9, 3635)\t0.0452066420636\n",
      "  (9, 1064)\t0.05542099372\n",
      "  (9, 2745)\t0.0724042565057\n",
      "  (9, 726)\t0.0487622860731\n",
      "  (9, 3751)\t0.064849180693\n",
      "  (9, 2554)\t0.0508119934161\n",
      "  (9, 4670)\t0.0467750411956\n",
      "  (9, 4852)\t0.0528746688396\n",
      "  (9, 247)\t0.0613781399834\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tf[0,:])\n",
    "print(X_train_tf[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First basic model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define and fit in one line\n",
    "clf = MultinomialNB().fit(X_train_tf, twenty_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test:  0.798934753662\n"
     ]
    }
   ],
   "source": [
    "#Score test data\n",
    "\n",
    "# Read test data\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                 remove=('headers', 'footers', 'quotes'),\n",
    "                 categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Transform text to counts\n",
    "X_test_counts = tf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tf-idf transformation\n",
    "X_test_tf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "# Prediction\n",
    "predicted = clf.predict(X_test_tf)\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy test: ', accuracy_score(twenty_test.target, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# Score 2 new docs\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "\n",
    "X_new_counts = tf_vectorizer.transform(docs_new)\n",
    "\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.95, max_features=5000, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "    ...False,\n",
       "         use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define the pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "# Fit all the pipeline\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79893475366178424"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate test data\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                    remove=('headers', 'footers', 'quotes'),\n",
    "                    categories=categories, \n",
    "                    shuffle=True, random_state=42)\n",
    "\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted == twenty_test.target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change classifier in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80692410119840208"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42)),\n",
    "                    ])\n",
    "#Fit\n",
    "_ = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# Predict\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "\n",
    "# Evaluate accuracy\n",
    "np.mean(predicted == twenty_test.target)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80892143808255657"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', svm.LinearSVC()),\n",
    "                    ])\n",
    "\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define estimator. No parameters of the search\n",
    "clf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', svm.LinearSVC()),\n",
    "                ])\n",
    "\n",
    "# Specify parameters and distributions to sample from\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_dist = {\"vect__max_features\": [1000, 2500, 5000, 7500, 10000, None], \n",
    "              \"vect__stop_words\": ['english', None], \n",
    "              \"clf__C\": [.1, .5, 1., 1.5, 2.]}\n",
    "\n",
    "# Define randomized search\n",
    "n_iter_search = 10\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search)\n",
    "\n",
    "# Run the randomized search\n",
    "random_search.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>param_vect__stop_words</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.435259</td>\n",
       "      <td>0.186195</td>\n",
       "      <td>0.834293</td>\n",
       "      <td>0.978734</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>english</td>\n",
       "      <td>{u'clf__C': 0.5, u'vect__max_features': 2500, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.841965</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.828685</td>\n",
       "      <td>0.980718</td>\n",
       "      <td>0.832224</td>\n",
       "      <td>0.976760</td>\n",
       "      <td>0.008963</td>\n",
       "      <td>0.010124</td>\n",
       "      <td>0.005617</td>\n",
       "      <td>0.001616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.433495</td>\n",
       "      <td>0.190499</td>\n",
       "      <td>0.781568</td>\n",
       "      <td>0.899203</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 0.1, u'vect__max_features': 1000, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.895612</td>\n",
       "      <td>0.772908</td>\n",
       "      <td>0.902926</td>\n",
       "      <td>0.766977</td>\n",
       "      <td>0.899070</td>\n",
       "      <td>0.009889</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.002987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.460868</td>\n",
       "      <td>0.182298</td>\n",
       "      <td>0.859548</td>\n",
       "      <td>0.982499</td>\n",
       "      <td>1</td>\n",
       "      <td>7500</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 1.0, u'vect__max_features': 7500, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.869854</td>\n",
       "      <td>0.982048</td>\n",
       "      <td>0.847278</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.861518</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.010476</td>\n",
       "      <td>0.009325</td>\n",
       "      <td>0.001122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.441299</td>\n",
       "      <td>0.188977</td>\n",
       "      <td>0.782898</td>\n",
       "      <td>0.974302</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>english</td>\n",
       "      <td>{u'clf__C': 1.5, u'vect__max_features': 1000, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.796813</td>\n",
       "      <td>0.972739</td>\n",
       "      <td>0.786189</td>\n",
       "      <td>0.975399</td>\n",
       "      <td>0.765646</td>\n",
       "      <td>0.974768</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>0.012932</td>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.457468</td>\n",
       "      <td>0.189954</td>\n",
       "      <td>0.787328</td>\n",
       "      <td>0.977182</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 1.5, u'vect__max_features': 1000, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.802125</td>\n",
       "      <td>0.975399</td>\n",
       "      <td>0.791501</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.768309</td>\n",
       "      <td>0.977424</td>\n",
       "      <td>0.006002</td>\n",
       "      <td>0.010357</td>\n",
       "      <td>0.014114</td>\n",
       "      <td>0.001368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.484521</td>\n",
       "      <td>0.184520</td>\n",
       "      <td>0.861320</td>\n",
       "      <td>0.982499</td>\n",
       "      <td>1.5</td>\n",
       "      <td>10000</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 1.5, u'vect__max_features': 10000,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869854</td>\n",
       "      <td>0.982048</td>\n",
       "      <td>0.849934</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.864181</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>0.009678</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.001122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.454389</td>\n",
       "      <td>0.191902</td>\n",
       "      <td>0.792645</td>\n",
       "      <td>0.970978</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 1.0, u'vect__max_features': 1000, ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.800797</td>\n",
       "      <td>0.968085</td>\n",
       "      <td>0.799469</td>\n",
       "      <td>0.972074</td>\n",
       "      <td>0.777630</td>\n",
       "      <td>0.972776</td>\n",
       "      <td>0.006120</td>\n",
       "      <td>0.010315</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>0.002066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.458575</td>\n",
       "      <td>0.183262</td>\n",
       "      <td>0.860434</td>\n",
       "      <td>0.981170</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 0.5, u'vect__max_features': None, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.868526</td>\n",
       "      <td>0.980053</td>\n",
       "      <td>0.856574</td>\n",
       "      <td>0.983378</td>\n",
       "      <td>0.856192</td>\n",
       "      <td>0.980080</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>0.001561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.450393</td>\n",
       "      <td>0.183927</td>\n",
       "      <td>0.828977</td>\n",
       "      <td>0.951043</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 0.1, u'vect__max_features': None, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.843293</td>\n",
       "      <td>0.954122</td>\n",
       "      <td>0.816733</td>\n",
       "      <td>0.951463</td>\n",
       "      <td>0.826897</td>\n",
       "      <td>0.947543</td>\n",
       "      <td>0.007579</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>0.010947</td>\n",
       "      <td>0.002702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.490448</td>\n",
       "      <td>0.183910</td>\n",
       "      <td>0.856890</td>\n",
       "      <td>0.982499</td>\n",
       "      <td>1.5</td>\n",
       "      <td>7500</td>\n",
       "      <td>None</td>\n",
       "      <td>{u'clf__C': 1.5, u'vect__max_features': 7500, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.864542</td>\n",
       "      <td>0.982048</td>\n",
       "      <td>0.844622</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>0.861518</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>0.009760</td>\n",
       "      <td>0.010077</td>\n",
       "      <td>0.008768</td>\n",
       "      <td>0.001122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.435259         0.186195         0.834293          0.978734   \n",
       "1       0.433495         0.190499         0.781568          0.899203   \n",
       "2       0.460868         0.182298         0.859548          0.982499   \n",
       "3       0.441299         0.188977         0.782898          0.974302   \n",
       "4       0.457468         0.189954         0.787328          0.977182   \n",
       "5       0.484521         0.184520         0.861320          0.982499   \n",
       "6       0.454389         0.191902         0.792645          0.970978   \n",
       "7       0.458575         0.183262         0.860434          0.981170   \n",
       "8       0.450393         0.183927         0.828977          0.951043   \n",
       "9       0.490448         0.183910         0.856890          0.982499   \n",
       "\n",
       "  param_clf__C param_vect__max_features param_vect__stop_words  \\\n",
       "0          0.5                     2500                english   \n",
       "1          0.1                     1000                   None   \n",
       "2            1                     7500                   None   \n",
       "3          1.5                     1000                english   \n",
       "4          1.5                     1000                   None   \n",
       "5          1.5                    10000                   None   \n",
       "6            1                     1000                   None   \n",
       "7          0.5                     None                   None   \n",
       "8          0.1                     None                   None   \n",
       "9          1.5                     7500                   None   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {u'clf__C': 0.5, u'vect__max_features': 2500, ...                5   \n",
       "1  {u'clf__C': 0.1, u'vect__max_features': 1000, ...               10   \n",
       "2  {u'clf__C': 1.0, u'vect__max_features': 7500, ...                3   \n",
       "3  {u'clf__C': 1.5, u'vect__max_features': 1000, ...                9   \n",
       "4  {u'clf__C': 1.5, u'vect__max_features': 1000, ...                8   \n",
       "5  {u'clf__C': 1.5, u'vect__max_features': 10000,...                1   \n",
       "6  {u'clf__C': 1.0, u'vect__max_features': 1000, ...                7   \n",
       "7  {u'clf__C': 0.5, u'vect__max_features': None, ...                2   \n",
       "8  {u'clf__C': 0.1, u'vect__max_features': None, ...                6   \n",
       "9  {u'clf__C': 1.5, u'vect__max_features': 7500, ...                4   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.841965            0.978723           0.828685   \n",
       "1           0.804781            0.895612           0.772908   \n",
       "2           0.869854            0.982048           0.847278   \n",
       "3           0.796813            0.972739           0.786189   \n",
       "4           0.802125            0.975399           0.791501   \n",
       "5           0.869854            0.982048           0.849934   \n",
       "6           0.800797            0.968085           0.799469   \n",
       "7           0.868526            0.980053           0.856574   \n",
       "8           0.843293            0.954122           0.816733   \n",
       "9           0.864542            0.982048           0.844622   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.980718           0.832224            0.976760      0.008963   \n",
       "1            0.902926           0.766977            0.899070      0.009889   \n",
       "2            0.984043           0.861518            0.981408      0.003911   \n",
       "3            0.975399           0.765646            0.974768      0.008202   \n",
       "4            0.978723           0.768309            0.977424      0.006002   \n",
       "5            0.984043           0.864181            0.981408      0.008559   \n",
       "6            0.972074           0.777630            0.972776      0.006120   \n",
       "7            0.983378           0.856192            0.980080      0.009978   \n",
       "8            0.951463           0.826897            0.947543      0.007579   \n",
       "9            0.984043           0.861518            0.981408      0.009760   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.010124        0.005617         0.001616  \n",
       "1        0.011081        0.016602         0.002987  \n",
       "2        0.010476        0.009325         0.001122  \n",
       "3        0.010658        0.012932         0.001135  \n",
       "4        0.010357        0.014114         0.001368  \n",
       "5        0.009678        0.008383         0.001122  \n",
       "6        0.010315        0.010617         0.002066  \n",
       "7        0.009732        0.005728         0.001561  \n",
       "8        0.009494        0.010947         0.002702  \n",
       "9        0.010077        0.008768         0.001122  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dictionary of search results to a Pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df_cv_results = pd.DataFrame.from_dict(random_search.cv_results_)\n",
    "df_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81424766977363516"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score & evaluate test data using the best estimator\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=10000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', svm.LinearSVC(C=1.5)),\n",
    "                    ])\n",
    "\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional metrics for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.76      0.61      0.68       319\n",
      "         comp.graphics       0.82      0.92      0.87       389\n",
      "               sci.med       0.88      0.85      0.86       396\n",
      "soc.religion.christian       0.78      0.84      0.81       398\n",
      "\n",
      "           avg / total       0.81      0.81      0.81      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(twenty_test.target, \n",
    "                                    predicted,\n",
    "                                    target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[196,  22,  24,  77],\n",
       "       [ 16, 356,  14,   3],\n",
       "       [ 14,  32, 337,  13],\n",
       "       [ 32,  22,  10, 334]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
