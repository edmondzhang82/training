{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn version: 0.18\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "print('Sklearn version:', sklearn_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                 remove=('headers', 'footers', 'quotes'),\n",
    "                 categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "---------------\n",
      "Target:  1\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(twenty_train.data[0])\n",
    "print('---------------')\n",
    "print('Target: ', twenty_train.target[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 5000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text preprocessing, tokenizing and filtering of stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=5000,\n",
    "                                stop_words='english')\n",
    "X_train_counts = tf_vectorizer.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2866)\t1\n",
      "  (0, 238)\t1\n",
      "  (0, 4522)\t1\n",
      "  (0, 2058)\t1\n",
      "  (0, 1123)\t1\n",
      "  (0, 3867)\t1\n",
      "  (0, 1543)\t1\n",
      "  (0, 3385)\t1\n",
      "  (0, 2197)\t1\n",
      "  (0, 1094)\t1\n",
      "  (0, 2643)\t1\n",
      "  (0, 1865)\t1\n",
      "  (0, 2237)\t1\n",
      "  (0, 1795)\t2\n",
      "  (0, 4520)\t1\n",
      "  (0, 2251)\t1\n",
      "  (0, 1090)\t1\n",
      "  (0, 4744)\t1\n",
      "  (0, 3276)\t1\n",
      "  (0, 357)\t1\n",
      "  (0, 3273)\t1\n",
      "  (0, 4299)\t1\n",
      "  (0, 4869)\t1\n",
      "  (0, 2014)\t1\n",
      "  (0, 2550)\t1\n",
      "  (0, 1445)\t1\n",
      "  (232, 0)\t2\n",
      "  (272, 0)\t1\n",
      "  (282, 0)\t1\n",
      "  (400, 0)\t1\n",
      "  (433, 0)\t2\n",
      "  (581, 0)\t2\n",
      "  (588, 0)\t1\n",
      "  (766, 0)\t1\n",
      "  (768, 0)\t2\n",
      "  (837, 0)\t3\n",
      "  (844, 0)\t1\n",
      "  (859, 0)\t1\n",
      "  (880, 0)\t1\n",
      "  (1030, 0)\t1\n",
      "  (1056, 0)\t6\n",
      "  (1057, 0)\t2\n",
      "  (1263, 0)\t1\n",
      "  (1475, 0)\t1\n",
      "  (1665, 0)\t16\n",
      "  (1795, 0)\t1\n",
      "  (1802, 0)\t1\n",
      "  (1833, 0)\t1\n",
      "  (1890, 0)\t2\n",
      "  (2069, 0)\t1\n",
      "  (2144, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts[0,:])\n",
    "print(X_train_counts[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 5000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From occurrences to frequencies\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tf = tfidf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1445)\t0.0998496101737\n",
      "  (0, 2550)\t0.0920875619201\n",
      "  (0, 2014)\t0.10905059472\n",
      "  (0, 4869)\t0.112409159775\n",
      "  (0, 4299)\t0.172232378831\n",
      "  (0, 3273)\t0.189497984618\n",
      "  (0, 357)\t0.196147304589\n",
      "  (0, 3276)\t0.239358101611\n",
      "  (0, 4744)\t0.242697172074\n",
      "  (0, 1090)\t0.185367646905\n",
      "  (0, 2251)\t0.281517460204\n",
      "  (0, 4520)\t0.239358101611\n",
      "  (0, 1795)\t0.326673936513\n",
      "  (0, 2237)\t0.217882788689\n",
      "  (0, 1865)\t0.182356290661\n",
      "  (0, 2643)\t0.0944312658437\n",
      "  (0, 1094)\t0.250397930473\n",
      "  (0, 2197)\t0.225991796704\n",
      "  (0, 3385)\t0.272954303671\n",
      "  (0, 1543)\t0.163780615995\n",
      "  (0, 3867)\t0.165608347231\n",
      "  (0, 1123)\t0.157610927262\n",
      "  (0, 2058)\t0.144807482284\n",
      "  (0, 4522)\t0.126533637604\n",
      "  (0, 238)\t0.170069829145\n",
      "  (0, 2866)\t0.190380209723\n",
      "  (232, 0)\t0.162673301572\n",
      "  (272, 0)\t0.0396045882998\n",
      "  (282, 0)\t0.0830143471237\n",
      "  (400, 0)\t0.00527736458963\n",
      "  (433, 0)\t0.00596499373539\n",
      "  (581, 0)\t0.150704657006\n",
      "  (588, 0)\t0.154296833105\n",
      "  (766, 0)\t0.126956846998\n",
      "  (768, 0)\t0.0117078298784\n",
      "  (837, 0)\t0.334685959895\n",
      "  (844, 0)\t0.207167396703\n",
      "  (859, 0)\t0.216506134034\n",
      "  (880, 0)\t0.0133502362916\n",
      "  (1030, 0)\t0.278741714593\n",
      "  (1056, 0)\t0.212612262278\n",
      "  (1057, 0)\t0.139865527738\n",
      "  (1263, 0)\t0.0889107355711\n",
      "  (1475, 0)\t0.275148224162\n",
      "  (1665, 0)\t0.22386057664\n",
      "  (1795, 0)\t0.0911799104224\n",
      "  (1802, 0)\t0.0165319212251\n",
      "  (1833, 0)\t0.11551299395\n",
      "  (1890, 0)\t0.0079616312192\n",
      "  (2069, 0)\t0.10844175005\n",
      "  (2144, 0)\t0.114983457384\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tf[0,:])\n",
    "print(X_train_tf[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First basic model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define and fit in one line\n",
    "clf = MultinomialNB().fit(X_train_tf, twenty_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test:  0.798934753662\n"
     ]
    }
   ],
   "source": [
    "#Score test data\n",
    "\n",
    "# Read test data\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                 remove=('headers', 'footers', 'quotes'),\n",
    "                 categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Transform text to counts\n",
    "X_test_counts = tf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tf-idf transformation\n",
    "X_test_tf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "# Prediction\n",
    "predicted = clf.predict(X_test_tf)\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy test: ', accuracy_score(twenty_test.target, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# Score 2 new docs\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "\n",
    "X_new_counts = tf_vectorizer.transform(docs_new)\n",
    "\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.95, max_features=5000, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "    ...False,\n",
       "         use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define the pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "# Fit all the pipeline\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79893475366178424"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate test data\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                    remove=('headers', 'footers', 'quotes'),\n",
    "                    categories=categories, \n",
    "                    shuffle=True, random_state=42)\n",
    "\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted == twenty_test.target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change classifier in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80692410119840208"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42)),\n",
    "                    ])\n",
    "#Fit\n",
    "_ = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# Predict\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "\n",
    "# Evaluate accuracy\n",
    "np.mean(predicted == twenty_test.target)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80892143808255657"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', svm.LinearSVC()),\n",
    "                    ])\n",
    "\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define estimator. No parameters of the search\n",
    "clf = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', svm.LinearSVC()),\n",
    "                ])\n",
    "\n",
    "# Specify parameters and distributions to sample from\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_dist = {\"vect__max_features\": [1000, 2500, 5000, 7500, 10000, None], \n",
    "              \"vect__stop_words\": ['english', None], \n",
    "              \"clf__C\": [.1, .5, 1., 1.5, 2.]}\n",
    "\n",
    "# Define randomized search\n",
    "n_iter_search = 10\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search)\n",
    "\n",
    "# Run the randomized search\n",
    "random_search.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>param_vect__stop_words</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0.332008</td>\n",
       "      <td> 0.134505</td>\n",
       "      <td> 0.781568</td>\n",
       "      <td> 0.899203</td>\n",
       "      <td> 0.1</td>\n",
       "      <td> 1000</td>\n",
       "      <td>    None</td>\n",
       "      <td> {u'clf__C': 0.1, u'vect__max_features': 1000, ...</td>\n",
       "      <td> 10</td>\n",
       "      <td> 0.804781</td>\n",
       "      <td> 0.895612</td>\n",
       "      <td> 0.772908</td>\n",
       "      <td> 0.902926</td>\n",
       "      <td> 0.766977</td>\n",
       "      <td> 0.899070</td>\n",
       "      <td> 0.008767</td>\n",
       "      <td> 0.006814</td>\n",
       "      <td> 0.016602</td>\n",
       "      <td> 0.002987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 0.331291</td>\n",
       "      <td> 0.134825</td>\n",
       "      <td> 0.848914</td>\n",
       "      <td> 0.961232</td>\n",
       "      <td> 0.1</td>\n",
       "      <td> 5000</td>\n",
       "      <td> english</td>\n",
       "      <td> {u'clf__C': 0.1, u'vect__max_features': 5000, ...</td>\n",
       "      <td>  5</td>\n",
       "      <td> 0.857902</td>\n",
       "      <td> 0.960771</td>\n",
       "      <td> 0.844622</td>\n",
       "      <td> 0.962101</td>\n",
       "      <td> 0.844208</td>\n",
       "      <td> 0.960823</td>\n",
       "      <td> 0.007606</td>\n",
       "      <td> 0.007903</td>\n",
       "      <td> 0.006361</td>\n",
       "      <td> 0.000615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 0.322758</td>\n",
       "      <td> 0.132899</td>\n",
       "      <td> 0.809039</td>\n",
       "      <td> 0.917812</td>\n",
       "      <td> 0.1</td>\n",
       "      <td> 1000</td>\n",
       "      <td> english</td>\n",
       "      <td> {u'clf__C': 0.1, u'vect__max_features': 1000, ...</td>\n",
       "      <td>  9</td>\n",
       "      <td> 0.818061</td>\n",
       "      <td> 0.916888</td>\n",
       "      <td> 0.806109</td>\n",
       "      <td> 0.919548</td>\n",
       "      <td> 0.802929</td>\n",
       "      <td> 0.916999</td>\n",
       "      <td> 0.008422</td>\n",
       "      <td> 0.007077</td>\n",
       "      <td> 0.006515</td>\n",
       "      <td> 0.001229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 0.368327</td>\n",
       "      <td> 0.134324</td>\n",
       "      <td> 0.844041</td>\n",
       "      <td> 0.982499</td>\n",
       "      <td> 1.5</td>\n",
       "      <td> 5000</td>\n",
       "      <td>    None</td>\n",
       "      <td> {u'clf__C': 1.5, u'vect__max_features': 5000, ...</td>\n",
       "      <td>  6</td>\n",
       "      <td> 0.848606</td>\n",
       "      <td> 0.982048</td>\n",
       "      <td> 0.833997</td>\n",
       "      <td> 0.984043</td>\n",
       "      <td> 0.849534</td>\n",
       "      <td> 0.981408</td>\n",
       "      <td> 0.005597</td>\n",
       "      <td> 0.006284</td>\n",
       "      <td> 0.007117</td>\n",
       "      <td> 0.001122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 0.328613</td>\n",
       "      <td> 0.133779</td>\n",
       "      <td> 0.836952</td>\n",
       "      <td> 0.950599</td>\n",
       "      <td> 0.1</td>\n",
       "      <td> 2500</td>\n",
       "      <td> english</td>\n",
       "      <td> {u'clf__C': 0.1, u'vect__max_features': 2500, ...</td>\n",
       "      <td>  7</td>\n",
       "      <td> 0.843293</td>\n",
       "      <td> 0.950133</td>\n",
       "      <td> 0.837981</td>\n",
       "      <td> 0.952128</td>\n",
       "      <td> 0.829561</td>\n",
       "      <td> 0.949535</td>\n",
       "      <td> 0.010776</td>\n",
       "      <td> 0.007290</td>\n",
       "      <td> 0.005652</td>\n",
       "      <td> 0.001108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td> 0.349612</td>\n",
       "      <td> 0.134311</td>\n",
       "      <td> 0.853345</td>\n",
       "      <td> 0.980505</td>\n",
       "      <td> 0.5</td>\n",
       "      <td> 5000</td>\n",
       "      <td>    None</td>\n",
       "      <td> {u'clf__C': 0.5, u'vect__max_features': 5000, ...</td>\n",
       "      <td>  3</td>\n",
       "      <td> 0.861886</td>\n",
       "      <td> 0.979388</td>\n",
       "      <td> 0.841965</td>\n",
       "      <td> 0.982048</td>\n",
       "      <td> 0.856192</td>\n",
       "      <td> 0.980080</td>\n",
       "      <td> 0.006864</td>\n",
       "      <td> 0.005967</td>\n",
       "      <td> 0.008381</td>\n",
       "      <td> 0.001127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td> 0.336235</td>\n",
       "      <td> 0.134724</td>\n",
       "      <td> 0.825875</td>\n",
       "      <td> 0.944619</td>\n",
       "      <td> 0.1</td>\n",
       "      <td> 5000</td>\n",
       "      <td>    None</td>\n",
       "      <td> {u'clf__C': 0.1, u'vect__max_features': 5000, ...</td>\n",
       "      <td>  8</td>\n",
       "      <td> 0.832669</td>\n",
       "      <td> 0.948803</td>\n",
       "      <td> 0.819389</td>\n",
       "      <td> 0.945479</td>\n",
       "      <td> 0.825566</td>\n",
       "      <td> 0.939575</td>\n",
       "      <td> 0.008222</td>\n",
       "      <td> 0.007257</td>\n",
       "      <td> 0.005428</td>\n",
       "      <td> 0.003816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td> 0.365994</td>\n",
       "      <td> 0.135015</td>\n",
       "      <td> 0.860877</td>\n",
       "      <td> 0.983386</td>\n",
       "      <td>   2</td>\n",
       "      <td> 7500</td>\n",
       "      <td> english</td>\n",
       "      <td> {u'clf__C': 2.0, u'vect__max_features': 7500, ...</td>\n",
       "      <td>  1</td>\n",
       "      <td> 0.871182</td>\n",
       "      <td> 0.984043</td>\n",
       "      <td> 0.852590</td>\n",
       "      <td> 0.984043</td>\n",
       "      <td> 0.858855</td>\n",
       "      <td> 0.982072</td>\n",
       "      <td> 0.004418</td>\n",
       "      <td> 0.007191</td>\n",
       "      <td> 0.007727</td>\n",
       "      <td> 0.000929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td> 0.334366</td>\n",
       "      <td> 0.136236</td>\n",
       "      <td> 0.855117</td>\n",
       "      <td> 0.964777</td>\n",
       "      <td> 0.1</td>\n",
       "      <td> None</td>\n",
       "      <td> english</td>\n",
       "      <td> {u'clf__C': 0.1, u'vect__max_features': None, ...</td>\n",
       "      <td>  2</td>\n",
       "      <td> 0.860558</td>\n",
       "      <td> 0.964096</td>\n",
       "      <td> 0.855246</td>\n",
       "      <td> 0.966090</td>\n",
       "      <td> 0.849534</td>\n",
       "      <td> 0.964143</td>\n",
       "      <td> 0.006585</td>\n",
       "      <td> 0.007077</td>\n",
       "      <td> 0.004500</td>\n",
       "      <td> 0.000929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td> 0.333563</td>\n",
       "      <td> 0.135127</td>\n",
       "      <td> 0.852902</td>\n",
       "      <td> 0.964777</td>\n",
       "      <td> 0.1</td>\n",
       "      <td> 7500</td>\n",
       "      <td> english</td>\n",
       "      <td> {u'clf__C': 0.1, u'vect__max_features': 7500, ...</td>\n",
       "      <td>  4</td>\n",
       "      <td> 0.861886</td>\n",
       "      <td> 0.964761</td>\n",
       "      <td> 0.851262</td>\n",
       "      <td> 0.965426</td>\n",
       "      <td> 0.845539</td>\n",
       "      <td> 0.964143</td>\n",
       "      <td> 0.009318</td>\n",
       "      <td> 0.006686</td>\n",
       "      <td> 0.006772</td>\n",
       "      <td> 0.000524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.332008         0.134505         0.781568          0.899203   \n",
       "1       0.331291         0.134825         0.848914          0.961232   \n",
       "2       0.322758         0.132899         0.809039          0.917812   \n",
       "3       0.368327         0.134324         0.844041          0.982499   \n",
       "4       0.328613         0.133779         0.836952          0.950599   \n",
       "5       0.349612         0.134311         0.853345          0.980505   \n",
       "6       0.336235         0.134724         0.825875          0.944619   \n",
       "7       0.365994         0.135015         0.860877          0.983386   \n",
       "8       0.334366         0.136236         0.855117          0.964777   \n",
       "9       0.333563         0.135127         0.852902          0.964777   \n",
       "\n",
       "  param_clf__C param_vect__max_features param_vect__stop_words  \\\n",
       "0          0.1                     1000                   None   \n",
       "1          0.1                     5000                english   \n",
       "2          0.1                     1000                english   \n",
       "3          1.5                     5000                   None   \n",
       "4          0.1                     2500                english   \n",
       "5          0.5                     5000                   None   \n",
       "6          0.1                     5000                   None   \n",
       "7            2                     7500                english   \n",
       "8          0.1                     None                english   \n",
       "9          0.1                     7500                english   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {u'clf__C': 0.1, u'vect__max_features': 1000, ...               10   \n",
       "1  {u'clf__C': 0.1, u'vect__max_features': 5000, ...                5   \n",
       "2  {u'clf__C': 0.1, u'vect__max_features': 1000, ...                9   \n",
       "3  {u'clf__C': 1.5, u'vect__max_features': 5000, ...                6   \n",
       "4  {u'clf__C': 0.1, u'vect__max_features': 2500, ...                7   \n",
       "5  {u'clf__C': 0.5, u'vect__max_features': 5000, ...                3   \n",
       "6  {u'clf__C': 0.1, u'vect__max_features': 5000, ...                8   \n",
       "7  {u'clf__C': 2.0, u'vect__max_features': 7500, ...                1   \n",
       "8  {u'clf__C': 0.1, u'vect__max_features': None, ...                2   \n",
       "9  {u'clf__C': 0.1, u'vect__max_features': 7500, ...                4   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.804781            0.895612           0.772908   \n",
       "1           0.857902            0.960771           0.844622   \n",
       "2           0.818061            0.916888           0.806109   \n",
       "3           0.848606            0.982048           0.833997   \n",
       "4           0.843293            0.950133           0.837981   \n",
       "5           0.861886            0.979388           0.841965   \n",
       "6           0.832669            0.948803           0.819389   \n",
       "7           0.871182            0.984043           0.852590   \n",
       "8           0.860558            0.964096           0.855246   \n",
       "9           0.861886            0.964761           0.851262   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.902926           0.766977            0.899070      0.008767   \n",
       "1            0.962101           0.844208            0.960823      0.007606   \n",
       "2            0.919548           0.802929            0.916999      0.008422   \n",
       "3            0.984043           0.849534            0.981408      0.005597   \n",
       "4            0.952128           0.829561            0.949535      0.010776   \n",
       "5            0.982048           0.856192            0.980080      0.006864   \n",
       "6            0.945479           0.825566            0.939575      0.008222   \n",
       "7            0.984043           0.858855            0.982072      0.004418   \n",
       "8            0.966090           0.849534            0.964143      0.006585   \n",
       "9            0.965426           0.845539            0.964143      0.009318   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.006814        0.016602         0.002987  \n",
       "1        0.007903        0.006361         0.000615  \n",
       "2        0.007077        0.006515         0.001229  \n",
       "3        0.006284        0.007117         0.001122  \n",
       "4        0.007290        0.005652         0.001108  \n",
       "5        0.005967        0.008381         0.001127  \n",
       "6        0.007257        0.005428         0.003816  \n",
       "7        0.007191        0.007727         0.000929  \n",
       "8        0.007077        0.004500         0.000929  \n",
       "9        0.006686        0.006772         0.000524  \n",
       "\n",
       "[10 rows x 19 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dictionary of search results to a Pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df_cv_results = pd.DataFrame.from_dict(random_search.cv_results_)\n",
    "df_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81424766977363516"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score & evaluate test data using the best estimator\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(max_df=0.95, min_df=2, max_features=10000, stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', svm.LinearSVC(C=1.5)),\n",
    "                    ])\n",
    "\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional metrics for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.76      0.61      0.68       319\n",
      "         comp.graphics       0.82      0.92      0.87       389\n",
      "               sci.med       0.88      0.85      0.86       396\n",
      "soc.religion.christian       0.78      0.84      0.81       398\n",
      "\n",
      "           avg / total       0.81      0.81      0.81      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(twenty_test.target, \n",
    "                                    predicted,\n",
    "                                    target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[196,  22,  24,  77],\n",
       "       [ 16, 356,  14,   3],\n",
       "       [ 14,  32, 337,  13],\n",
       "       [ 32,  22,  10, 334]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
