{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "    - Download data in the server\n",
    "    - Convert test to sequences.\n",
    "    - Configure sequences for a RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data in the server\n",
    "\n",
    "### Command line in the server\n",
    "    Path to data:\n",
    "        cd /home/ubuntu/data/training/keras\n",
    "    Download dataset: \n",
    "        wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    Uncompress it:\n",
    "        tar -zxvf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convert test to sequences\n",
    "    - List of all text files\n",
    "    - Read files into python\n",
    "    - Tokenize\n",
    "    - Create dictionaries to recode\n",
    "    - Recode tokens into ids and create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports and paths\n",
    "import numpy as np\n",
    "\n",
    "data_path='/home/ubuntu/data/training/keras/aclImdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jorge/data/training/keras/aclImdb/train/pos/8938_9.txt\n"
     ]
    }
   ],
   "source": [
    "# Generator of list of files in a folder and subfolders\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "def gen_find(filepattern, toppath):\n",
    "    '''\n",
    "    Generator with a recursive list of files in the toppath that match filepattern \n",
    "    Inputs:\n",
    "        filepattern(str): Command stype pattern \n",
    "        toppath(str): Root path\n",
    "    '''\n",
    "    for path, dirlist, filelist in os.walk(toppath):\n",
    "        for name in fnmatch.filter(filelist, filepattern):\n",
    "            yield os.path.join(path, name)\n",
    "\n",
    "#Test\n",
    "print(gen_find(\"*.txt\", data_path+'train/pos/').next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A wonderful film version of the best-selling book and smash Broadway play about the lives of Sadie and Bessie Delany, two African-American sisters who both lived over the age of 100 and told their story of witnessing a century of American history. Ruby Dee and Diahann Carroll give very good performances as Bessie and Sadie, respectively. Amy Madigan also is good as Amy Hill Hearth, the white New York Times reporter whose article about the sisters launched the book, etc. Many of the flashback scenes and even many of the present-day ones are very powerful, if not quite as inspirational as in the book. That is the only real drawback, combined with the fact that certain aspects of the story are not presented clearly, such as the inter-racial background of the sisters' mother and why their father was so stern. But other than that, a very well-done, excellently performed, powerful movie.\", \"Now this is what I'd call a good horror. With occult/supernatural undertones, this nice low-budget French movie caught my attention from the very first scene. This proves you don't need wild FX or lots of gore to make an effective horror movie.<br /><br />The plot revolves around 4 cellmates in a prison, and each of these characters (and their motives) become gradually more interesting, as the movie builds up tension to the finale. Most of the action we see through the eyes of Carrere, who has just entered prison and has to get used to living with these 3 other inmates.<br /><br />I won't say much because this movie really deserves to be more widely seen. There a few flaws though: the FX are not that good, but they're used effectively; the plot leaves some mysteries open; and things get very confusing towards the end, but Malefique redeems itself by the time it's over.<br /><br />I thought his was a very good movie, 8/10\"]\n"
     ]
    }
   ],
   "source": [
    "def read_sentences(path):\n",
    "    sentences = []\n",
    "    sentences_list = gen_find(\"*.txt\", path)\n",
    "    for ff in sentences_list:\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    return sentences        \n",
    "\n",
    "#Test\n",
    "print(read_sentences(data_path+'train/pos/')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Bobby is a goofy kid who smiles far too much and wants sex. So he buys a van to aid in this quest. The acting is lame, the comedy is pathetic and the script is no more than a loosely strung chain of clich\\xc3\\xa9s and cheap thrills. The makers of the film obviously wanted to capture some of the out there craziness of other films of the time, but fell a long way short. They even resort to Bobby slipping on a banana skin, because this will supposedly add comedic value.<br /><br />I'm struggling to find a redeeming feature of the film. If you like DeVito, this is another classic DeVito kind of role - but he's only a supporting actor and there for clich\\xc3\\xa9 value.\", \"It's the worst movie I've ever seen. The action is so unclear, work of cameras is so poor, actors are so affected ... and this lamentable 5 minutes of Arnie on the screen. My advice from the bottom of my heart - don't watch it unless you like such a low class torture.\"]\n"
     ]
    }
   ],
   "source": [
    "print(read_sentences(data_path+'train/neg/')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing... Done!\n",
      "[[u'A', u'wonderful', u'film', u'version', u'of', u'the', u'best-selling', u'book', u'and', u'smash', u'Broadway', u'play', u'about', u'the', u'lives', u'of', u'Sadie', u'and', u'Bessie', u'Delany', u',', u'two', u'African-American', u'sisters', u'who', u'both', u'lived', u'over', u'the', u'age', u'of', u'100', u'and', u'told', u'their', u'story', u'of', u'witnessing', u'a', u'century', u'of', u'American', u'history', u'.', u'Ruby', u'Dee', u'and', u'Diahann', u'Carroll', u'give', u'very', u'good', u'performances', u'as', u'Bessie', u'and', u'Sadie', u',', u'respectively', u'.', u'Amy', u'Madigan', u'also', u'is', u'good', u'as', u'Amy', u'Hill', u'Hearth', u',', u'the', u'white', u'New', u'York', u'Times', u'reporter', u'whose', u'article', u'about', u'the', u'sisters', u'launched', u'the', u'book', u',', u'etc', u'.', u'Many', u'of', u'the', u'flashback', u'scenes', u'and', u'even', u'many', u'of', u'the', u'present-day', u'ones', u'are', u'very', u'powerful', u',', u'if', u'not', u'quite', u'as', u'inspirational', u'as', u'in', u'the', u'book', u'.', u'That', u'is', u'the', u'only', u'real', u'drawback', u',', u'combined', u'with', u'the', u'fact', u'that', u'certain', u'aspects', u'of', u'the', u'story', u'are', u'not', u'presented', u'clearly', u',', u'such', u'as', u'the', u'inter-racial', u'background', u'of', u'the', u'sisters', u\"'\", u'mother', u'and', u'why', u'their', u'father', u'was', u'so', u'stern', u'.', u'But', u'other', u'than', u'that', u',', u'a', u'very', u'well-done', u',', u'excellently', u'performed', u',', u'powerful', u'movie', u'.'], [u'Now', u'this', u'is', u'what', u'I', u\"'d\", u'call', u'a', u'good', u'horror', u'.', u'With', u'occult/supernatural', u'undertones', u',', u'this', u'nice', u'low-budget', u'French', u'movie', u'caught', u'my', u'attention', u'from', u'the', u'very', u'first', u'scene', u'.', u'This', u'proves', u'you', u'do', u\"n't\", u'need', u'wild', u'FX', u'or', u'lots', u'of', u'gore', u'to', u'make', u'an', u'effective', u'horror', u'movie.', u'<', u'br', u'/', u'>', u'<', u'br', u'/', u'>', u'The', u'plot', u'revolves', u'around', u'4', u'cellmates', u'in', u'a', u'prison', u',', u'and', u'each', u'of', u'these', u'characters', u'(', u'and', u'their', u'motives', u')', u'become', u'gradually', u'more', u'interesting', u',', u'as', u'the', u'movie', u'builds', u'up', u'tension', u'to', u'the', u'finale', u'.', u'Most', u'of', u'the', u'action', u'we', u'see', u'through', u'the', u'eyes', u'of', u'Carrere', u',', u'who', u'has', u'just', u'entered', u'prison', u'and', u'has', u'to', u'get', u'used', u'to', u'living', u'with', u'these', u'3', u'other', u'inmates.', u'<', u'br', u'/', u'>', u'<', u'br', u'/', u'>', u'I', u'wo', u\"n't\", u'say', u'much', u'because', u'this', u'movie', u'really', u'deserves', u'to', u'be', u'more', u'widely', u'seen', u'.', u'There', u'a', u'few', u'flaws', u'though', u':', u'the', u'FX', u'are', u'not', u'that', u'good', u',', u'but', u'they', u\"'re\", u'used', u'effectively', u';', u'the', u'plot', u'leaves', u'some', u'mysteries', u'open', u';', u'and', u'things', u'get', u'very', u'confusing', u'towards', u'the', u'end', u',', u'but', u'Malefique', u'redeems', u'itself', u'by', u'the', u'time', u'it', u\"'s\", u'over.', u'<', u'br', u'/', u'>', u'<', u'br', u'/', u'>', u'I', u'thought', u'his', u'was', u'a', u'very', u'good', u'movie', u',', u'8/10']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(sentences):\n",
    "    from nltk import word_tokenize\n",
    "    print 'Tokenizing...',\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens += [word_tokenize(sentence.decode('utf-8'))]\n",
    "    print('Done!')\n",
    "\n",
    "    return tokens\n",
    "\n",
    "print(tokenize(read_sentences(data_path+'train/pos/')[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing... Done!\n",
      "Tokenizing... Done!\n"
     ]
    }
   ],
   "source": [
    "sentences_trn_pos = tokenize(read_sentences(data_path+'train/pos/'))\n",
    "sentences_trn_neg = tokenize(read_sentences(data_path+'train/neg/'))\n",
    "sentences_trn = sentences_trn_pos + sentences_trn_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary.. 7056193  total words  135098  unique words\n",
      "(2, 289298)\n"
     ]
    }
   ],
   "source": [
    "#create the dictionary to conver words to numbers. Order it with most frequent words first\n",
    "def build_dict(sentences):\n",
    "#    from collections import OrderedDict\n",
    "\n",
    "    '''\n",
    "    Build dictionary of train words\n",
    "    Outputs: \n",
    "     - Dictionary of word --> word index\n",
    "     - Dictionary of word --> word count freq\n",
    "    '''\n",
    "    print 'Building dictionary..',\n",
    "    wordcount = dict()\n",
    "    #For each worn in each sentence, cummulate frequency\n",
    "    for ss in sentences:\n",
    "        for w in ss:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = wordcount.values() # List of frequencies\n",
    "    keys = wordcount.keys() #List of words\n",
    "    \n",
    "    sorted_idx = reversed(np.argsort(counts))\n",
    "    \n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "    print np.sum(counts), ' total words ', len(keys), ' unique words'\n",
    "\n",
    "    return worddict, wordcount\n",
    "\n",
    "\n",
    "worddict, wordcount = build_dict(sentences_trn)\n",
    "\n",
    "print(worddict['the'], wordcount['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "def generate_sequence(sentences, dictionary):\n",
    "    '''\n",
    "    Convert tokenized text in sequences of integers\n",
    "    '''\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([137, 424, 26, 328, 7, 2, 17914, 302, 5, 8144, 2246, 329, 55, 2, 487, 7, 20032, 5, 23759, 40433, 3, 132, 7118, 2922, 47, 238, 1539, 151, 2, 695, 7, 1320, 5, 600, 82, 80, 7, 8073, 6, 1448, 7, 338, 541, 4, 5092, 5486, 5, 42665, 13131, 223, 65, 63, 368, 22, 23759, 5, 20032, 3, 5633, 4, 4330, 29365, 111, 9, 63, 22, 4330, 3306, 63946, 3, 2, 587, 535, 773, 4487, 2486, 625, 8569, 55, 2, 2922, 9465, 2, 302, 3, 637, 4, 1412, 7, 2, 2794, 162, 5, 83, 131, 7, 2, 21381, 674, 35, 65, 991, 3, 78, 36, 200, 22, 6972, 22, 14, 2, 302, 4, 267, 9, 2, 77, 174, 11709, 3, 2630, 23, 2, 208, 17, 817, 1378, 7, 2, 80, 35, 36, 1353, 737, 3, 163, 22, 2, 45078, 980, 7, 2, 2922, 92, 488, 5, 188, 82, 355, 20, 53, 10488, 4, 118, 102, 93, 17, 3, 6, 65, 6239, 3, 6767, 2671, 3, 991, 25, 4], 1)\n"
     ]
    }
   ],
   "source": [
    "# Create train and test data\n",
    "\n",
    "#Read train sentences and generate target y\n",
    "train_x_pos = generate_sequence(sentences_trn_pos, worddict)\n",
    "train_x_neg = generate_sequence(sentences_trn_neg, worddict)\n",
    "X_train_full = train_x_pos + train_x_neg\n",
    "y_train_full = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "print(X_train_full[0], y_train_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing... Done!\n",
      "Tokenizing... Done!\n",
      "[137, 30688, 3344, 38048, 17, 299, 17, 245, 9, 228, 2, 3379, 7, 1726, 4, 2463, 27811, 18, 501, 9, 7308, 5, 2732, 3, 2, 410, 288, 57, 22, 1522, 222, 22, 16, 89, 247, 6, 2105, 618, 4, 601, 5, 5722, 238, 223, 504, 368, 5, 238, 315, 8, 12882, 2, 593, 7, 18525, 91, 16, 18, 6, 45107, 3344, 38048, 17, 83, 31358, 96, 619, 792, 5, 376, 4, 8431, 2260, 406, 238, 2, 818, 6, 603, 24, 82, 304, 347, 38, 157, 23, 112, 3, 40, 18, 53, 1092, 17, 34, 96, 285, 2, 1069, 5896, 8, 2, 820, 7, 1356, 5, 6033, 33, 2, 110, 1425, 48, 392, 8, 4, 10568, 74276, 9, 1624, 5, 6663, 22, 2269, 57603, 3, 40, 56, 274, 7698, 14, 3096, 17, 40, 697, 430, 8, 194, 55, 2, 3344, 18461, 2797, 8, 19, 231, 3, 6, 208, 17, 178, 38, 1, 257, 45, 2, 64, 1135, 4, 137, 2100, 1061, 410, 4]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Read test sentences and generate target y\n",
    "sentences_tst_pos = read_sentences(data_path+'test/pos/')\n",
    "sentences_tst_neg = read_sentences(data_path+'test/neg/')\n",
    "\n",
    "test_x_pos = generate_sequence(tokenize(sentences_tst_pos), worddict)\n",
    "test_x_neg = generate_sequence(tokenize(sentences_tst_neg), worddict)\n",
    "X_test_full = test_x_pos + test_x_neg\n",
    "y_test_full = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "print(X_test_full[0])\n",
    "print(y_test_full[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure sequences for a RNN model\n",
    "    - Remove words with low frequency\n",
    "    - Truncate / complete sequences to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208.0\n"
     ]
    }
   ],
   "source": [
    "#Median length of sentences\n",
    "print np.median([len(x) for x in X_test_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 50000 # Number of most frequent words selected. the less frequent recode to 0\n",
    "maxlen = 200  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 374, 20, 2, 110, 374, 664, 44, 6287, 13020, 6218, 5, 20, 42, 7, 484, 250, 44, 2, 1216, 801, 18258, 4032, 2583, 452, 2, 3016, 3, 107, 427, 2, 875, 4, 458, 2804, 2485, 3, 838, 5, 3, 8, 6, 3307, 2890, 3, 102, 4061, 3, 89, 3326, 427, 5269, 4769, 22, 28240, 3, 2, 2115, 7, 13020, 6218, 2092, 2562, 33, 2, 875, 4, 4056, 5479, 14246, 5, 18258, 18, 13417, 5164, 3, 19, 9, 200, 4846, 4, 61, 9, 46, 906, 374, 4, 1, 92, 1521, 3, 1700, 46, 92, 2992, 5, 1, 333, 3244, 37, 3358, 148, 4, 118, 16, 18, 161, 322, 175, 4, 4770, 4]\n"
     ]
    }
   ],
   "source": [
    "#Select the most frequent max_features, recode others using 0\n",
    "def remove_features(x):\n",
    "    return [[0 if w >= max_features else w for w in sen] for sen in x]\n",
    "\n",
    "X_train = remove_features(X_train_full)\n",
    "X_test  = remove_features(X_test_full)\n",
    "y_train = y_train_full\n",
    "y_test = y_test_full\n",
    "\n",
    "print(X_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN Black (CNMeM is disabled, cuDNN 5103)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:599: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('X_train shape:', (25000, 200))\n",
      "('X_test shape:', (25000, 200))\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0   137 30688  3344 38048    17   299    17   245     9   228\n",
      "     2  3379     7  1726     4  2463 27811    18   501     9  7308     5\n",
      "  2732     3     2   410   288    57    22  1522   222    22    16    89\n",
      "   247     6  2105   618     4   601     5  5722   238   223   504   368\n",
      "     5   238   315     8 12882     2   593     7 18525    91    16    18\n",
      "     6 45107  3344 38048    17    83 31358    96   619   792     5   376\n",
      "     4  8431  2260   406   238     2   818     6   603    24    82   304\n",
      "   347    38   157    23   112     3    40    18    53  1092    17    34\n",
      "    96   285     2  1069  5896     8     2   820     7  1356     5  6033\n",
      "    33     2   110  1425    48   392     8     4 10568     0     9  1624\n",
      "     5  6663    22  2269     0     3    40    56   274  7698    14  3096\n",
      "    17    40   697   430     8   194    55     2  3344 18461  2797     8\n",
      "    19   231     3     6   208    17   178    38     1   257    45     2\n",
      "    64  1135     4   137  2100  1061   410     4]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Cut or complete the sentences to length = maxlen\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
