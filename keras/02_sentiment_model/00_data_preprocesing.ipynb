{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "    - Download data in the server\n",
    "    - Convert test to sequences.\n",
    "    - Configure sequences for a RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data in the server\n",
    "\n",
    "### Command line in the server\n",
    "    Path to data:\n",
    "        cd /home/ubuntu/data/training/keras\n",
    "    Download dataset: \n",
    "        wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    Uncompress it:\n",
    "        tar -zxvf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convert test to sequences\n",
    "    - List of all text files\n",
    "    - Read files into python\n",
    "    - Tokenize\n",
    "    - Create dictionaries to recode\n",
    "    - Recode tokens into ids and create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports and paths\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_path='/home/ubuntu/data/training/keras/aclImdb/'\n",
    "data_path='/Users/jorge/data/training/keras/aclImdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jorge/data/training/keras/aclImdb/train/pos/0_9.txt\n"
     ]
    }
   ],
   "source": [
    "# Generator of list of files in a folder and subfolders\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "def gen_find(filepattern, toppath):\n",
    "    '''\n",
    "    Generator with a recursive list of files in the toppath that match filepattern \n",
    "    Inputs:\n",
    "        filepattern(str): Command stype pattern \n",
    "        toppath(str): Root path\n",
    "    '''\n",
    "    for path, dirlist, filelist in os.walk(toppath):\n",
    "        for name in fnmatch.filter(filelist, filepattern):\n",
    "            yield os.path.join(path, name)\n",
    "\n",
    "#Test\n",
    "print(gen_find(\"*.txt\", data_path+'train/pos/').next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!', 'Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they\\'ll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it\\'s like to be homeless? That is Goddard Bolt\\'s lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet\\'s on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can\\'t step off the sidewalk. He\\'s given the nickname Pepto by a vagrant after it\\'s written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They\\'re survivors. Bolt isn\\'t. He\\'s not used to reaching mutual agreements like he once did when being rich where it\\'s fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn\\'t necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks\\' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it\\'s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don\\'t know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.']\n"
     ]
    }
   ],
   "source": [
    "def read_sentences(path):\n",
    "    sentences = []\n",
    "    sentences_list = gen_find(\"*.txt\", path)\n",
    "    for ff in sentences_list:\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    return sentences        \n",
    "\n",
    "#Test\n",
    "print(read_sentences(data_path+'train/pos/')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\", \"Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...<br /><br />Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.<br /><br />The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.<br /><br />The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.<br /><br />Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\"]\n"
     ]
    }
   ],
   "source": [
    "print(read_sentences(data_path+'train/neg/')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n",
      "[[u'Bromwell', u'High', u'is', u'a', u'cartoon', u'comedy', u'.', u'It', u'ran', u'at', u'the', u'same', u'time', u'as', u'some', u'other', u'programs', u'about', u'school', u'life', u',', u'such', u'as', u'``', u'Teachers', u\"''\", u'.', u'My', u'35', u'years', u'in', u'the', u'teaching', u'profession', u'lead', u'me', u'to', u'believe', u'that', u'Bromwell', u'High', u\"'s\", u'satire', u'is', u'much', u'closer', u'to', u'reality', u'than', u'is', u'``', u'Teachers', u\"''\", u'.', u'The', u'scramble', u'to', u'survive', u'financially', u',', u'the', u'insightful', u'students', u'who', u'can', u'see', u'right', u'through', u'their', u'pathetic', u'teachers', u\"'\", u'pomp', u',', u'the', u'pettiness', u'of', u'the', u'whole', u'situation', u',', u'all', u'remind', u'me', u'of', u'the', u'schools', u'I', u'knew', u'and', u'their', u'students', u'.', u'When', u'I', u'saw', u'the', u'episode', u'in', u'which', u'a', u'student', u'repeatedly', u'tried', u'to', u'burn', u'down', u'the', u'school', u',', u'I', u'immediately', u'recalled', u'...', u'...', u'...', u'at', u'...', u'...', u'...', u'.', u'High', u'.', u'A', u'classic', u'line', u':', u'INSPECTOR', u':', u'I', u\"'m\", u'here', u'to', u'sack', u'one', u'of', u'your', u'teachers', u'.', u'STUDENT', u':', u'Welcome', u'to', u'Bromwell', u'High', u'.', u'I', u'expect', u'that', u'many', u'adults', u'of', u'my', u'age', u'think', u'that', u'Bromwell', u'High', u'is', u'far', u'fetched', u'.', u'What', u'a', u'pity', u'that', u'it', u'is', u\"n't\", u'!'], [u'Homelessness', u'(', u'or', u'Houselessness', u'as', u'George', u'Carlin', u'stated', u')', u'has', u'been', u'an', u'issue', u'for', u'years', u'but', u'never', u'a', u'plan', u'to', u'help', u'those', u'on', u'the', u'street', u'that', u'were', u'once', u'considered', u'human', u'who', u'did', u'everything', u'from', u'going', u'to', u'school', u',', u'work', u',', u'or', u'vote', u'for', u'the', u'matter', u'.', u'Most', u'people', u'think', u'of', u'the', u'homeless', u'as', u'just', u'a', u'lost', u'cause', u'while', u'worrying', u'about', u'things', u'such', u'as', u'racism', u',', u'the', u'war', u'on', u'Iraq', u',', u'pressuring', u'kids', u'to', u'succeed', u',', u'technology', u',', u'the', u'elections', u',', u'inflation', u',', u'or', u'worrying', u'if', u'they', u\"'ll\", u'be', u'next', u'to', u'end', u'up', u'on', u'the', u'streets.', u'<', u'br', u'/', u'>', u'<', u'br', u'/', u'>', u'But', u'what', u'if', u'you', u'were', u'given', u'a', u'bet', u'to', u'live', u'on', u'the', u'streets', u'for', u'a', u'month', u'without', u'the', u'luxuries', u'you', u'once', u'had', u'from', u'a', u'home', u',', u'the', u'entertainment', u'sets', u',', u'a', u'bathroom', u',', u'pictures', u'on', u'the', u'wall', u',', u'a', u'computer', u',', u'and', u'everything', u'you', u'once', u'treasure', u'to', u'see', u'what', u'it', u\"'s\", u'like', u'to', u'be', u'homeless', u'?', u'That', u'is', u'Goddard', u'Bolt', u\"'s\", u'lesson.', u'<', u'br', u'/', u'>', u'<', u'br', u'/', u'>', u'Mel', u'Brooks', u'(', u'who', u'directs', u')', u'who', u'stars', u'as', u'Bolt', u'plays', u'a', u'rich', u'man', u'who', u'has', u'everything', u'in', u'the', u'world', u'until', u'deciding', u'to', u'make', u'a', u'bet', u'with', u'a', u'sissy', u'rival', u'(', u'Jeffery', u'Tambor', u')', u'to', u'see', u'if', u'he', u'can', u'live', u'in', u'the', u'streets', u'for', u'thirty', u'days', u'without', u'the', u'luxuries', u';', u'if', u'Bolt', u'succeeds', u',', u'he', u'can', u'do', u'what', u'he', u'wants', u'with', u'a', u'future', u'project', u'of', u'making', u'more', u'buildings', u'.', u'The', u'bet', u\"'s\", u'on', u'where', u'Bolt', u'is', u'thrown', u'on', u'the', u'street', u'with', u'a', u'bracelet', u'on', u'his', u'leg', u'to', u'monitor', u'his', u'every', u'move', u'where', u'he', u'ca', u\"n't\", u'step', u'off', u'the', u'sidewalk', u'.', u'He', u\"'s\", u'given', u'the', u'nickname', u'Pepto', u'by', u'a', u'vagrant', u'after', u'it', u\"'s\", u'written', u'on', u'his', u'forehead', u'where', u'Bolt', u'meets', u'other', u'characters', u'including', u'a', u'woman', u'by', u'the', u'name', u'of', u'Molly', u'(', u'Lesley', u'Ann', u'Warren', u')', u'an', u'ex-dancer', u'who', u'got', u'divorce', u'before', u'losing', u'her', u'home', u',', u'and', u'her', u'pals', u'Sailor', u'(', u'Howard', u'Morris', u')', u'and', u'Fumes', u'(', u'Teddy', u'Wilson', u')', u'who', u'are', u'already', u'used', u'to', u'the', u'streets', u'.', u'They', u\"'re\", u'survivors', u'.', u'Bolt', u'is', u\"n't\", u'.', u'He', u\"'s\", u'not', u'used', u'to', u'reaching', u'mutual', u'agreements', u'like', u'he', u'once', u'did', u'when', u'being', u'rich', u'where', u'it', u\"'s\", u'fight', u'or', u'flight', u',', u'kill', u'or', u'be', u'killed.', u'<', u'br', u'/', u'>', u'<', u'br', u'/', u'>', u'While', u'the', u'love', u'connection', u'between', u'Molly', u'and', u'Bolt', u'was', u\"n't\", u'necessary', u'to', u'plot', u',', u'I', u'found', u'``', u'Life', u'Stinks', u\"''\", u'to', u'be', u'one', u'of', u'Mel', u'Brooks', u\"'\", u'observant', u'films', u'where', u'prior', u'to', u'being', u'a', u'comedy', u',', u'it', u'shows', u'a', u'tender', u'side', u'compared', u'to', u'his', u'slapstick', u'work', u'such', u'as', u'Blazing', u'Saddles', u',', u'Young', u'Frankenstein', u',', u'or', u'Spaceballs', u'for', u'the', u'matter', u',', u'to', u'show', u'what', u'it', u\"'s\", u'like', u'having', u'something', u'valuable', u'before', u'losing', u'it', u'the', u'next', u'day', u'or', u'on', u'the', u'other', u'hand', u'making', u'a', u'stupid', u'bet', u'like', u'all', u'rich', u'people', u'do', u'when', u'they', u'do', u\"n't\", u'know', u'what', u'to', u'do', u'with', u'their', u'money', u'.', u'Maybe', u'they', u'should', u'give', u'it', u'to', u'the', u'homeless', u'instead', u'of', u'using', u'it', u'like', u'Monopoly', u'money.', u'<', u'br', u'/', u'>', u'<', u'br', u'/', u'>', u'Or', u'maybe', u'this', u'film', u'will', u'inspire', u'you', u'to', u'help', u'others', u'.']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(sentences):\n",
    "    from nltk import word_tokenize\n",
    "    print( 'Tokenizing...',)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens += [word_tokenize(sentence.decode('utf-8'))]\n",
    "    print('Done!')\n",
    "\n",
    "    return tokens\n",
    "\n",
    "print(tokenize(read_sentences(data_path+'train/pos/')[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n",
      "Tokenizing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "sentences_trn_pos = tokenize(read_sentences(data_path+'train/pos/'))\n",
    "sentences_trn_neg = tokenize(read_sentences(data_path+'train/neg/'))\n",
    "sentences_trn = sentences_trn_pos + sentences_trn_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary..\n",
      "7056193  total words  135098  unique words\n",
      "2 289298\n"
     ]
    }
   ],
   "source": [
    "#create the dictionary to conver words to numbers. Order it with most frequent words first\n",
    "def build_dict(sentences):\n",
    "#    from collections import OrderedDict\n",
    "\n",
    "    '''\n",
    "    Build dictionary of train words\n",
    "    Outputs: \n",
    "     - Dictionary of word --> word index\n",
    "     - Dictionary of word --> word count freq\n",
    "    '''\n",
    "    print( 'Building dictionary..',)\n",
    "    wordcount = dict()\n",
    "    #For each worn in each sentence, cummulate frequency\n",
    "    for ss in sentences:\n",
    "        for w in ss:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = wordcount.values() # List of frequencies\n",
    "    keys = wordcount.keys() #List of words\n",
    "    \n",
    "    sorted_idx = reversed(np.argsort(counts))\n",
    "    \n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "    print( np.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "\n",
    "    return worddict, wordcount\n",
    "\n",
    "\n",
    "worddict, wordcount = build_dict(sentences_trn)\n",
    "\n",
    "print(worddict['the'], wordcount['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "def generate_sequence(sentences, dictionary):\n",
    "    '''\n",
    "    Convert tokenized text in sequences of integers\n",
    "    '''\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25771, 2010, 9, 6, 1154, 252, 4, 51, 2178, 43, 2, 185, 74, 22, 62, 102, 6180, 55, 457, 144, 3, 163, 22, 32, 30392, 31, 4, 331, 5910, 176, 14, 2, 5314, 6415, 505, 87, 8, 285, 17, 25771, 2010, 18, 2009, 9, 94, 2504, 8, 685, 93, 9, 32, 30392, 31, 4, 21, 31367, 8, 2136, 12271, 3, 2, 6460, 1527, 47, 71, 84, 231, 165, 82, 1286, 5864, 92, 23687, 3, 2, 55076, 7, 2, 236, 919, 3, 45, 3054, 87, 7, 2, 6585, 15, 697, 5, 82, 1527, 4, 283, 15, 234, 2, 410, 14, 72, 6, 1530, 3872, 802, 8, 3892, 211, 2, 457, 3, 15, 1257, 15934, 69, 69, 69, 43, 69, 69, 69, 4, 2010, 4, 137, 378, 402, 90, 62700, 90, 15, 167, 164, 8, 11052, 42, 7, 150, 5864, 4, 44635, 90, 9161, 8, 25771, 2010, 4, 15, 550, 17, 131, 1507, 7, 86, 695, 121, 17, 25771, 2010, 9, 262, 9587, 4, 218, 6, 2576, 17, 16, 9, 30, 41] 1\n"
     ]
    }
   ],
   "source": [
    "# Create train and test data\n",
    "\n",
    "#Read train sentences and generate target y\n",
    "train_x_pos = generate_sequence(sentences_trn_pos, worddict)\n",
    "train_x_neg = generate_sequence(sentences_trn_neg, worddict)\n",
    "X_train_full = train_x_pos + train_x_neg\n",
    "y_train_full = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "print(X_train_full[0], y_train_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n",
      "Tokenizing...\n",
      "Done!\n",
      "[15, 448, 5, 234, 19, 25, 268, 419, 138, 129, 33387, 8, 44, 6, 184, 390, 7, 2014, 4, 15, 254, 942, 17, 15, 20, 6106, 8, 84, 16, 103, 49, 67, 15, 697, 7, 14659, 17791, 40, 20, 77, 517, 8, 60, 252, 4, 15, 20, 394, 4, 17791, 275, 2, 123, 7, 3086, 16280, 65, 108, 3, 5, 1832, 10676, 275, 997, 7851, 23, 163, 14025, 4, 21, 1943, 7, 6, 63, 25, 9, 17, 16, 71, 4335, 23, 293, 1442, 4, 61, 42, 89, 636, 17, 4, 21, 446, 843, 28, 72, 20, 3048, 58, 27, 20, 3099, 44, 2197, 347, 2, 110, 385, 7, 2, 25, 3, 5, 81, 1668, 8, 1758, 347, 2, 381, 385, 4, 458, 16245, 2, 843, 15, 36, 77, 234, 131, 380, 14, 1758, 3, 29, 131, 447, 2346, 400, 22, 108, 3, 284, 2834, 36, 8, 369, 289, 84, 112, 2692, 4, 61, 25, 20, 106, 3, 5, 15, 1432, 17, 34, 169, 84, 16, 187, 34, 2323, 4]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Read test sentences and generate target y\n",
    "sentences_tst_pos = read_sentences(data_path+'test/pos/')\n",
    "sentences_tst_neg = read_sentences(data_path+'test/neg/')\n",
    "\n",
    "test_x_pos = generate_sequence(tokenize(sentences_tst_pos), worddict)\n",
    "test_x_neg = generate_sequence(tokenize(sentences_tst_neg), worddict)\n",
    "X_test_full = test_x_pos + test_x_neg\n",
    "y_test_full = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "print(X_test_full[0])\n",
    "print(y_test_full[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure sequences for a RNN model\n",
    "    - Remove words with low frequency\n",
    "    - Truncate / complete sequences to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median length:  208.0\n"
     ]
    }
   ],
   "source": [
    "#Median length of sentences\n",
    "print('Median length: ', np.median([len(x) for x in X_test_full]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 50000 # Number of most frequent words selected. the less frequent recode to 0\n",
    "maxlen = 200  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4759, 689, 204, 1054, 5694, 1144, 70, 38, 2505, 2032, 3, 2, 1, 32, 20986, 31, 3, 23, 19, 248, 2990, 2422, 507, 55, 2, 25169, 2522, 12691, 141, 6, 220, 338, 0, 5801, 49, 38, 4673, 980, 8, 329, 452, 38, 1, 11650, 14, 67, 20, 2416, 22, 32, 21, 9263, 5756, 4338, 12475, 4, 31, 15, 167, 85, 359, 7, 10289, 3, 5, 168, 23397, 12674, 2422, 1569, 35, 6, 11870, 6, 2758, 28, 107, 1043, 250, 8, 2800, 976, 23, 32, 16197, 31, 5, 32, 2216, 702, 31, 27, 3, 29, 62, 114, 19, 26, 20, 9869, 45, 2, 14646, 12, 13, 10, 11, 12, 13, 10, 11, 21, 26, 532, 23, 62, 1544, 657, 924, 28, 945, 6, 0, 328, 7, 2, 1226, 657, 924, 7, 4278, 18, 32, 0, 31, 5, 32, 5944, 31, 27, 3, 29, 47269, 403, 1378, 24, 116, 110, 11964, 605, 4, 1358, 2, 245, 1100, 8, 2, 2522, 12691, 209, 1357, 70, 65, 108, 4, 5694, 88, 6, 356, 313, 5, 299, 6, 10530, 24, 1150, 3723, 17632, 28, 15, 489, 2, 0, 4387, 7, 2, 245, 33, 316, 132, 7, 2, 1007, 27, 17, 20959, 2, 133, 1091, 54, 887, 62, 2083, 1985, 1183, 8, 2, 4148, 4, 135, 18, 62, 1461, 123, 970, 73, 2, 703, 1258, 1, 9, 2706, 44, 1235, 7, 2, 17467, 14, 459, 3894, 5, 468, 8242, 47, 2773, 38, 248, 23992, 22, 6, 562, 8, 113, 115, 24, 6, 10289, 282, 4, 154, 111, 88, 6, 63, 313, 7, 2304, 5299, 67, 286, 33, 14, 2, 1854, 92, 1892, 539, 5367, 4, 46630, 3, 6, 2161, 383, 4286, 3, 9, 824, 6636, 1402, 164, 4, 20948, 156, 111, 39, 371, 2, 318, 18619, 5, 2465, 3060, 24, 1856, 46, 1783, 1, 849, 7, 1251, 5, 5936, 43, 2, 477, 7, 2, 16298, 19242, 12, 13, 10, 11, 12, 13, 10, 11, 221, 139, 114, 19, 9, 186, 8, 155, 36, 77, 103, 16, 18, 509, 33, 6, 332, 80, 29, 111, 103, 127, 14, 19, 551, 883, 2, 185, 15376, 151, 5, 151, 3, 29, 5694, 1425, 33, 6, 148, 93, 907, 143, 5, 546, 11212, 64, 696, 543, 2, 407, 93, 40, 147, 76, 14, 1033, 7, 16, 4, 1265, 2, 4578, 895, 3, 19, 9, 6, 356, 5, 823, 26, 8, 4205, 24, 17, 1006, 8, 180, 46, 327, 4]\n"
     ]
    }
   ],
   "source": [
    "#Select the most frequent max_features, recode others using 0\n",
    "def remove_features(x):\n",
    "    return [[0 if w >= max_features else w for w in sen] for sen in x]\n",
    "\n",
    "X_train = remove_features(X_train_full)\n",
    "X_test  = remove_features(X_test_full)\n",
    "y_train = y_train_full\n",
    "y_test = y_test_full\n",
    "\n",
    "print(X_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named keras.preprocessing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cace74c5b4df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Cut or complete the sentences to length = maxlen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pad sequences (samples x time)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named keras.preprocessing"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Cut or complete the sentences to length = maxlen\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
