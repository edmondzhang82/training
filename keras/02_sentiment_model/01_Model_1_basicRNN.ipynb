{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Basic RNN\n",
    "    - A simple RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "from collections import OrderedDict\n",
    "from nltk import word_tokenize\n",
    "\n",
    "data_path='/home/ubuntu/data/training/keras/aclImdb/'\n",
    "\n",
    "\n",
    "\n",
    "# Generator of list of files in a folder and subfolders\n",
    "def gen_find(filepath,top):\n",
    "    for path, dirlist, filelist in os.walk(top):\n",
    "        for name in fnmatch.filter(filelist,filepath):\n",
    "            yield os.path.join(path,name)\n",
    "\n",
    "def read_sentences(path):\n",
    "    sentences = []\n",
    "    sentences_list = gen_find(\"*.txt\", path)\n",
    "    for ff in sentences_list:\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    return sentences \n",
    "\n",
    "def tokenize(sentences):\n",
    "    print( 'Tokenizing...',)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens += [word_tokenize(sentence.decode('utf-8'))]\n",
    "    print('Done!')\n",
    "    return tokens\n",
    "\n",
    "def build_dict(sentences):\n",
    "    print( 'Building dictionary..',)\n",
    "    wordcount = dict()\n",
    "    for ss in sentences:\n",
    "        for w in ss:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = wordcount.values()\n",
    "    keys = wordcount.keys()\n",
    "    sorted_idx = np.argsort(counts)[::-1]\n",
    "\n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "    print( np.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "    return worddict, wordcount\n",
    "\n",
    "def generate_sequence(sentences, dictionary):\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
    "    return seqs\n",
    "\n",
    "#Data extraction\n",
    "\n",
    "#Extract training sentences\n",
    "sentences_trn_pos = tokenize(read_sentences(data_path+'train/pos/'))\n",
    "sentences_trn_neg = tokenize(read_sentences(data_path+'train/neg/'))\n",
    "sentences_trn = sentences_trn_pos + sentences_trn_neg\n",
    "\n",
    "#Build train dictionary\n",
    "worddict, wordcount = build_dict(sentences_trn)\n",
    "\n",
    "#Generate train data\n",
    "train_x_pos = generate_sequence(sentences_trn_pos, worddict)\n",
    "train_x_neg = generate_sequence(sentences_trn_neg, worddict)\n",
    "X_train_full = train_x_pos + train_x_neg\n",
    "y_train_full = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "\n",
    "#Read test sentences and generate target y\n",
    "sentences_tst_pos = read_sentences(data_path+'test/pos/')\n",
    "sentences_tst_neg = read_sentences(data_path+'test/neg/')\n",
    "\n",
    "test_x_pos = generate_sequence(tokenize(sentences_tst_pos), worddict)\n",
    "test_x_neg = generate_sequence(tokenize(sentences_tst_neg), worddict)\n",
    "X_test_full = test_x_pos + test_x_neg\n",
    "y_test_full = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "\n",
    "print('Preprocess done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare sequences to model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_features = 20000 # Number of most frequent words selected. the less frequent recode to 0\n",
    "max_len = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "\n",
    "#Select the most frequent max_features, recode others using 0\n",
    "def remove_features(x):\n",
    "    return [[0 if w >= max_features else w for w in sen] for sen in x]\n",
    "\n",
    "X_train = remove_features(X_train_full)\n",
    "X_test  = remove_features(X_test_full)\n",
    "y_train = y_train_full\n",
    "y_test = y_test_full\n",
    "\n",
    "\n",
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
    "\n",
    "\n",
    "# Cut or complete the sentences to length = maxlen\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "dim_embedings = 128 #Dimension of the embedings vector\n",
    "num_hidden_rnn = 128 #Num of neurons in the Recurent network \n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, SimpleRNN, LSTM, Dropout, Dense, merge\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "print('Build model 1 - Basic model...')\n",
    "\n",
    "# LAYER 1: inputs\n",
    "seq_prev_input = Input(shape=(max_len, ), dtype='int32') \n",
    "\n",
    "# LAYER 2: Create embedings\n",
    "embeds = Embedding(max_features, dim_embedings, input_length=max_len)(seq_prev_input)\n",
    "\n",
    "# LAYERS 3: RNN - Simple RNN\n",
    "rnn_out = SimpleRNN(num_hidden_rnn)(embeds)\n",
    "\n",
    "# LAYER 4: Dense layer to outputs - softmax activation\n",
    "output = Dense(2, activation='softmax')(rnn_out)\n",
    "\n",
    "# Model Architecture defined\n",
    "model_1 = Model(input=seq_prev_input, output=output)\n",
    "model_1.summary()\n",
    "\n",
    "# Compile model and select optimizer\n",
    "rms_optimizer = RMSprop(lr=0.001)\n",
    "model_1.compile(loss='sparse_categorical_crossentropy', optimizer=rms_optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot the model graph\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model_1).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "batch_size = 128\n",
    "\n",
    "print(\"Train...\")\n",
    "history = model_1.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\n",
    "                      validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot graphs in the notebook output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Score and obtain probabilities\n",
    "pred_test = model_1.predict(X_test)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import metrics\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "#Calculate accuracy with sklearn\n",
    "print('Accuracy: ',accuracy_score(y_test, [1 if p>0.5 else 0 for p in pred_test[:,1]]))\n",
    "\n",
    "#Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_test[:,1])\n",
    "print('AUC: ', auc(fpr, tpr))\n",
    "\n",
    "#Plot ROC curve\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate one positive record\n",
    "i = 1 # 1, 2, ... , 25000\n",
    "print('Sentence: ',sentences_tst_pos[i])\n",
    "print('target: ',y_test_full[i])\n",
    "print('Prediction [neg, pos]: ', pred_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate one negative record\n",
    "i = -2 # -2, -3, ... -25000\n",
    "print('Sentence: ',sentences_tst_neg[i])\n",
    "print('target: ',y_test[i])\n",
    "print('Prediction [neg, pos]: ', pred_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Score new text\n",
    "def score_new_text(text):\n",
    "    seq = generate_sequence(tokenize([text]), worddict)\n",
    "    seq = remove_features(seq)\n",
    "    seq = sequence.pad_sequences(seq, maxlen=max_len)\n",
    "    pred_test = model_1.predict(seq, batch_size=1)\n",
    "    return float(pred_test[:,1])\n",
    "\n",
    "\n",
    "text = \"You have to start worrying when you see that Michael Madsen is leading the Cast of any movie. I wont go through the list of shame that is his movie career.<br /><br />I watched 45 minutes and still was not sure what really was going on. The movie consisted of a love hate relationship between Madsen and Argento, Which basically was Madsen insulting her, threatening violence and generally treating her like dirt. She on the other hand loves him, then shes doesn't, then she does, the she desires him, then she loves him again......whats wrong with you woman !!!! <br /><br />The Script is awful, lousy soundtrack and pointless aggressive and crude sexuality which i believe was added to entice some viewers as the movie has little else to offer. I would have given the movie a 1 but it just about managed a 2 with a little excitement in the last 20 minutes. It did actually answer one question in the final few minutes but i am not going to share that, i will make you suffer for the full movie like i did.\"\n",
    "print(text)\n",
    "print('Positive score:', score_new_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
