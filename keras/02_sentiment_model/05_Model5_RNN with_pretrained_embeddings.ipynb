{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with pretrained embeddings\n",
    "    - Glove embeddings. Vectors of 300 dim.\n",
    "    - Model: Basic LSTM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "from collections import OrderedDict\n",
    "from nltk import word_tokenize\n",
    "\n",
    "data_path='/home/ubuntu/data/training/keras/aclImdb/'\n",
    "\n",
    "\n",
    "\n",
    "# Generator of list of files in a folder and subfolders\n",
    "def gen_find(filepath,top):\n",
    "    for path, dirlist, filelist in os.walk(top):\n",
    "        for name in fnmatch.filter(filelist,filepath):\n",
    "            yield os.path.join(path,name)\n",
    "\n",
    "def read_sentences(path):\n",
    "    sentences = []\n",
    "    sentences_list = gen_find(\"*.txt\", path)\n",
    "    for ff in sentences_list:\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    return sentences \n",
    "\n",
    "def tokenize(sentences):\n",
    "    print('Tokenizing... ',)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens += [word_tokenize(sentence.decode('utf-8'))]\n",
    "    print('Done!')\n",
    "    return tokens\n",
    "\n",
    "def build_dict(sentences):\n",
    "    print( 'Building dictionary... ',)\n",
    "    wordcount = dict()\n",
    "    for ss in sentences:\n",
    "        for w in ss:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = wordcount.values()\n",
    "    keys = wordcount.keys()\n",
    "    sorted_idx = np.argsort(counts)[::-1]\n",
    "\n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "    print( np.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "    return worddict, wordcount\n",
    "\n",
    "def generate_sequence(sentences, dictionary):\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
    "    return seqs\n",
    "\n",
    "#Data extraction\n",
    "\n",
    "#Extract training sentences\n",
    "sentences_trn_pos = tokenize(read_sentences(data_path+'train/pos/'))\n",
    "sentences_trn_neg = tokenize(read_sentences(data_path+'train/neg/'))\n",
    "sentences_trn = sentences_trn_pos + sentences_trn_neg\n",
    "\n",
    "#Build train dictionary\n",
    "worddict, wordcount = build_dict(sentences_trn)\n",
    "\n",
    "#Generate train data\n",
    "train_x_pos = generate_sequence(sentences_trn_pos, worddict)\n",
    "train_x_neg = generate_sequence(sentences_trn_neg, worddict)\n",
    "X_train = train_x_pos + train_x_neg\n",
    "y_train = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "\n",
    "#Read test sentences and generate target y\n",
    "sentences_tst_pos = read_sentences(data_path+'test/pos/')\n",
    "sentences_tst_neg = read_sentences(data_path+'test/neg/')\n",
    "\n",
    "test_x_pos = generate_sequence(tokenize(sentences_tst_pos), worddict)\n",
    "test_x_neg = generate_sequence(tokenize(sentences_tst_neg), worddict)\n",
    "X_test = test_x_pos + test_x_neg\n",
    "y_test = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "\n",
    "print('Preprocess done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings and join with the current dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load embeddings\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "embed_dim = 300\n",
    "\n",
    "embedding_path = '/home/ubuntu/data/training/keras/'\n",
    "\n",
    "df_glove = pd.read_csv(embedding_path + \"glove.6B.\"+str(embed_dim)+\"d.txt\", index_col=0 ,sep=' ',\n",
    "                   header = None, quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "\n",
    "#Merge with the dictionary of the current texts: Inner join, only words in the corpus and in glove.\n",
    "df_glove = df_glove.merge(pd.DataFrame.from_dict(worddict, orient='index'), left_index=True, right_index=True)\n",
    "print('Merged words: ', df_glove.shape[0])\n",
    "\n",
    "#Create dictionary: word_number_id --> [glove vector associated]\n",
    "glove={}\n",
    "for i,r in df_glove[:].iterrows():\n",
    "    glove[int(r[0])] = [r[j] for j in range(1,embed_dim+1)]\n",
    "print('Dictionary length: ', len(glove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare sequences to model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create embeddings 3D tensors\n",
    "max_len = 100\n",
    "\n",
    "def embedd(x):\n",
    "    r = np.zeros((max_len, embed_dim))\n",
    "    pos = max_len-1\n",
    "    for i in xrange(len(x),0,-1):\n",
    "        found = True\n",
    "        try:\n",
    "            v = np.array([glove[x[i-1]]])\n",
    "        except:\n",
    "            found = False\n",
    "        if found and pos>=0:\n",
    "            r[pos,:] = v \n",
    "            pos += -1\n",
    "    return r\n",
    "        \n",
    "X_train = np.array([embedd(s) for s in X_train], dtype=np.float32)\n",
    "print( X_train.shape)\n",
    "\n",
    "X_test = np.array([embedd(s) for s in X_test], dtype=np.float32)\n",
    "print( X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "num_hidden_rnn = 128 #Num of neurons in the Recurent network \n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, SimpleRNN, LSTM, Dropout, Dense, merge\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "print('Build model 1 - Basic model...')\n",
    "\n",
    "# LAYER 1: inputs\n",
    "seq_prev_input = Input(shape=(max_len, embed_dim), dtype='float32') \n",
    "\n",
    "# LAYER 2: Create embedings\n",
    "#embeds = Embedding(max_features, dim_embedings, input_length=max_len)(seq_prev_input)\n",
    "\n",
    "# LAYERS 3: RNN - forwards LSTM with dropout\n",
    "forward = LSTM(num_hidden_rnn, return_sequences=True,\n",
    "                 dropout_W=0.3, dropout_U=0.3, name='Forward1')(seq_prev_input)\n",
    "rnn_out = LSTM(num_hidden_rnn, return_sequences=False,\n",
    "                 dropout_W=0.3, dropout_U=0.3, name='Forward2')(forward)\n",
    "\n",
    "\n",
    "# LAYER 4: Dense layer to outputs - softmax activation\n",
    "output = Dense(2, activation='softmax')(rnn_out)\n",
    "\n",
    "# Model Architecture defined\n",
    "model_1 = Model(input=seq_prev_input, output=output)\n",
    "model_1.summary()\n",
    "\n",
    "# Compile model and select optimizer\n",
    "model_1.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot the model graph\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model_1).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "batch_size = 128\n",
    "\n",
    "print(\"Train...\")\n",
    "history = model_1.fit(X_train, y_train, batch_size=batch_size, nb_epoch=20,\n",
    "                      validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot graphs in the notebook output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Score and obtain probabilities\n",
    "pred_test = model_1.predict(X_test)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import metrics\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "#Calculate accuracy with sklearn\n",
    "print('Accuracy: ',accuracy_score(y_test, [1 if p>0.5 else 0 for p in pred_test[:,1]]))\n",
    "\n",
    "#Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_test[:,1])\n",
    "print('AUC: ', auc(fpr, tpr) ) \n",
    "\n",
    "#Plot ROC curve\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate one positive record\n",
    "i = 1 # 1, 2, ... , 25000\n",
    "print('Sentence: ',sentences_tst_pos[i])\n",
    "print('target: ',y_test[i])\n",
    "print('Prediction [neg, pos]: ', pred_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate one negative record\n",
    "i = -2 # -2, -3, ... -25000\n",
    "print('Sentence: ',sentences_tst_neg[i])\n",
    "print('target: ',y_test[i])\n",
    "print('Prediction [neg, pos]: ', pred_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
