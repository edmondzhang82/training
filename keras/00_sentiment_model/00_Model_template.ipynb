{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model template\n",
    "    - Preprocess data.\n",
    "    - Prepare sequences to model.\n",
    "    - Build model.\n",
    "    - Validate it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing... Done!\n",
      "Tokenizing... Done!\n",
      "Building dictionary.. 7056193  total words  135098  unique words\n",
      "Tokenizing... Done!\n",
      "Tokenizing... Done!\n",
      "Preprocess done!\n"
     ]
    }
   ],
   "source": [
    "#Imports \n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "from collections import OrderedDict\n",
    "from nltk import word_tokenize\n",
    "\n",
    "data_path='/home/jorge/data/training/keras/aclImdb/'\n",
    "\n",
    "\n",
    "\n",
    "# Generator of list of files in a folder and subfolders\n",
    "def gen_find(filepath,top):\n",
    "    for path, dirlist, filelist in os.walk(top):\n",
    "        for name in fnmatch.filter(filelist,filepath):\n",
    "            yield os.path.join(path,name)\n",
    "\n",
    "def read_sentences(path):\n",
    "    sentences = []\n",
    "    sentences_list = gen_find(\"*.txt\", path)\n",
    "    for ff in sentences_list:\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    return sentences \n",
    "\n",
    "def tokenize(sentences):\n",
    "    print 'Tokenizing...',\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens += [word_tokenize(sentence.decode('utf-8'))]\n",
    "    print('Done!')\n",
    "    return tokens\n",
    "\n",
    "def build_dict(sentences):\n",
    "    print 'Building dictionary..',\n",
    "    wordcount = dict()\n",
    "    for ss in sentences:\n",
    "        for w in ss:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = wordcount.values()\n",
    "    keys = wordcount.keys()\n",
    "    sorted_idx = np.argsort(counts)[::-1]\n",
    "\n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "    print np.sum(counts), ' total words ', len(keys), ' unique words'\n",
    "    return worddict, wordcount\n",
    "\n",
    "def generate_sequence(sentences, dictionary):\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
    "    return seqs\n",
    "\n",
    "#Data extraction\n",
    "\n",
    "#Extract training sentences\n",
    "sentences_trn_pos = tokenize(read_sentences(data_path+'train/pos/'))\n",
    "sentences_trn_neg = tokenize(read_sentences(data_path+'train/neg/'))\n",
    "sentences_trn = sentences_trn_pos + sentences_trn_neg\n",
    "\n",
    "#Build train dictionary\n",
    "worddict, wordcount = build_dict(sentences_trn)\n",
    "\n",
    "#Generate train data\n",
    "train_x_pos = generate_sequence(sentences_trn_pos, worddict)\n",
    "train_x_neg = generate_sequence(sentences_trn_neg, worddict)\n",
    "X_train_full = train_x_pos + train_x_neg\n",
    "y_train_full = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "\n",
    "#Read test sentences and generate target y\n",
    "sentences_tst_pos = read_sentences(data_path+'test/pos/')\n",
    "sentences_tst_neg = read_sentences(data_path+'test/neg/')\n",
    "\n",
    "test_x_pos = generate_sequence(tokenize(sentences_tst_pos), worddict)\n",
    "test_x_neg = generate_sequence(tokenize(sentences_tst_neg), worddict)\n",
    "X_test_full = test_x_pos + test_x_neg\n",
    "y_test_full = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "\n",
    "print('Preprocess done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare sequences to model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN Black (CNMeM is disabled, cuDNN 5103)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:599: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('X_train shape:', (25000, 200))\n",
      "('X_test shape:', (25000, 200))\n"
     ]
    }
   ],
   "source": [
    "max_features = 50000 # Number of most frequent words selected. the less frequent recode to 0\n",
    "max_len = 200  # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "\n",
    "#Select the most frequent max_features, recode others using 0\n",
    "def remove_features(x):\n",
    "    return [[0 if w >= max_features else w for w in sen] for sen in x]\n",
    "\n",
    "X_train = remove_features(X_train_full)\n",
    "X_test  = remove_features(X_test_full)\n",
    "y_train = y_train_full\n",
    "y_test = y_test_full\n",
    "\n",
    "\n",
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
    "\n",
    "\n",
    "# Cut or complete the sentences to length = maxlen\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model 1 - Basic model...\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "dim_embedings = 32 #Dimension of the embedings vector\n",
    "num_hidden_rnn = 256 #Num of neurons in the Recurent network \n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, SimpleRNN, LSTM, Dropout, Dense, merge\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "print('Build model 1 - Basic model...')\n",
    "\n",
    "# LAYER 1: inputs\n",
    "seq_prev_input = Input(shape=(max_len, ), dtype='int32') \n",
    "\n",
    "# LAYER 2: Create embedings\n",
    "embeds = Embedding(max_features, dim_embedings, input_length=max_len)(seq_prev_input)\n",
    "\n",
    "# LAYERS 3: RNN - forwards LSTM with dropout\n",
    "#PUT YOUR MODEL HERE!!!\n",
    "rnn_out = SimpleRNN(num_hidden_rnn)(embeds)\n",
    "\n",
    "\n",
    "# LAYER 4: Dense layer to outputs - softmax activation\n",
    "output = Dense(2, activation='softmax')(rnn_out)\n",
    "\n",
    "# Model Architecture defined\n",
    "model_1 = Model(input=seq_prev_input, output=output)\n",
    "\n",
    "# Compile model and select optimizer\n",
    "rms_optimizer = RMSprop(lr=0.001)\n",
    "model_1.compile(loss='sparse_categorical_crossentropy', optimizer=rms_optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# LAYER 3: RNN \n",
    "rnn_out = SimpleRNN(num_hidden_rnn)(embeds)\n",
    "\n",
    "\n",
    "# LAYER 3: RNN \n",
    "rnn_out = LSTM(num_hidden_rnn, return_sequences=False, name='Forward')(embeds)\n",
    "\n",
    "\n",
    "\n",
    "# LAYER 3: RNN \n",
    "forwards = LSTM(num_hidden_rnn, return_sequences=False, name='Forward')(embeds)\n",
    "backwards = LSTM(num_hidden_rnn, return_sequences=False, go_backwards=True, name='Backward')(embeds)\n",
    "rnn_out = merge([forwards, backwards], mode='concat', concat_axis=-1, name='forward - backward')\n",
    "\n",
    "\n",
    "# LAYER 3: RNN \n",
    "forwards1 = LSTM(num_hidden_rnn, return_sequences=False, name='Forward')(embeds)\n",
    "backwards1 = LSTM(num_hidden_rnn, return_sequences=False, go_backwards=True, name='Backward')(embeds)\n",
    "merged1 = merge([forwards1, backwards1], mode='concat', concat_axis=-1, name='forward - backward')\n",
    "\n",
    "forwards2 = LSTM(num_hidden_rnn, return_sequences=False,\n",
    "                 dropout_W=0.5, dropout_U=0.5, name='Forward')(merged1)\n",
    "backwards2 = LSTM(num_hidden_rnn, return_sequences=False, go_backwards=True, \n",
    "                  dropout_W=0.5, dropout_U=0.5, name='Backward')(merged1)\n",
    "rnn_out = merge([forwards2, backwards2], mode='concat', concat_axis=-1, name='forward - backward')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"268pt\" viewBox=\"0.00 0.00 174.00 268.00\" width=\"174pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 264)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-264 170,-264 170,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140074192079120 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140074192079120</title>\n",
       "<polygon fill=\"none\" points=\"17.5,-223 17.5,-259 148.5,-259 148.5,-223 17.5,-223\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-237.3\">input_1 (InputLayer)</text>\n",
       "</g>\n",
       "<!-- 140074190919056 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140074190919056</title>\n",
       "<polygon fill=\"none\" points=\"-0.5,-149 -0.5,-185 166.5,-185 166.5,-149 -0.5,-149\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-163.3\">embedding_1 (Embedding)</text>\n",
       "</g>\n",
       "<!-- 140074192079120&#45;&gt;140074190919056 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140074192079120-&gt;140074190919056</title>\n",
       "<path d=\"M83,-222.937C83,-214.807 83,-204.876 83,-195.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-195.441 83,-185.441 79.5001,-195.441 86.5001,-195.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140074190600464 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140074190600464</title>\n",
       "<polygon fill=\"none\" points=\"0.5,-75 0.5,-111 165.5,-111 165.5,-75 0.5,-75\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-89.3\">simplernn_1 (SimpleRNN)</text>\n",
       "</g>\n",
       "<!-- 140074190919056&#45;&gt;140074190600464 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140074190919056-&gt;140074190600464</title>\n",
       "<path d=\"M83,-148.937C83,-140.807 83,-130.876 83,-121.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-121.441 83,-111.441 79.5001,-121.441 86.5001,-121.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140072568562384 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140072568562384</title>\n",
       "<polygon fill=\"none\" points=\"29.5,-1 29.5,-37 136.5,-37 136.5,-1 29.5,-1\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-15.3\">dense_1 (Dense)</text>\n",
       "</g>\n",
       "<!-- 140074190600464&#45;&gt;140072568562384 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140074190600464-&gt;140072568562384</title>\n",
       "<path d=\"M83,-74.937C83,-66.8072 83,-56.8761 83,-47.7047\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-47.4406 83,-37.4407 79.5001,-47.4407 86.5001,-47.4406\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot the model graph\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model_1).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 23s - loss: 0.7343 - acc: 0.5098 - val_loss: 0.6766 - val_acc: 0.5660\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 23s - loss: 0.6647 - acc: 0.5867 - val_loss: 0.6785 - val_acc: 0.5695\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 23s - loss: 0.6125 - acc: 0.6584 - val_loss: 0.7072 - val_acc: 0.5800\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 22s - loss: 0.5682 - acc: 0.7028 - val_loss: 0.6939 - val_acc: 0.5857\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 22s - loss: 0.5165 - acc: 0.7429 - val_loss: 0.7408 - val_acc: 0.5877\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 22s - loss: 0.4649 - acc: 0.7782 - val_loss: 0.7588 - val_acc: 0.5858\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 22s - loss: 0.4127 - acc: 0.8130 - val_loss: 0.7996 - val_acc: 0.5857\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 22s - loss: 0.3685 - acc: 0.8332 - val_loss: 0.8386 - val_acc: 0.5841\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 22s - loss: 0.3254 - acc: 0.8584 - val_loss: 0.9271 - val_acc: 0.5860\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 22s - loss: 0.2894 - acc: 0.8774 - val_loss: 0.7802 - val_acc: 0.5952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f65254a33d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "batch_size = 128\n",
    "\n",
    "print(\"Train...\")\n",
    "history = model_1.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\n",
    "                      validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot graphs in the notebook output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Score and obtain probabilities\n",
    "pred_test = model_1.predict(X_test)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59524\n",
      "AUC:  0.6424343104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6527e7b610>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65254a3450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import metrics\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "\n",
    "#Calculate accuracy with sklearn\n",
    "print(accuracy_score(y_test, [1 if p>0.5 else 0 for p in pred_test[:,1]]))\n",
    "\n",
    "#Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_test[:,1])\n",
    "print 'AUC: ', auc(fpr, tpr)  \n",
    "\n",
    "#Plot ROC curve\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sentence: ', \"This short was the first short released by Paramount Famous Studios and was one of several done by the studio showing Popeye engaged directly against the enemy, most often the Japanese. While Warner Brothers, Disney and, to a lesser extent, other studios, did shorts often depicting Germans as foils, the majority of Famous Studios efforts focused on the Japanese. Given Pearl Harbor and Popeye's naval ties, this is quite understandable. This is an average short. Seein' Red, White an' Blue and Spinach For Britain have aged better. But it's still worth watching. Recommended.\")\n",
      "('target: ', 1)\n",
      "('Prediction [pos, neg]: ', array([ 0.0156774 ,  0.98432261]))\n"
     ]
    }
   ],
   "source": [
    "#Evaluate one positive record\n",
    "i = 1 # 1, 2, ... , 25000\n",
    "print('Sentence: ',sentences_tst_pos[i])\n",
    "print('target: ',y_test_full[i])\n",
    "print('Prediction [neg, pos]: ', pred_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sentence: ', \"You have to start worrying when you see that Michael Madsen is leading the Cast of any movie. I wont go through the list of shame that is his movie career.<br /><br />I watched 45 minutes and still was not sure what really was going on. The movie consisted of a love hate relationship between Madsen and Argento, Which basically was Madsen insulting her, threatening violence and generally treating her like dirt. She on the other hand loves him, then shes doesn't, then she does, the she desires him, then she loves him again......whats wrong with you woman !!!! <br /><br />The Script is awful, lousy soundtrack and pointless aggressive and crude sexuality which i believe was added to entice some viewers as the movie has little else to offer. I would have given the movie a 1 but it just about managed a 2 with a little excitement in the last 20 minutes. It did actually answer one question in the final few minutes but i am not going to share that, i will make you suffer for the full movie like i did.\")\n",
      "('target: ', 0)\n",
      "('Prediction [neg, pos]: ', array([ 0.36560684,  0.63439316]))\n"
     ]
    }
   ],
   "source": [
    "#Evaluate one negative record\n",
    "i = -2 # -2, -3, ... -25000\n",
    "print('Sentence: ',sentences_tst_neg[i])\n",
    "print('target: ',y_test[i])\n",
    "print('Prediction [neg, pos]: ', pred_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing... Done!\n",
      "('Positive score:', array([[ 0.34814963,  0.6518504 ],\n",
      "       [ 0.86565292,  0.13434713],\n",
      "       [ 0.33367744,  0.66632259],\n",
      "       ..., \n",
      "       [ 0.18456709,  0.81543291],\n",
      "       [ 0.41626734,  0.5837326 ],\n",
      "       [ 0.39694729,  0.60305274]]))\n"
     ]
    }
   ],
   "source": [
    "# Score new text\n",
    "def score_new_text(text):\n",
    "    seq = generate_sequence(tokenize([text]), worddict)\n",
    "    seq = remove_features(seq)\n",
    "    seq = sequence.pad_sequences(seq, maxlen=max_len)\n",
    "    pred_test = model_1.predict(seq, batch_size=1)\n",
    "    return float(pred_test[:,1])\n",
    "\n",
    "\n",
    "text = \"You have to start worrying when you see that Michael Madsen is leading the Cast of any movie. I wont go through the list of shame that is his movie career.<br /><br />I watched 45 minutes and still was not sure what really was going on. The movie consisted of a love hate relationship between Madsen and Argento, Which basically was Madsen insulting her, threatening violence and generally treating her like dirt. She on the other hand loves him, then shes doesn't, then she does, the she desires him, then she loves him again......whats wrong with you woman !!!! <br /><br />The Script is awful, lousy soundtrack and pointless aggressive and crude sexuality which i believe was added to entice some viewers as the movie has little else to offer. I would have given the movie a 1 but it just about managed a 2 with a little excitement in the last 20 minutes. It did actually answer one question in the final few minutes but i am not going to share that, i will make you suffer for the full movie like i did.\"\n",
    "print('Positive score:', score_new_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
