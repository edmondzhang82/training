{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is what created the dataset pickled and generate a first model at word level.\n",
    "\n",
    "1) You need to download this file and put it in the same directory as this file.\n",
    "https://github.com/moses-smt/mosesdecoder/raw/master/scripts/tokenizer/tokenizer.perl . Give it execution permission.\n",
    "\n",
    "2) Get the dataset from http://ai.stanford.edu/~amaas/data/sentiment/ and extract it in the current directory.\n",
    "\n",
    "3) Then run this script.\n",
    "\"\"\"\n",
    "\n",
    "dataset_path='/home/jorge/proyectos/tesis/RNN/sentiment/'\n",
    "\n",
    "import numpy\n",
    "import cPickle as pkl\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "# tokenizer.perl is from Moses: https://github.com/moses-smt/mosesdecoder/tree/master/scripts/tokenizer\n",
    "tokenizer_cmd = ['./tokenizer.perl', '-l', 'en', '-q', '-']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Separate words and punctuation signs by spaces\n",
    "def tokenize(sentences):\n",
    "\n",
    "    print 'Tokenizing..',\n",
    "    text = \"\\n\".join(sentences)\n",
    "    tokenizer = Popen(tokenizer_cmd, stdin=PIPE, stdout=PIPE)\n",
    "    tok_text, _ = tokenizer.communicate(text)\n",
    "    toks = tok_text.split('\\n')[:-1]\n",
    "    print 'Done'\n",
    "\n",
    "    return toks\n",
    "\n",
    "#create the dictionary to conver words to numbres. Order it with most frequent words first\n",
    "def build_dict(path):\n",
    "    sentences = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir('%s/pos/' % path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    os.chdir('%s/neg/' % path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    os.chdir(currdir)\n",
    "\n",
    "    sentences = tokenize(sentences)\n",
    "\n",
    "    print 'Building dictionary..',\n",
    "    wordcount = dict()\n",
    "    for ss in sentences:\n",
    "        words = ss.strip().lower().split()\n",
    "        for w in words:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = wordcount.values()\n",
    "    keys = wordcount.keys()\n",
    "    sorted_idx = numpy.argsort(counts)[::-1]\n",
    "\n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "    print numpy.sum(counts), ' total words ', len(keys), ' unique words'\n",
    "\n",
    "    return worddict\n",
    "\n",
    "\n",
    "#Read the original corpus \n",
    "def grab_data(path, dictionary):\n",
    "    sentences = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    os.chdir(currdir)\n",
    "    sentences = tokenize(sentences)\n",
    "\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        words = ss.strip().lower().split()\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in words]\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing.. Done\n",
      "Building dictionary.. 7113725  total words  101758  unique words\n",
      "Tokenizing.. Done\n",
      "Tokenizing.. Done\n",
      "Tokenizing.. Done\n",
      "Tokenizing.. Done\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset from http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "#Create dictionary using the train data.\n",
    "path = dataset_path\n",
    "dictionary = build_dict(os.path.join(path, 'train'))\n",
    "\n",
    "#Read train sentences and generate target y\n",
    "train_x_pos = grab_data(path+'train/pos', dictionary)\n",
    "train_x_neg = grab_data(path+'train/neg', dictionary)\n",
    "X_train = train_x_pos + train_x_neg\n",
    "y_train = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "#Read test sentences and generate target y\n",
    "test_x_pos = grab_data(path+'test/pos', dictionary)\n",
    "test_x_neg = grab_data(path+'test/neg', dictionary)\n",
    "X_test = test_x_pos + test_x_neg\n",
    "y_test = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save to use in others models\n",
    "f = open('imdb.pkl', 'wb')\n",
    "pkl.dump((X_train, y_train), f, -1)\n",
    "pkl.dump((X_test, y_test), f, -1)\n",
    "f.close()\n",
    "\n",
    "f = open('imdb.dict.pkl', 'wb')\n",
    "pkl.dump(dictionary, f, -1)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN Black (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "'''Train a LSTM on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF+LogReg.\n",
    "Notes:\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "GPU command:\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_lstm.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select the most frequent max_features, recode others using 0\n",
    "def remove_features(x):\n",
    "    return [[0 if w >= max_features else w for w in sen] for sen in x]\n",
    "\n",
    "X_train = remove_features(X_train)\n",
    "X_test  = remove_features(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('X_train shape:', (25000, 100))\n",
      "('X_test shape:', (25000, 100))\n"
     ]
    }
   ],
   "source": [
    "# Cut or complete the sentences to length = maxlen\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model 1...\n"
     ]
    }
   ],
   "source": [
    "print('Build model 1...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(LSTM(128))  # try using a GRU instead, for fun\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              class_mode=\"binary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 72s - loss: 0.4677 - acc: 0.7766 - val_loss: 0.3867 - val_acc: 0.8313\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 72s - loss: 0.2592 - acc: 0.8970 - val_loss: 0.3858 - val_acc: 0.8366\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 72s - loss: 0.1698 - acc: 0.9370 - val_loss: 0.4329 - val_acc: 0.8268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a20e08110>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "batch_size = 128\n",
    "\n",
    "print(\"Train...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3,\n",
    "          validation_data=(X_test, y_test), show_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 23s    \n",
      "Test score:  0.432894129109\n",
      "Test accuracy:  0.82684\n",
      "25000/25000 [==============================] - 23s    \n",
      " 0.9109590272\n"
     ]
    }
   ],
   "source": [
    "#Evaluate accuracy in test set\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size,\n",
    "                            show_accuracy=True)\n",
    "print 'Test score: ', score\n",
    "print 'Test accuracy: ', acc\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print 'AUC: ', roc_auc_score(y_test, model.predict_proba(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model 2...\n"
     ]
    }
   ],
   "source": [
    "print('Build model 2...')\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model2.add(LSTM(512, return_sequences=True))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(LSTM(512, return_sequences=False))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(1))\n",
    "model2.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              class_mode=\"binary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 238s - loss: 0.6474 - acc: 0.6055 - val_loss: 0.6069 - val_acc: 0.6698\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 236s - loss: 0.3811 - acc: 0.8340 - val_loss: 0.4247 - val_acc: 0.8020\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 236s - loss: 0.2571 - acc: 0.9001 - val_loss: 0.5101 - val_acc: 0.8220\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 237s - loss: 0.1824 - acc: 0.9330 - val_loss: 0.4593 - val_acc: 0.8140\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 236s - loss: 0.1174 - acc: 0.9608 - val_loss: 0.5137 - val_acc: 0.8212\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 236s - loss: 0.0726 - acc: 0.9764 - val_loss: 0.6529 - val_acc: 0.8179\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 236s - loss: 0.0511 - acc: 0.9836 - val_loss: 0.8236 - val_acc: 0.8078\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 236s - loss: 0.0414 - acc: 0.9873 - val_loss: 0.7463 - val_acc: 0.8212\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 236s - loss: 0.0314 - acc: 0.9901 - val_loss: 0.8345 - val_acc: 0.7958\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 236s - loss: 0.0757 - acc: 0.9730 - val_loss: 0.6775 - val_acc: 0.8131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a03dc66d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "batch_size = 128\n",
    "\n",
    "print(\"Train...\")\n",
    "model2.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10,\n",
    "           validation_data=(X_test, y_test), show_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 68s    \n",
      "Test score:  0.677513689604\n",
      "Test accuracy:  0.81308\n",
      "25000/25000 [==============================] - 83s    \n",
      " 0.8855979264\n"
     ]
    }
   ],
   "source": [
    "#Evaluate accuracy in test set\n",
    "score, acc = model2.evaluate(X_test, y_test,\n",
    "                             batch_size=batch_size,\n",
    "                             show_accuracy=True)\n",
    "print 'Test score: ', score\n",
    "print 'Test accuracy: ', acc\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print 'AUC: ', roc_auc_score(y_test, model2.predict_proba(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
