{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a RNN model to text generation\n",
    "- RNN model at character level\n",
    "    - Input: n character previous\n",
    "    - Output: next character\n",
    "    - Model LSTM\n",
    "- Use 'El Quijote' to train the generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Header\n",
    "import numpy as np\n",
    "\n",
    "path = '/home/ubuntu/data/training/keras/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and generate sequences\n",
    "\n",
    "Download quijote from guttenberg project\n",
    "\n",
    "\n",
    "wget http://www.gutenberg.org/cache/epub/2000/pg2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 2198927)\n",
      "('Chars list: ', ['\\n', '\\r', ' ', '!', '\"', '#', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x81', '\\x89', '\\x8d', '\\x91', '\\x93', '\\x9a', '\\xa0', '\\xa1', '\\xa9', '\\xab', '\\xad', '\\xaf', '\\xb1', '\\xb3', '\\xb9', '\\xba', '\\xbb', '\\xbc', '\\xbf', '\\xc2', '\\xc3', '\\xef'])\n",
      "('total chars:', 80)\n"
     ]
    }
   ],
   "source": [
    "#Read book\n",
    "text = open(path + \"pg2000.txt\").read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('Chars list: ', chars)\n",
    "print('total chars:', len(chars))\n",
    "\n",
    "#Dictionaries to convert char to num & num to char\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb sequences:', 732869)\n",
      "e nombre el río taj - o\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "# One sentence of length 20 for each 3 characters\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(300, len(text) - maxlen, step): #Start in line 30 to exclude Gutenberg header.\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print sentences[4996], '-', next_chars[4996]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "('X shape: ', (732869, 20, 80))\n",
      "('y shape: ', (732869, 80))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X: One row by sentence\n",
    "    in each row a matrix of bool 0/1 of dim length_sentence x num_chars coding the sentence. Dummy variables\n",
    "y: One row by sentence\n",
    "    in each row a vector of bool of lengt num_chars with 1 in the next char position\n",
    "'''\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('y shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model 1\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "prev (InputLayer)                (None, 20, 80)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 20, 512)       1214464     prev[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 512)           2099200     lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 80)            41040       lstm_4[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 3354704\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: 2 stacked LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM\n",
    "\n",
    "\n",
    "print('Build model 1')\n",
    "seq_prev_input = Input(shape=(maxlen, len(chars)), name='prev') \n",
    "                \n",
    "# apply forwards LSTM\n",
    "forwards1 = LSTM(512, dropout_W=0.3, dropout_U=0.3, return_sequences=True)(seq_prev_input)\n",
    "\n",
    "forwards2 = LSTM(512, dropout_W=0.3, dropout_U=0.3, return_sequences=False)(forwards1)\n",
    "\n",
    "output = Dense(len(chars), activation='softmax')(forwards2)\n",
    "\n",
    "model1 = Model(input=seq_prev_input, output=output)\n",
    "model1.summary()\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"268pt\" viewBox=\"0.00 0.00 120.00 268.00\" width=\"120pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 264)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-264 116,-264 116,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140649508976848 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140649508976848</title>\n",
       "<polygon fill=\"none\" points=\"-0.5,-223 -0.5,-259 112.5,-259 112.5,-223 -0.5,-223\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"56\" y=\"-237.3\">prev (InputLayer)</text>\n",
       "</g>\n",
       "<!-- 140649511447376 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140649511447376</title>\n",
       "<polygon fill=\"none\" points=\"4,-149 4,-185 108,-185 108,-149 4,-149\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"56\" y=\"-163.3\">lstm_3 (LSTM)</text>\n",
       "</g>\n",
       "<!-- 140649508976848&#45;&gt;140649511447376 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140649508976848-&gt;140649511447376</title>\n",
       "<path d=\"M56,-222.937C56,-214.807 56,-204.876 56,-195.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"59.5001,-195.441 56,-185.441 52.5001,-195.441 59.5001,-195.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140649392692432 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140649392692432</title>\n",
       "<polygon fill=\"none\" points=\"4,-75 4,-111 108,-111 108,-75 4,-75\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"56\" y=\"-89.3\">lstm_4 (LSTM)</text>\n",
       "</g>\n",
       "<!-- 140649511447376&#45;&gt;140649392692432 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140649511447376-&gt;140649392692432</title>\n",
       "<path d=\"M56,-148.937C56,-140.807 56,-130.876 56,-121.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"59.5001,-121.441 56,-111.441 52.5001,-121.441 59.5001,-121.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140649375123536 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140649375123536</title>\n",
       "<polygon fill=\"none\" points=\"2.5,-1 2.5,-37 109.5,-37 109.5,-1 2.5,-1\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"56\" y=\"-15.3\">dense_2 (Dense)</text>\n",
       "</g>\n",
       "<!-- 140649392692432&#45;&gt;140649375123536 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140649392692432-&gt;140649375123536</title>\n",
       "<path d=\"M56,-74.937C56,-66.8072 56,-56.8761 56,-47.7047\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"59.5001,-47.4406 56,-37.4407 52.5001,-47.4407 59.5001,-47.4406\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the model\n",
    "#Plot the model graph\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model1).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600000 samples, validate on 132869 samples\n",
      "Epoch 1/30\n",
      "600000/600000 [==============================] - 523s - loss: 2.2971 - acc: 0.3154 - val_loss: 1.9305 - val_acc: 0.4111\n",
      "Epoch 2/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.8107 - acc: 0.4357 - val_loss: 1.7215 - val_acc: 0.4805\n",
      "Epoch 3/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.6351 - acc: 0.4875 - val_loss: 1.6226 - val_acc: 0.5123\n",
      "Epoch 4/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.5340 - acc: 0.5168 - val_loss: 1.5604 - val_acc: 0.5306\n",
      "Epoch 5/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.4689 - acc: 0.5361 - val_loss: 1.5244 - val_acc: 0.5443\n",
      "Epoch 6/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.4232 - acc: 0.5494 - val_loss: 1.5053 - val_acc: 0.5508\n",
      "Epoch 7/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.3890 - acc: 0.5596 - val_loss: 1.4823 - val_acc: 0.5579\n",
      "Epoch 8/30\n",
      "600000/600000 [==============================] - 524s - loss: 1.3618 - acc: 0.5672 - val_loss: 1.4643 - val_acc: 0.5625\n",
      "Epoch 9/30\n",
      "600000/600000 [==============================] - 524s - loss: 1.3409 - acc: 0.5730 - val_loss: 1.4587 - val_acc: 0.5667\n",
      "Epoch 10/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.3231 - acc: 0.5784 - val_loss: 1.4466 - val_acc: 0.5690\n",
      "Epoch 11/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.3084 - acc: 0.5819 - val_loss: 1.4398 - val_acc: 0.5736\n",
      "Epoch 12/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2954 - acc: 0.5864 - val_loss: 1.4360 - val_acc: 0.5740\n",
      "Epoch 13/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2839 - acc: 0.5885 - val_loss: 1.4296 - val_acc: 0.5776\n",
      "Epoch 14/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2751 - acc: 0.5913 - val_loss: 1.4275 - val_acc: 0.5785\n",
      "Epoch 15/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2645 - acc: 0.5944 - val_loss: 1.4230 - val_acc: 0.5794\n",
      "Epoch 16/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2578 - acc: 0.5963 - val_loss: 1.4182 - val_acc: 0.5816\n",
      "Epoch 17/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2508 - acc: 0.5988 - val_loss: 1.4202 - val_acc: 0.5818\n",
      "Epoch 18/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2455 - acc: 0.6005 - val_loss: 1.4151 - val_acc: 0.5812\n",
      "Epoch 19/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2394 - acc: 0.6017 - val_loss: 1.4133 - val_acc: 0.5836\n",
      "Epoch 20/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2332 - acc: 0.6037 - val_loss: 1.4099 - val_acc: 0.5848\n",
      "Epoch 21/30\n",
      "600000/600000 [==============================] - 524s - loss: 1.2288 - acc: 0.6051 - val_loss: 1.4164 - val_acc: 0.5837\n",
      "Epoch 22/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2247 - acc: 0.6057 - val_loss: 1.4102 - val_acc: 0.5861\n",
      "Epoch 23/30\n",
      "600000/600000 [==============================] - 524s - loss: 1.2215 - acc: 0.6067 - val_loss: 1.4080 - val_acc: 0.5873\n",
      "Epoch 24/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2167 - acc: 0.6078 - val_loss: 1.4099 - val_acc: 0.5855\n",
      "Epoch 25/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2124 - acc: 0.6097 - val_loss: 1.4050 - val_acc: 0.5876\n",
      "Epoch 26/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2111 - acc: 0.6101 - val_loss: 1.4105 - val_acc: 0.5868\n",
      "Epoch 27/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2059 - acc: 0.6116 - val_loss: 1.4027 - val_acc: 0.5879\n",
      "Epoch 28/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2026 - acc: 0.6122 - val_loss: 1.4048 - val_acc: 0.5868\n",
      "Epoch 29/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.1990 - acc: 0.6138 - val_loss: 1.4090 - val_acc: 0.5887\n",
      "Epoch 30/30\n",
      "600000/600000 [==============================] - 524s - loss: 1.1969 - acc: 0.6139 - val_loss: 1.4053 - val_acc: 0.5889\n"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "history = model1.fit(X[:600000], y[:600000], batch_size=512, nb_epoch=30,\n",
    "           validation_data=(X[600000:], y[600000:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train on 600000 samples, validate on 132869 samples\n",
    "Epoch 1/30\n",
    "600000/600000 [==============================] - 523s - loss: 2.2971 - acc: 0.3154 - val_loss: 1.9305 - val_acc: 0.4111\n",
    "Epoch 2/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.8107 - acc: 0.4357 - val_loss: 1.7215 - val_acc: 0.4805\n",
    "Epoch 3/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.6351 - acc: 0.4875 - val_loss: 1.6226 - val_acc: 0.5123\n",
    "Epoch 4/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.5340 - acc: 0.5168 - val_loss: 1.5604 - val_acc: 0.5306\n",
    "Epoch 5/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4689 - acc: 0.5361 - val_loss: 1.5244 - val_acc: 0.5443\n",
    "Epoch 6/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4232 - acc: 0.5494 - val_loss: 1.5053 - val_acc: 0.5508\n",
    "Epoch 7/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3890 - acc: 0.5596 - val_loss: 1.4823 - val_acc: 0.5579\n",
    "Epoch 8/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3618 - acc: 0.5672 - val_loss: 1.4643 - val_acc: 0.5625\n",
    "Epoch 9/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3409 - acc: 0.5730 - val_loss: 1.4587 - val_acc: 0.5667\n",
    "Epoch 10/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.3231 - acc: 0.5784 - val_loss: 1.4466 - val_acc: 0.5690\n",
    "Epoch 11/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3084 - acc: 0.5819 - val_loss: 1.4398 - val_acc: 0.5736\n",
    "Epoch 12/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2954 - acc: 0.5864 - val_loss: 1.4360 - val_acc: 0.5740\n",
    "Epoch 13/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2839 - acc: 0.5885 - val_loss: 1.4296 - val_acc: 0.5776\n",
    "Epoch 14/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2751 - acc: 0.5913 - val_loss: 1.4275 - val_acc: 0.5785\n",
    "Epoch 15/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2645 - acc: 0.5944 - val_loss: 1.4230 - val_acc: 0.5794\n",
    "Epoch 16/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2578 - acc: 0.5963 - val_loss: 1.4182 - val_acc: 0.5816\n",
    "Epoch 17/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2508 - acc: 0.5988 - val_loss: 1.4202 - val_acc: 0.5818\n",
    "Epoch 18/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2455 - acc: 0.6005 - val_loss: 1.4151 - val_acc: 0.5812\n",
    "Epoch 19/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2394 - acc: 0.6017 - val_loss: 1.4133 - val_acc: 0.5836\n",
    "Epoch 20/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2332 - acc: 0.6037 - val_loss: 1.4099 - val_acc: 0.5848\n",
    "Epoch 21/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2288 - acc: 0.6051 - val_loss: 1.4164 - val_acc: 0.5837\n",
    "Epoch 22/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2247 - acc: 0.6057 - val_loss: 1.4102 - val_acc: 0.5861\n",
    "Epoch 23/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2215 - acc: 0.6067 - val_loss: 1.4080 - val_acc: 0.5873\n",
    "Epoch 24/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2167 - acc: 0.6078 - val_loss: 1.4099 - val_acc: 0.5855\n",
    "Epoch 25/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2124 - acc: 0.6097 - val_loss: 1.4050 - val_acc: 0.5876\n",
    "Epoch 26/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2111 - acc: 0.6101 - val_loss: 1.4105 - val_acc: 0.5868\n",
    "Epoch 27/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2059 - acc: 0.6116 - val_loss: 1.4027 - val_acc: 0.5879\n",
    "Epoch 28/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2026 - acc: 0.6122 - val_loss: 1.4048 - val_acc: 0.5868\n",
    "Epoch 29/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.1990 - acc: 0.6138 - val_loss: 1.4090 - val_acc: 0.5887\n",
    "Epoch 30/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.1969 - acc: 0.6139 - val_loss: 1.4053 - val_acc: 0.5889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAHfCAYAAABwGPAaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb1b3ad3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] /home/ubuntu/data/training/keras/models/w_text_generation_model1.h5 already exists - overwrite? [y/n]y\n",
      "[TIP] Next time specify overwrite=True in save_weights!\n"
     ]
    }
   ],
   "source": [
    "#Save model\n",
    "model_name = 'text_generation_model1'\n",
    "\n",
    "json_string = model1.to_json()\n",
    "open(path + 'models/mdl_' + model_name + '.json', 'w').write(json_string)\n",
    "model1.save_weights(path + 'models/w_' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN Black (CNMeM is disabled, cuDNN 5103)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:599: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model_name = 'text_generation_model1'\n",
    "\n",
    "model1 = model_from_json(open(path + 'models/mdl_' + model_name + '.json').read())\n",
    "model1.load_weights(path + 'models/w_' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 20\n",
    "\n",
    "\n",
    "def sample(a, diversity=1.0):\n",
    "    '''\n",
    "    helper function to sample an index from a probability array\n",
    "    - Diversity control the level of randomless\n",
    "    '''\n",
    "    a = np.log(a) / diversity\n",
    "    a = np.exp(a) / np.sum(np.exp(a), axis=0)\n",
    "    a /= np.sum(a+0.0000001) #Precission error\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "\n",
    "def generate_text(sentence, diversity, current_model, num_char=400):\n",
    "    sentence_init = sentence\n",
    "    generated = ''\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = current_model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    print()\n",
    "    print('DIVERSITY: ',diversity)\n",
    "    print(sentence_init + generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "('DIVERSITY: ', 0.2)\n",
      "mire vuestra merced a los demás de la mano de la mano de los dos de la mano de la mano de la mano de la mano, y que le diese la mano de la cabeza de la mano de la mano de la cabeza, y a los demás de la cabeza, y adonde se le había de ser mano a la memoria del cabrero, y aun a los demás de los dos de los demás de los demás de los demás de la mujer y desde aquí a la mano, y que no se hallará mucha cosa de la m\n",
      "()\n",
      "('DIVERSITY: ', 0.5)\n",
      "mire vuestra merced sería que no sobre buen hombre que debe de ser alguna de un mundo que no le habéis de ser muy bien con sus manos, que se los hagan los ojos de los pensamientos de los caballeros andantes y\n",
      "malambrunos de los libros de amor y verdaderamente ha de ser mejor que tenga tal trasletado en un caballero andante caballero andante, que el más deste mal de los dos de la cuadrilla de la vida había de ser\n",
      "()\n",
      "('DIVERSITY: ', 1)\n",
      "mire vuestra merced que las cosas fue como continente, la famuna del arriero, sin duda\n",
      "repreventan. sólo esto, dotea en la mar, y no le digo mi señora\n",
      "ninguna floja hermosísima fee trazón, y otra manera se libre. trevía\n",
      "cristiano, emperador ni a un vuestro riqueza a lo que dijese a los días, cardenio y ellos tras los más alegres a\n",
      "tu cuida, esperando a su renegado\n",
      "remo que esperar encantador con\n",
      "que las \n",
      "()\n",
      "('DIVERSITY: ', 1.2)\n",
      "mire vuestra merced le hay que la casa le\n",
      "miraban intención era\n",
      "mortalle, lo fondéme a las bomadas puestas una dueña\n",
      "doña historia se pusiese, comenzó a mano, la vije\n",
      "supla discurso se conoce, le conotó,\n",
      "con todo esto de mi señora. sólo por haberse suciso a pelo era\n",
      "venía a la lengua los dos los muchas quinca\n",
      "tumbadadas, como otra mujereforó moraban, te dijo:\n",
      "\n",
      "diciéndole:\n",
      "\n",
      "-pues, es olta -replic\n",
      "()\n",
      "('DIVERSITY: ', 0.2)\n",
      "de lo que sucedió a la mano de la mano, sino que se le diesen en su casa, porque no le ha de ser más de los caballeros andantes que en la cabeza de la cabeza, y los demás de los demás de los ojos de la mano de la mano, y dijo:\n",
      "\n",
      "-¡oh sancho -dijo el cura-, y así, se le dijo:\n",
      "\n",
      "-¡oh buen caballero -dijo el cura-, que está en la más falta de la mano de los demás de la mano de la mano, y a los demás de la m\n",
      "()\n",
      "('DIVERSITY: ', 0.5)\n",
      "de lo que sucedió a la mitad del rey que el jumento de la trifaldi, le dijo:\n",
      "\n",
      "-no se hallaré la mano, y de los cuales le habían de ser algunos días que no se ha de ser entendimiento de la mano, con tanto menester como lo vio fortuna en el mundo.\n",
      "\n",
      "-¿dónde vuestra merced dicen que se ha de ser consigo mucho días, que yo soy bajar al que le había dado con las manos de los deseos, y que en la memoria del cami\n",
      "()\n",
      "('DIVERSITY: ', 1)\n",
      "de lo que sucedió a buscar armado aasí que hizo\n",
      "en responder puesto al ánimo villano\n",
      "indubir en el contento donde podía hacer otra alguna, dijo:\n",
      "\n",
      "-señora dura -respondió sancho-, volvió sancho una cosa y volvieron en un rucio. y esta vez ha de\n",
      "devenidado como él, era tal hidalgo en el suelo en las almas, con tanto punto, lo hicieron comerzar, o una montaña, o�fendéis la empresa. no os\n",
      "entiende en guis\n",
      "()\n",
      "('DIVERSITY: ', 1.2)\n",
      "de lo que sucedió alguno, y tú osaría que se hito grandísimo cuerpo, que si el albernar desto según era me despertaron felice, que más díjelo, sin sabéis juramientos aquellos \n",
      "oídos comenzó:\n",
      "\n",
      "-no ha oído agradarse con don\n",
      "quijote, cuya odonte diró\n",
      "que allí se puedan comer.\n",
      "\n",
      "-¡menos era, para que era más, desde adlleñando la nueva en sus falsas, aquellos\n",
      "lugares, como era por ver en la mano y t\n",
      "()\n",
      "('DIVERSITY: ', 0.2)\n",
      "de allí a poco comenzó a su amo, y aun lo menos se le había de haber de ser con ella en la mesma cosa que le diese en el mundo de la mano, porque en la mujer de la mano de la mano, y los demás de los demás de los demás de los demás de los dos de la cabeza, y a los demás de la mano de los demás de los demás de los demás de la mujer de la mano de la mano de la cabeza, porque no es muy bueno que en la memoria\n",
      "()\n",
      "('DIVERSITY: ', 0.5)\n",
      "de allí a poco comenzó a su caído,\n",
      "y luego se llevo a los deseos; y, así como el caballero de la mano de su caballero, por su señora, y que ella le aventaja un alma la cabeza del cura que está en el mundo de los papeles de los del loco, y el rey me llegó a su amo, y del mundo me parece que en el mundo que en el mundo de las armas de la mano de la boca, desde la mano a su señor, y el duque no se ha de saber, \n",
      "()\n",
      "('DIVERSITY: ', 1)\n",
      "de allí a poco comenzó a cuantos tenían; y así, después de\n",
      "dios, y que más propósito para la tierra.\n",
      "   respondió don quijote:\n",
      "\n",
      "-¡oh deseos hazaños y oídos se hicieran condesas que ha ichomidado particiento; y es esto en esto dellos.\n",
      "\n",
      "-\n",
      "»-piense virtu, esperaba dellos cuando yo tienen con caer vida al deseo un don quijote muchas grandezas\n",
      "hacha con un apartán, como no le habré hallado, porque no\n",
      "()\n",
      "('DIVERSITY: ', 1.2)\n",
      "de allí a poco comer,\n",
      "erále el ciudo y digan a\n",
      "todo lo doncello; con\n",
      "sus brazos nombre, como de\n",
      "imaginar, a la cual no le parecieron que, como ni cuyurbil me recogedo; a lo menos caballero andante, que ella no hacían otra cosa\n",
      "le había dado un franco alis, y dijó:\n",
      "\n",
      "-señor don quijotea, no se haya pasado, y era la silla una figura, porque ni entonces\n",
      "durado al corral y discreto de la intención\n",
      "de los c\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = 'mire vuestra merced '\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "\n",
    "sentence = 'de lo que sucedió a'\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "\n",
    "sentence = 'de allí a poco come'\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "mire vuestra merced decís, y que se le pareció que en la cabeza de la cabeza, y el caballero del caballero de la cabeza, y los demás de los demás de los demás de los demás de la mano de la mujer de la mano de la mano, y aun a su señora dulcinea del toboso, y el cura que el caballero de la mano que le pareció que estaba en la misma cosa que en la mitad del caballero de la mano, se le dijo:\n",
    "\n",
    "-no sé -respondi\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "mire vuestra merced que no está en la cabeza. pero, en efeto, pues todo aquello que está en el mundo que cada uno debía\n",
    "de ser con la misma sierra parte de la muerte de la mano, con todo el mundo me la conoció, que tenía por el primero que están el rostro en las manos, y al señor don quijote -respondió sancho-, porque en la puerta de los sucesos del buen estado de la desencantada de los ojos.\n",
    "\n",
    "-¿qué mal \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "mire vuestra merced que, aunque, desdicho y tiempo viene, a\n",
    "cuya cabeza destos tres, los informaciones que dejara de serle, que me fueron? ¿admirado,\n",
    "\n",
    "y, creyendo que me va y enfermo. y esto que tienen entonces las requiebros que algunos limpios en un ampeoso como si improvisentes en sus insimulables y en los míos al que el honesto, en la mano de mucho premio y don quijote fingióno los dos o sabidores, y,\n",
    "acom\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "mire vuestra merced paso que no sa cluero-. subió, señor,\n",
    "le hubiera vuelto el duque, que pica, yo fue pose�ría de guardar cierto. para los demás, esperando la misma tragua debe de haberlas hallado su santa hijo ni debían para mí, porque yo hay de platar pre subir tú turba -dijo el deleitable-; otras porturas, sino como alabanzas y\n",
    "comedimientos puciese\n",
    "los días de lo que von mirado después de\n",
    "visión de\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de lo que sucedió a la mano de la mano de la mano de los ojos de la mano de la mano de la mano, y aun más que se le dijese la mano de la mano y en la mitad del caballero del caballero de la mano de la mano, y que el parte de la mano, y que en la mitad del caballero de la mano, sino a la puerta de la mano de la mano de la mano de los demás de la cabeza, y el cura y la cabeza de la mano y en la cabeza de la cabeza, \n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de lo que sucedió a camila, por ser tan alta de la mano y en voz brazo? ¿qué es lo que pudiera, señor don quijote que en las fermosuras que después que le sacarán con don quijote había de ser la duquesa, la cual no le había de ser muy buena como gausa, y si este deseo de ser mejor que en la cabeza está a los dos de la mano, y su amo no le puede dar a su casa por los manos de mano, diciendo:\n",
    "\n",
    "-pues, ¿qué \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de lo que sucedió a las más faltas cuentan haciendo:\n",
    "\n",
    "-�qcorrían por esto, bueno -respondió cómo está aquel mano traía furia por ella estajo\n",
    "más que andaba y fingión de sus apartieron de modo que no\n",
    "hay vasto qué hizo con galdáis que soy sin duda, duque ni en la más\n",
    "buena gran señora dulcinea; y así lo han don quijote y no bastaba junto a nuestra\n",
    "señora dulcinea, ahora\n",
    "la entienda, el aposento a\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de lo que sucedió al\n",
    "admiración, dijo:\n",
    "\n",
    "-eso me esplevo en razón encantada, y su venimo tiempo, que pasaba tan hirtoria. es\n",
    "\n",
    "don quijote, le dijo:\n",
    "\n",
    "y cuando jamás:\n",
    "  si ya entienden en mi padre desde aquí vean fuerza, como yo\n",
    "costa yo he oído decir, el rey\n",
    "\n",
    "vención que ésto, que lo més comenzó a vuestra merced colgaré por la orden y parece que\n",
    "pudieron semplarse. el cual, si de la mono.\n",
    "acudi�\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de allí a poco comenzó a su caballero andante, que en la más hermosa de la mano, y el cura que el mundo tenía con el cura y el de la mano de la cabeza de la mano de la mano de la mano de la mano de la cabeza, y a los demás de los cuatro de la mano de la mano de la mano y en la mitad del caballero de la cabeza, y le dijo:\n",
    "\n",
    "-¡oh sancho -dijo el cura-, que no se le habían de ser manos de los demás de los de la\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de allí a poco comenzó a la vida de la mano de\n",
    "arriba al mundo. pero, con todo eso, ha de ser el rostro de las linajes de su escudero. por el rey en el mundo de la industria que en su caballero andante; y, aunque se volvió a camila y de la mano de la muerte de mi cabeza? y así, como el tal caballero andante, que los demás juramentos, y con la mitad del toboso, y así, por decir que os son de allí a los deseos \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de allí a poco comenzó\n",
    "a dos crazos de entender que tratar la nueva al desde nuevas andantes de solos con los detros donde me hubieran\n",
    "de subir la sumiera y rabia la duquesa\n",
    "   a un hacer con un gato, que le\n",
    "dijo juntar la locura para su\n",
    "amo, para que quisieron decir alguna, vino\n",
    "todas aquellos demás caballeros. pero, sancho, tanto, viendo camila la primero tiene: prodición\n",
    "que yo le dio caminar las belloz\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de allí a poco comer. otros días se le pidía hablar de cerra\n",
    "   que el gate predice\n",
    "y otras puntas del cordel bestia.\n",
    "\n",
    "-yo no por eso, decaría sobre la venta, porque él saslos demás sin él que\n",
    "llevase a buscar de la suma, sancho milático, cuando posían los zogados. y si así, mi rendido a\n",
    "cuerpo, ni en llopar salió el baece-, que el hábísimo que viene por\n",
    "los tratas y tontos que sean, y el de los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
