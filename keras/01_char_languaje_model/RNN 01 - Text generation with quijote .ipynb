{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a RNN model to text generation\n",
    "- RNN model at character level\n",
    "    - Input: n character previous\n",
    "    - Output: next character\n",
    "    - Model LSTM\n",
    "- Use 'El Quijote' to train the generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Header\n",
    "import numpy as np\n",
    "\n",
    "path = '/home/ubuntu/data/training/keras/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and generate sequences\n",
    "\n",
    "Download quijote from guttenberg project\n",
    "\n",
    "\n",
    "wget http://www.gutenberg.org/cache/epub/2000/pg2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corpus length:', 2198927)\n",
      "('Chars list: ', ['\\n', '\\r', ' ', '!', '\"', '#', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x81', '\\x89', '\\x8d', '\\x91', '\\x93', '\\x9a', '\\xa0', '\\xa1', '\\xa9', '\\xab', '\\xad', '\\xaf', '\\xb1', '\\xb3', '\\xb9', '\\xba', '\\xbb', '\\xbc', '\\xbf', '\\xc2', '\\xc3', '\\xef'])\n",
      "('total chars:', 80)\n"
     ]
    }
   ],
   "source": [
    "#Read book\n",
    "text = open(path + \"pg2000.txt\").read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('Chars list: ', chars)\n",
    "print('total chars:', len(chars))\n",
    "\n",
    "#Dictionaries to convert char to num & num to char\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb sequences:', 732869)\n",
      "e nombre el r√≠o taj - o\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "# One sentence of length 20 for each 3 characters\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(300, len(text) - maxlen, step): #Start in line 30 to exclude Gutenberg header.\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print sentences[4996], '-', next_chars[4996]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "('X shape: ', (732869, 20, 80))\n",
      "('y shape: ', (732869, 80))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X: One row by sentence\n",
    "    in each row a matrix of bool 0/1 of dim length_sentence x num_chars coding the sentence. Dummy variables\n",
    "y: One row by sentence\n",
    "    in each row a vector of bool of lengt num_chars with 1 in the next char position\n",
    "'''\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('y shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model 1\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "prev (InputLayer)                (None, 20, 80)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 20, 512)       1214464     prev[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 512)           2099200     lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 80)            41040       lstm_4[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 3354704\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: 2 stacked LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM\n",
    "\n",
    "\n",
    "print('Build model 1')\n",
    "seq_prev_input = Input(shape=(maxlen, len(chars)), name='prev') \n",
    "                \n",
    "# apply forwards LSTM\n",
    "forwards1 = LSTM(512, dropout_W=0.3, dropout_U=0.3, return_sequences=True)(seq_prev_input)\n",
    "\n",
    "forwards2 = LSTM(512, dropout_W=0.3, dropout_U=0.3, return_sequences=False)(forwards1)\n",
    "\n",
    "output = Dense(len(chars), activation='softmax')(forwards2)\n",
    "\n",
    "model1 = Model(input=seq_prev_input, output=output)\n",
    "model1.summary()\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"268pt\" viewBox=\"0.00 0.00 120.00 268.00\" width=\"120pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 264)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-264 116,-264 116,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140649508976848 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140649508976848</title>\n",
       "<polygon fill=\"none\" points=\"-0.5,-223 -0.5,-259 112.5,-259 112.5,-223 -0.5,-223\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"56\" y=\"-237.3\">prev (InputLayer)</text>\n",
       "</g>\n",
       "<!-- 140649511447376 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140649511447376</title>\n",
       "<polygon fill=\"none\" points=\"4,-149 4,-185 108,-185 108,-149 4,-149\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"56\" y=\"-163.3\">lstm_3 (LSTM)</text>\n",
       "</g>\n",
       "<!-- 140649508976848&#45;&gt;140649511447376 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140649508976848-&gt;140649511447376</title>\n",
       "<path d=\"M56,-222.937C56,-214.807 56,-204.876 56,-195.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"59.5001,-195.441 56,-185.441 52.5001,-195.441 59.5001,-195.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140649392692432 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140649392692432</title>\n",
       "<polygon fill=\"none\" points=\"4,-75 4,-111 108,-111 108,-75 4,-75\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"56\" y=\"-89.3\">lstm_4 (LSTM)</text>\n",
       "</g>\n",
       "<!-- 140649511447376&#45;&gt;140649392692432 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140649511447376-&gt;140649392692432</title>\n",
       "<path d=\"M56,-148.937C56,-140.807 56,-130.876 56,-121.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"59.5001,-121.441 56,-111.441 52.5001,-121.441 59.5001,-121.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140649375123536 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140649375123536</title>\n",
       "<polygon fill=\"none\" points=\"2.5,-1 2.5,-37 109.5,-37 109.5,-1 2.5,-1\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"56\" y=\"-15.3\">dense_2 (Dense)</text>\n",
       "</g>\n",
       "<!-- 140649392692432&#45;&gt;140649375123536 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140649392692432-&gt;140649375123536</title>\n",
       "<path d=\"M56,-74.937C56,-66.8072 56,-56.8761 56,-47.7047\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"59.5001,-47.4406 56,-37.4407 52.5001,-47.4407 59.5001,-47.4406\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the model\n",
    "#Plot the model graph\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model1).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600000 samples, validate on 132869 samples\n",
      "Epoch 1/30\n",
      "600000/600000 [==============================] - 523s - loss: 2.2971 - acc: 0.3154 - val_loss: 1.9305 - val_acc: 0.4111\n",
      "Epoch 2/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.8107 - acc: 0.4357 - val_loss: 1.7215 - val_acc: 0.4805\n",
      "Epoch 3/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.6351 - acc: 0.4875 - val_loss: 1.6226 - val_acc: 0.5123\n",
      "Epoch 4/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.5340 - acc: 0.5168 - val_loss: 1.5604 - val_acc: 0.5306\n",
      "Epoch 5/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.4689 - acc: 0.5361 - val_loss: 1.5244 - val_acc: 0.5443\n",
      "Epoch 6/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.4232 - acc: 0.5494 - val_loss: 1.5053 - val_acc: 0.5508\n",
      "Epoch 7/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.3890 - acc: 0.5596 - val_loss: 1.4823 - val_acc: 0.5579\n",
      "Epoch 8/30\n",
      "600000/600000 [==============================] - 524s - loss: 1.3618 - acc: 0.5672 - val_loss: 1.4643 - val_acc: 0.5625\n",
      "Epoch 9/30\n",
      "600000/600000 [==============================] - 524s - loss: 1.3409 - acc: 0.5730 - val_loss: 1.4587 - val_acc: 0.5667\n",
      "Epoch 10/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.3231 - acc: 0.5784 - val_loss: 1.4466 - val_acc: 0.5690\n",
      "Epoch 11/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.3084 - acc: 0.5819 - val_loss: 1.4398 - val_acc: 0.5736\n",
      "Epoch 12/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2954 - acc: 0.5864 - val_loss: 1.4360 - val_acc: 0.5740\n",
      "Epoch 13/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2839 - acc: 0.5885 - val_loss: 1.4296 - val_acc: 0.5776\n",
      "Epoch 14/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2751 - acc: 0.5913 - val_loss: 1.4275 - val_acc: 0.5785\n",
      "Epoch 15/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2645 - acc: 0.5944 - val_loss: 1.4230 - val_acc: 0.5794\n",
      "Epoch 16/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2578 - acc: 0.5963 - val_loss: 1.4182 - val_acc: 0.5816\n",
      "Epoch 17/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2508 - acc: 0.5988 - val_loss: 1.4202 - val_acc: 0.5818\n",
      "Epoch 18/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2455 - acc: 0.6005 - val_loss: 1.4151 - val_acc: 0.5812\n",
      "Epoch 19/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2394 - acc: 0.6017 - val_loss: 1.4133 - val_acc: 0.5836\n",
      "Epoch 20/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2332 - acc: 0.6037 - val_loss: 1.4099 - val_acc: 0.5848\n",
      "Epoch 21/30\n",
      "600000/600000 [==============================] - 524s - loss: 1.2288 - acc: 0.6051 - val_loss: 1.4164 - val_acc: 0.5837\n",
      "Epoch 22/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2247 - acc: 0.6057 - val_loss: 1.4102 - val_acc: 0.5861\n",
      "Epoch 23/30\n",
      "600000/600000 [==============================] - 524s - loss: 1.2215 - acc: 0.6067 - val_loss: 1.4080 - val_acc: 0.5873\n",
      "Epoch 24/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2167 - acc: 0.6078 - val_loss: 1.4099 - val_acc: 0.5855\n",
      "Epoch 25/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2124 - acc: 0.6097 - val_loss: 1.4050 - val_acc: 0.5876\n",
      "Epoch 26/30\n",
      "600000/600000 [==============================] - 522s - loss: 1.2111 - acc: 0.6101 - val_loss: 1.4105 - val_acc: 0.5868\n",
      "Epoch 27/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2059 - acc: 0.6116 - val_loss: 1.4027 - val_acc: 0.5879\n",
      "Epoch 28/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.2026 - acc: 0.6122 - val_loss: 1.4048 - val_acc: 0.5868\n",
      "Epoch 29/30\n",
      "600000/600000 [==============================] - 523s - loss: 1.1990 - acc: 0.6138 - val_loss: 1.4090 - val_acc: 0.5887\n",
      "Epoch 30/30\n",
      "600000/600000 [==============================] - 524s - loss: 1.1969 - acc: 0.6139 - val_loss: 1.4053 - val_acc: 0.5889\n"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "history = model1.fit(X[:600000], y[:600000], batch_size=512, nb_epoch=30,\n",
    "           validation_data=(X[600000:], y[600000:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train on 600000 samples, validate on 132869 samples\n",
    "Epoch 1/30\n",
    "600000/600000 [==============================] - 523s - loss: 2.2971 - acc: 0.3154 - val_loss: 1.9305 - val_acc: 0.4111\n",
    "Epoch 2/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.8107 - acc: 0.4357 - val_loss: 1.7215 - val_acc: 0.4805\n",
    "Epoch 3/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.6351 - acc: 0.4875 - val_loss: 1.6226 - val_acc: 0.5123\n",
    "Epoch 4/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.5340 - acc: 0.5168 - val_loss: 1.5604 - val_acc: 0.5306\n",
    "Epoch 5/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4689 - acc: 0.5361 - val_loss: 1.5244 - val_acc: 0.5443\n",
    "Epoch 6/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4232 - acc: 0.5494 - val_loss: 1.5053 - val_acc: 0.5508\n",
    "Epoch 7/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3890 - acc: 0.5596 - val_loss: 1.4823 - val_acc: 0.5579\n",
    "Epoch 8/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3618 - acc: 0.5672 - val_loss: 1.4643 - val_acc: 0.5625\n",
    "Epoch 9/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3409 - acc: 0.5730 - val_loss: 1.4587 - val_acc: 0.5667\n",
    "Epoch 10/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.3231 - acc: 0.5784 - val_loss: 1.4466 - val_acc: 0.5690\n",
    "Epoch 11/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3084 - acc: 0.5819 - val_loss: 1.4398 - val_acc: 0.5736\n",
    "Epoch 12/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2954 - acc: 0.5864 - val_loss: 1.4360 - val_acc: 0.5740\n",
    "Epoch 13/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2839 - acc: 0.5885 - val_loss: 1.4296 - val_acc: 0.5776\n",
    "Epoch 14/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2751 - acc: 0.5913 - val_loss: 1.4275 - val_acc: 0.5785\n",
    "Epoch 15/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2645 - acc: 0.5944 - val_loss: 1.4230 - val_acc: 0.5794\n",
    "Epoch 16/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2578 - acc: 0.5963 - val_loss: 1.4182 - val_acc: 0.5816\n",
    "Epoch 17/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2508 - acc: 0.5988 - val_loss: 1.4202 - val_acc: 0.5818\n",
    "Epoch 18/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2455 - acc: 0.6005 - val_loss: 1.4151 - val_acc: 0.5812\n",
    "Epoch 19/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2394 - acc: 0.6017 - val_loss: 1.4133 - val_acc: 0.5836\n",
    "Epoch 20/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2332 - acc: 0.6037 - val_loss: 1.4099 - val_acc: 0.5848\n",
    "Epoch 21/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2288 - acc: 0.6051 - val_loss: 1.4164 - val_acc: 0.5837\n",
    "Epoch 22/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2247 - acc: 0.6057 - val_loss: 1.4102 - val_acc: 0.5861\n",
    "Epoch 23/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2215 - acc: 0.6067 - val_loss: 1.4080 - val_acc: 0.5873\n",
    "Epoch 24/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2167 - acc: 0.6078 - val_loss: 1.4099 - val_acc: 0.5855\n",
    "Epoch 25/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2124 - acc: 0.6097 - val_loss: 1.4050 - val_acc: 0.5876\n",
    "Epoch 26/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2111 - acc: 0.6101 - val_loss: 1.4105 - val_acc: 0.5868\n",
    "Epoch 27/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2059 - acc: 0.6116 - val_loss: 1.4027 - val_acc: 0.5879\n",
    "Epoch 28/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2026 - acc: 0.6122 - val_loss: 1.4048 - val_acc: 0.5868\n",
    "Epoch 29/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.1990 - acc: 0.6138 - val_loss: 1.4090 - val_acc: 0.5887\n",
    "Epoch 30/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.1969 - acc: 0.6139 - val_loss: 1.4053 - val_acc: 0.5889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAHfCAYAAABwGPAaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb1b3ad3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] /home/ubuntu/data/training/keras/models/w_text_generation_model1.h5 already exists - overwrite? [y/n]y\n",
      "[TIP] Next time specify overwrite=True in save_weights!\n"
     ]
    }
   ],
   "source": [
    "#Save model\n",
    "model_name = 'text_generation_model1'\n",
    "\n",
    "json_string = model1.to_json()\n",
    "open(path + 'models/mdl_' + model_name + '.json', 'w').write(json_string)\n",
    "model1.save_weights(path + 'models/w_' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN Black (CNMeM is disabled, cuDNN 5103)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:599: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model_name = 'text_generation_model1'\n",
    "\n",
    "model1 = model_from_json(open(path + 'models/mdl_' + model_name + '.json').read())\n",
    "model1.load_weights(path + 'models/w_' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 20\n",
    "\n",
    "\n",
    "def sample(a, diversity=1.0):\n",
    "    '''\n",
    "    helper function to sample an index from a probability array\n",
    "    - Diversity control the level of randomless\n",
    "    '''\n",
    "    a = np.log(a) / diversity\n",
    "    a = np.exp(a) / np.sum(np.exp(a), axis=0)\n",
    "    a /= np.sum(a+0.0000001) #Precission error\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "\n",
    "def generate_text(sentence, diversity, current_model, num_char=400):\n",
    "    sentence_init = sentence\n",
    "    generated = ''\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = current_model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    print()\n",
    "    print('DIVERSITY: ',diversity)\n",
    "    print(sentence_init + generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "('DIVERSITY: ', 0.2)\n",
      "mire vuestra merced a los dem√°s de la mano de la mano de los dos de la mano de la mano de la mano de la mano, y que le diese la mano de la cabeza de la mano de la mano de la cabeza, y a los dem√°s de la cabeza, y adonde se le hab√≠a de ser mano a la memoria del cabrero, y aun a los dem√°s de los dos de los dem√°s de los dem√°s de los dem√°s de la mujer y desde aqu√≠ a la mano, y que no se hallar√° mucha cosa de la m\n",
      "()\n",
      "('DIVERSITY: ', 0.5)\n",
      "mire vuestra merced ser√≠a que no sobre buen hombre que debe de ser alguna de un mundo que no le hab√©is de ser muy bien con sus manos, que se los hagan los ojos de los pensamientos de los caballeros andantes y\n",
      "malambrunos de los libros de amor y verdaderamente ha de ser mejor que tenga tal trasletado en un caballero andante caballero andante, que el m√°s deste mal de los dos de la cuadrilla de la vida hab√≠a de ser\n",
      "()\n",
      "('DIVERSITY: ', 1)\n",
      "mire vuestra merced que las cosas fue como continente, la famuna del arriero, sin duda\n",
      "repreventan. s√≥lo esto, dotea en la mar, y no le digo mi se√±ora\n",
      "ninguna floja hermos√≠sima fee traz√≥n, y otra manera se libre. trev√≠a\n",
      "cristiano, emperador ni a un vuestro riqueza a lo que dijese a los d√≠as, cardenio y ellos tras los m√°s alegres a\n",
      "tu cuida, esperando a su renegado\n",
      "remo que esperar encantador con\n",
      "que las \n",
      "()\n",
      "('DIVERSITY: ', 1.2)\n",
      "mire vuestra merced le hay que la casa le\n",
      "miraban intenci√≥n era\n",
      "mortalle, lo fond√©me a las bomadas puestas una due√±a\n",
      "do√±a historia se pusiese, comenz√≥ a mano, la vije\n",
      "supla discurso se conoce, le conot√≥,\n",
      "con todo esto de mi se√±ora. s√≥lo por haberse suciso a pelo era\n",
      "ven√≠a a la lengua los dos los muchas quinca\n",
      "tumbadadas, como otra mujerefor√≥ moraban, te dijo:\n",
      "\n",
      "dici√©ndole:\n",
      "\n",
      "-pues, es olta -replic\n",
      "()\n",
      "('DIVERSITY: ', 0.2)\n",
      "de lo que sucedi√≥ a la mano de la mano, sino que se le diesen en su casa, porque no le ha de ser m√°s de los caballeros andantes que en la cabeza de la cabeza, y los dem√°s de los dem√°s de los ojos de la mano de la mano, y dijo:\n",
      "\n",
      "-¬°oh sancho -dijo el cura-, y as√≠, se le dijo:\n",
      "\n",
      "-¬°oh buen caballero -dijo el cura-, que est√° en la m√°s falta de la mano de los dem√°s de la mano de la mano, y a los dem√°s de la m\n",
      "()\n",
      "('DIVERSITY: ', 0.5)\n",
      "de lo que sucedi√≥ a la mitad del rey que el jumento de la trifaldi, le dijo:\n",
      "\n",
      "-no se hallar√© la mano, y de los cuales le hab√≠an de ser algunos d√≠as que no se ha de ser entendimiento de la mano, con tanto menester como lo vio fortuna en el mundo.\n",
      "\n",
      "-¬ød√≥nde vuestra merced dicen que se ha de ser consigo mucho d√≠as, que yo soy bajar al que le hab√≠a dado con las manos de los deseos, y que en la memoria del cami\n",
      "()\n",
      "('DIVERSITY: ', 1)\n",
      "de lo que sucedi√≥ a buscar armado aas√≠ que hizo\n",
      "en responder puesto al √°nimo villano\n",
      "indubir en el contento donde pod√≠a hacer otra alguna, dijo:\n",
      "\n",
      "-se√±ora dura -respondi√≥ sancho-, volvi√≥ sancho una cosa y volvieron en un rucio. y esta vez ha de\n",
      "devenidado como √©l, era tal hidalgo en el suelo en las almas, con tanto punto, lo hicieron comerzar, o una monta√±a, oÔøΩfend√©is la empresa. no os\n",
      "entiende en guis\n",
      "()\n",
      "('DIVERSITY: ', 1.2)\n",
      "de lo que sucedi√≥ alguno, y t√∫ osar√≠a que se hito grand√≠simo cuerpo, que si el albernar desto seg√∫n era me despertaron felice, que m√°s d√≠jelo, sin sab√©is juramientos aquellos \n",
      "o√≠dos comenz√≥:\n",
      "\n",
      "-no ha o√≠do agradarse con don\n",
      "quijote, cuya odonte dir√≥\n",
      "que all√≠ se puedan comer.\n",
      "\n",
      "-¬°menos era, para que era m√°s, desde adlle√±ando la nueva en sus falsas, aquellos\n",
      "lugares, como era por ver en la mano y t\n",
      "()\n",
      "('DIVERSITY: ', 0.2)\n",
      "de all√≠ a poco comenz√≥ a su amo, y aun lo menos se le hab√≠a de haber de ser con ella en la mesma cosa que le diese en el mundo de la mano, porque en la mujer de la mano de la mano, y los dem√°s de los dem√°s de los dem√°s de los dem√°s de los dos de la cabeza, y a los dem√°s de la mano de los dem√°s de los dem√°s de los dem√°s de la mujer de la mano de la mano de la cabeza, porque no es muy bueno que en la memoria\n",
      "()\n",
      "('DIVERSITY: ', 0.5)\n",
      "de all√≠ a poco comenz√≥ a su ca√≠do,\n",
      "y luego se llevo a los deseos; y, as√≠ como el caballero de la mano de su caballero, por su se√±ora, y que ella le aventaja un alma la cabeza del cura que est√° en el mundo de los papeles de los del loco, y el rey me lleg√≥ a su amo, y del mundo me parece que en el mundo que en el mundo de las armas de la mano de la boca, desde la mano a su se√±or, y el duque no se ha de saber, \n",
      "()\n",
      "('DIVERSITY: ', 1)\n",
      "de all√≠ a poco comenz√≥ a cuantos ten√≠an; y as√≠, despu√©s de\n",
      "dios, y que m√°s prop√≥sito para la tierra.\n",
      "   respondi√≥ don quijote:\n",
      "\n",
      "-¬°oh deseos haza√±os y o√≠dos se hicieran condesas que ha ichomidado particiento; y es esto en esto dellos.\n",
      "\n",
      "-\n",
      "¬ª-piense virtu, esperaba dellos cuando yo tienen con caer vida al deseo un don quijote muchas grandezas\n",
      "hacha con un apart√°n, como no le habr√© hallado, porque no\n",
      "()\n",
      "('DIVERSITY: ', 1.2)\n",
      "de all√≠ a poco comer,\n",
      "er√°le el ciudo y digan a\n",
      "todo lo doncello; con\n",
      "sus brazos nombre, como de\n",
      "imaginar, a la cual no le parecieron que, como ni cuyurbil me recogedo; a lo menos caballero andante, que ella no hac√≠an otra cosa\n",
      "le hab√≠a dado un franco alis, y dij√≥:\n",
      "\n",
      "-se√±or don quijotea, no se haya pasado, y era la silla una figura, porque ni entonces\n",
      "durado al corral y discreto de la intenci√≥n\n",
      "de los c\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = 'mire vuestra merced '\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "\n",
    "sentence = 'de lo que sucedi√≥ a'\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "\n",
    "sentence = 'de all√≠ a poco come'\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "mire vuestra merced dec√≠s, y que se le pareci√≥ que en la cabeza de la cabeza, y el caballero del caballero de la cabeza, y los dem√°s de los dem√°s de los dem√°s de los dem√°s de la mano de la mujer de la mano de la mano, y aun a su se√±ora dulcinea del toboso, y el cura que el caballero de la mano que le pareci√≥ que estaba en la misma cosa que en la mitad del caballero de la mano, se le dijo:\n",
    "\n",
    "-no s√© -respondi\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "mire vuestra merced que no est√° en la cabeza. pero, en efeto, pues todo aquello que est√° en el mundo que cada uno deb√≠a\n",
    "de ser con la misma sierra parte de la muerte de la mano, con todo el mundo me la conoci√≥, que ten√≠a por el primero que est√°n el rostro en las manos, y al se√±or don quijote -respondi√≥ sancho-, porque en la puerta de los sucesos del buen estado de la desencantada de los ojos.\n",
    "\n",
    "-¬øqu√© mal \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "mire vuestra merced que, aunque, desdicho y tiempo viene, a\n",
    "cuya cabeza destos tres, los informaciones que dejara de serle, que me fueron? ¬øadmirado,\n",
    "\n",
    "y, creyendo que me va y enfermo. y esto que tienen entonces las requiebros que algunos limpios en un ampeoso como si improvisentes en sus insimulables y en los m√≠os al que el honesto, en la mano de mucho premio y don quijote fingi√≥no los dos o sabidores, y,\n",
    "acom\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "mire vuestra merced paso que no sa cluero-. subi√≥, se√±or,\n",
    "le hubiera vuelto el duque, que pica, yo fue poseÔøΩr√≠a de guardar cierto. para los dem√°s, esperando la misma tragua debe de haberlas hallado su santa hijo ni deb√≠an para m√≠, porque yo hay de platar pre subir t√∫ turba -dijo el deleitable-; otras porturas, sino como alabanzas y\n",
    "comedimientos puciese\n",
    "los d√≠as de lo que von mirado despu√©s de\n",
    "visi√≥n de\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de lo que sucedi√≥ a la mano de la mano de la mano de los ojos de la mano de la mano de la mano, y aun m√°s que se le dijese la mano de la mano y en la mitad del caballero del caballero de la mano de la mano, y que el parte de la mano, y que en la mitad del caballero de la mano, sino a la puerta de la mano de la mano de la mano de los dem√°s de la cabeza, y el cura y la cabeza de la mano y en la cabeza de la cabeza, \n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de lo que sucedi√≥ a camila, por ser tan alta de la mano y en voz brazo? ¬øqu√© es lo que pudiera, se√±or don quijote que en las fermosuras que despu√©s que le sacar√°n con don quijote hab√≠a de ser la duquesa, la cual no le hab√≠a de ser muy buena como gausa, y si este deseo de ser mejor que en la cabeza est√° a los dos de la mano, y su amo no le puede dar a su casa por los manos de mano, diciendo:\n",
    "\n",
    "-pues, ¬øqu√© \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de lo que sucedi√≥ a las m√°s faltas cuentan haciendo:\n",
    "\n",
    "-ÔøΩqcorr√≠an por esto, bueno -respondi√≥ c√≥mo est√° aquel mano tra√≠a furia por ella estajo\n",
    "m√°s que andaba y fingi√≥n de sus apartieron de modo que no\n",
    "hay vasto qu√© hizo con gald√°is que soy sin duda, duque ni en la m√°s\n",
    "buena gran se√±ora dulcinea; y as√≠ lo han don quijote y no bastaba junto a nuestra\n",
    "se√±ora dulcinea, ahora\n",
    "la entienda, el aposento a\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de lo que sucedi√≥ al\n",
    "admiraci√≥n, dijo:\n",
    "\n",
    "-eso me esplevo en raz√≥n encantada, y su venimo tiempo, que pasaba tan hirtoria. es\n",
    "\n",
    "don quijote, le dijo:\n",
    "\n",
    "y cuando jam√°s:\n",
    "  si ya entienden en mi padre desde aqu√≠ vean fuerza, como yo\n",
    "costa yo he o√≠do decir, el rey\n",
    "\n",
    "venci√≥n que √©sto, que lo m√©s comenz√≥ a vuestra merced colgar√© por la orden y parece que\n",
    "pudieron semplarse. el cual, si de la mono.\n",
    "acudiÔøΩ\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de all√≠ a poco comenz√≥ a su caballero andante, que en la m√°s hermosa de la mano, y el cura que el mundo ten√≠a con el cura y el de la mano de la cabeza de la mano de la mano de la mano de la mano de la cabeza, y a los dem√°s de los cuatro de la mano de la mano de la mano y en la mitad del caballero de la cabeza, y le dijo:\n",
    "\n",
    "-¬°oh sancho -dijo el cura-, que no se le hab√≠an de ser manos de los dem√°s de los de la\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de all√≠ a poco comenz√≥ a la vida de la mano de\n",
    "arriba al mundo. pero, con todo eso, ha de ser el rostro de las linajes de su escudero. por el rey en el mundo de la industria que en su caballero andante; y, aunque se volvi√≥ a camila y de la mano de la muerte de mi cabeza? y as√≠, como el tal caballero andante, que los dem√°s juramentos, y con la mitad del toboso, y as√≠, por decir que os son de all√≠ a los deseos \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de all√≠ a poco comenz√≥\n",
    "a dos crazos de entender que tratar la nueva al desde nuevas andantes de solos con los detros donde me hubieran\n",
    "de subir la sumiera y rabia la duquesa\n",
    "   a un hacer con un gato, que le\n",
    "dijo juntar la locura para su\n",
    "amo, para que quisieron decir alguna, vino\n",
    "todas aquellos dem√°s caballeros. pero, sancho, tanto, viendo camila la primero tiene: prodici√≥n\n",
    "que yo le dio caminar las belloz\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de all√≠ a poco comer. otros d√≠as se le pid√≠a hablar de cerra\n",
    "   que el gate predice\n",
    "y otras puntas del cordel bestia.\n",
    "\n",
    "-yo no por eso, decar√≠a sobre la venta, porque √©l saslos dem√°s sin √©l que\n",
    "llevase a buscar de la suma, sancho mil√°tico, cuando pos√≠an los zogados. y si as√≠, mi rendido a\n",
    "cuerpo, ni en llopar sali√≥ el baece-, que el h√°b√≠simo que viene por\n",
    "los tratas y tontos que sean, y el de los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
