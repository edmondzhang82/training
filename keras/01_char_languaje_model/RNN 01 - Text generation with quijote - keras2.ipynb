{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a RNN model to text generation\n",
    "- RNN model at character level\n",
    "    - Input: n character previous\n",
    "    - Output: next character\n",
    "    - Model LSTM\n",
    "- Use 'El Quijote' to train the generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Header\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "print('Keras version: ', keras.__version__)\n",
    "\n",
    "# GPU devices visible by python\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# Limit memory usage\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "path = '/home/ubuntu/data/training/keras/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and generate sequences\n",
    "\n",
    "Download quijote from guttenberg project\n",
    "\n",
    "wget http://www.gutenberg.org/cache/epub/2000/pg2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2117498\n",
      "Chars list:  ['\\n', ' ', '!', '\"', '#', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '«', '»', '¿', 'à', 'á', 'é', 'í', 'ï', 'ñ', 'ó', 'ù', 'ú', 'ü', '\\ufeff']\n",
      "total chars: 72\n"
     ]
    }
   ],
   "source": [
    "#Read book\n",
    "text = open(path + \"pg2000.txt\").read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('Chars list: ', chars)\n",
    "print('total chars:', len(chars))\n",
    "\n",
    "#Dictionaries to convert char to num & num to char\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 705726\n",
      "tregará a medea; si  - d\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "# One sentence of length 20 for each 3 characters\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(300, len(text) - maxlen, step): #Start in line 30 to exclude Gutenberg header.\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print(sentences[4996], '-', next_chars[4996])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "X shape:  (705726, 20, 72)\n",
      "y shape:  (705726, 72)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X: One row by sentence\n",
    "    in each row a matrix of bool 0/1 of dim length_sentence x num_chars coding the sentence. Dummy variables\n",
    "y: One row by sentence\n",
    "    in each row a vector of bool of lengt num_chars with 1 in the next char position\n",
    "'''\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('y shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prev (InputLayer)            (None, 20, 72)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 20, 512)           1198080   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 72)                36936     \n",
      "=================================================================\n",
      "Total params: 3,334,216.0\n",
      "Trainable params: 3,334,216.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: 2 stacked LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM\n",
    "\n",
    "\n",
    "print('Build model 1')\n",
    "seq_prev_input = Input(shape=(maxlen, len(chars)), name='prev') \n",
    "                \n",
    "# apply forwards LSTM\n",
    "forwards1 = LSTM(512, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)(seq_prev_input)\n",
    "\n",
    "forwards2 = LSTM(512, return_sequences=False, dropout=0.3, recurrent_dropout=0.3)(forwards1)\n",
    "\n",
    "output = Dense(len(chars), activation='softmax')(forwards2)\n",
    "\n",
    "model = Model(inputs=seq_prev_input, outputs=output)\n",
    "model.summary()\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 116.00 264.00\" width=\"116pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 112,-260 112,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140610162887368 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140610162887368</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 108,-255.5 108,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-233.8\">prev: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140610162887480 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140610162887480</title>\n",
       "<polygon fill=\"none\" points=\"5,-146.5 5,-182.5 103,-182.5 103,-146.5 5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-160.8\">lstm_3: LSTM</text>\n",
       "</g>\n",
       "<!-- 140610162887368&#45;&gt;140610162887480 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140610162887368-&gt;140610162887480</title>\n",
       "<path d=\"M54,-219.313C54,-211.289 54,-201.547 54,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"57.5001,-192.529 54,-182.529 50.5001,-192.529 57.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140610162888320 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140610162888320</title>\n",
       "<polygon fill=\"none\" points=\"5,-73.5 5,-109.5 103,-109.5 103,-73.5 5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-87.8\">lstm_4: LSTM</text>\n",
       "</g>\n",
       "<!-- 140610162887480&#45;&gt;140610162888320 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140610162887480-&gt;140610162888320</title>\n",
       "<path d=\"M54,-146.313C54,-138.289 54,-128.547 54,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"57.5001,-119.529 54,-109.529 50.5001,-119.529 57.5001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140610162888488 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140610162888488</title>\n",
       "<polygon fill=\"none\" points=\"3,-0.5 3,-36.5 105,-36.5 105,-0.5 3,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140610162888320&#45;&gt;140610162888488 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140610162888320-&gt;140610162888488</title>\n",
       "<path d=\"M54,-73.3129C54,-65.2895 54,-55.5475 54,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"57.5001,-46.5288 54,-36.5288 50.5001,-46.5289 57.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the model\n",
    "#Plot the model graph\n",
    "from IPython.display import SVG\n",
    "from keras.utils import vis_utils\n",
    "\n",
    "SVG(vis_utils.model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600000 samples, validate on 105726 samples\n",
      "Epoch 1/50\n",
      "600000/600000 [==============================] - 128s - loss: 2.2960 - acc: 0.3154 - val_loss: 2.0004 - val_acc: 0.4005\n",
      "Epoch 2/50\n",
      "600000/600000 [==============================] - 128s - loss: 1.8696 - acc: 0.4224 - val_loss: 1.8162 - val_acc: 0.4574\n",
      "Epoch 3/50\n",
      "600000/600000 [==============================] - 128s - loss: 1.7005 - acc: 0.4715 - val_loss: 1.7047 - val_acc: 0.4973\n",
      "Epoch 4/50\n",
      "600000/600000 [==============================] - 126s - loss: 1.5980 - acc: 0.5014 - val_loss: 1.6492 - val_acc: 0.5153\n",
      "Epoch 5/50\n",
      "305152/600000 [==============>...............] - ETA: 58s - loss: 1.5394 - acc: 0.5184"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "history = model.fit(X[:600000], y[:600000], batch_size=512, epochs=50,\n",
    "           validation_data=(X[600000:], y[600000:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train on 600000 samples, validate on 132869 samples\n",
    "Epoch 1/30\n",
    "600000/600000 [==============================] - 523s - loss: 2.2971 - acc: 0.3154 - val_loss: 1.9305 - val_acc: 0.4111\n",
    "Epoch 2/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.8107 - acc: 0.4357 - val_loss: 1.7215 - val_acc: 0.4805\n",
    "Epoch 3/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.6351 - acc: 0.4875 - val_loss: 1.6226 - val_acc: 0.5123\n",
    "Epoch 4/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.5340 - acc: 0.5168 - val_loss: 1.5604 - val_acc: 0.5306\n",
    "Epoch 5/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4689 - acc: 0.5361 - val_loss: 1.5244 - val_acc: 0.5443\n",
    "Epoch 6/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4232 - acc: 0.5494 - val_loss: 1.5053 - val_acc: 0.5508\n",
    "Epoch 7/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3890 - acc: 0.5596 - val_loss: 1.4823 - val_acc: 0.5579\n",
    "Epoch 8/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3618 - acc: 0.5672 - val_loss: 1.4643 - val_acc: 0.5625\n",
    "Epoch 9/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3409 - acc: 0.5730 - val_loss: 1.4587 - val_acc: 0.5667\n",
    "Epoch 10/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.3231 - acc: 0.5784 - val_loss: 1.4466 - val_acc: 0.5690\n",
    "Epoch 11/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3084 - acc: 0.5819 - val_loss: 1.4398 - val_acc: 0.5736\n",
    "Epoch 12/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2954 - acc: 0.5864 - val_loss: 1.4360 - val_acc: 0.5740\n",
    "Epoch 13/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2839 - acc: 0.5885 - val_loss: 1.4296 - val_acc: 0.5776\n",
    "Epoch 14/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2751 - acc: 0.5913 - val_loss: 1.4275 - val_acc: 0.5785\n",
    "Epoch 15/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2645 - acc: 0.5944 - val_loss: 1.4230 - val_acc: 0.5794\n",
    "Epoch 16/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2578 - acc: 0.5963 - val_loss: 1.4182 - val_acc: 0.5816\n",
    "Epoch 17/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2508 - acc: 0.5988 - val_loss: 1.4202 - val_acc: 0.5818\n",
    "Epoch 18/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2455 - acc: 0.6005 - val_loss: 1.4151 - val_acc: 0.5812\n",
    "Epoch 19/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2394 - acc: 0.6017 - val_loss: 1.4133 - val_acc: 0.5836\n",
    "Epoch 20/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2332 - acc: 0.6037 - val_loss: 1.4099 - val_acc: 0.5848\n",
    "Epoch 21/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2288 - acc: 0.6051 - val_loss: 1.4164 - val_acc: 0.5837\n",
    "Epoch 22/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2247 - acc: 0.6057 - val_loss: 1.4102 - val_acc: 0.5861\n",
    "Epoch 23/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2215 - acc: 0.6067 - val_loss: 1.4080 - val_acc: 0.5873\n",
    "Epoch 24/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2167 - acc: 0.6078 - val_loss: 1.4099 - val_acc: 0.5855\n",
    "Epoch 25/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2124 - acc: 0.6097 - val_loss: 1.4050 - val_acc: 0.5876\n",
    "Epoch 26/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2111 - acc: 0.6101 - val_loss: 1.4105 - val_acc: 0.5868\n",
    "Epoch 27/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2059 - acc: 0.6116 - val_loss: 1.4027 - val_acc: 0.5879\n",
    "Epoch 28/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2026 - acc: 0.6122 - val_loss: 1.4048 - val_acc: 0.5868\n",
    "Epoch 29/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.1990 - acc: 0.6138 - val_loss: 1.4090 - val_acc: 0.5887\n",
    "Epoch 30/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.1969 - acc: 0.6139 - val_loss: 1.4053 - val_acc: 0.5889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAHVCAYAAADcnaM7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMdYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe9f267f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "\n",
    "save_model(model, path + 'models/text_generation_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model1 = load_model(path + 'models/text_generation_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 20\n",
    "\n",
    "\n",
    "def sample(a, diversity=1.0):\n",
    "    '''\n",
    "    helper function to sample an index from a probability array\n",
    "    - Diversity control the level of randomless\n",
    "    '''\n",
    "    a = np.log(a) / diversity\n",
    "    a = np.exp(a) / np.sum(np.exp(a), axis=0)\n",
    "    a /= np.sum(a+0.0000001) #Precission error\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "\n",
    "def generate_text(sentence, diversity, current_model, num_char=400):\n",
    "    sentence_init = sentence\n",
    "    generated = ''\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = current_model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    print()\n",
    "    print('DIVERSITY: ',diversity)\n",
    "    print(sentence_init + generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIVERSITY:  0.2\n",
      "mire vuestra merced de la de la de la serier de la cuera de la verta que se señor de la meras de la cuello que el mer de la desta de la cante y el con la meros de la venta de la que se había de la meras de la meriento de la merer que no se la de le había de la meras de la merar a la cuerto de la merer de la cante a la meras de la desta de la ser de la de se la meros de la verta de la heres de la contero que el alla d\n"
     ]
    }
   ],
   "source": [
    "sentence = 'mire vuestra merced '\n",
    "generate_text(sentence, 0.2, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel/__main__.py:9: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIVERSITY:  0.2\n",
      "mire vuestra merced lo ha de ser mi señor don quijote -dijo el cura-, que estaba en la mitad del caballero de la mano, y le dijo:\n",
      "\n",
      "-¿qué dios se le diga en la mitad del mundo, y el caballero del caballero de la mano de los de los demás de la mujer de la mano, que está en el mundo los ojos de la mano, y que la había de ser el deseo de su caballero andante, y el cura que le dijese la mano de la mano, y de la ma\n",
      "\n",
      "DIVERSITY:  0.5\n",
      "mire vuestra merced cuando le dijo:\n",
      "\n",
      "-esto se le había de hacer más de los ojos de su grande aventura, con mucha falta de los río, y el tal se puedo preguntar por los\n",
      "tres más propósitos de los dos las comedimientos de los dos de los de la vida, arrojó las alforjas de los ojos de su amo, el cual, como lo cual visto en el suelo. pero no sé qué tiempos que con los de los dos pastores con toda la cabeza de la\n",
      "\n",
      "DIVERSITY:  1\n",
      "mire vuestra merced a los amorosos que\n",
      "sabes:\n",
      "\n",
      "todos estos en el mundo que a caballero andante como los pensamientos de los escuderos; según lo menos, murmurono -replicó don quijote-; y, dándole más determinaciones ni pensamientos y grandes temerosos bondanes de mi padre de su mano a ella con dársele cuán gileo no le\n",
      "hallaré dos herederos estoy dijo:\n",
      "\n",
      "-uno miraba, goloquio con que impresas, inaunitas com\n",
      "\n",
      "DIVERSITY:  1.2\n",
      "mire vuestra merced pagase lotario desde el temerle, que tienen por allí a los negocios\n",
      "alientos.\n",
      "\n",
      "-esá entiendo yo que tengo\n",
      "todo pulto, me dieron y mayores\n",
      "judaban ser moros, del nombre no hay pala las que cargaten bien los\n",
      "unos hubiesen resuntotido.\n",
      "\n",
      "-con todo aquello otro cuantos vos habían dicho, siempre ayudaría aquella jaez, y no ha de determinar tiene; mi otra tendrá ya la duquesa, donde le dio m\n",
      "\n",
      "DIVERSITY:  0.2\n",
      "de lo que sucedió a su amo, y el cura que el caballero del mundo. en lo que dijo el cura, y el cura que el caballero de la mano de la cabeza, y el cura que el caballero de la mano, sino que en los demás de la mano de la mano, que no se le diga que en la cabeza de la mano, y el cura que en la mitad del caballero de la mano de la mano de la mano de la cabeza, y el cura que le dijo a su amo, y aun a los demás de los \n",
      "\n",
      "DIVERSITY:  0.5\n",
      "de lo que sucedió al caballero de la cabeza, y los demás personajes de los dos de los amorosos que ellas no se ha de comparar a su caballero, sino al lugar de las mujeres y de los reyes, que no se hacen y contra un caballero del cura y desta manera me ha de ser de la mano, sino al caballero de la cabeza, y de que todos los dos menos de la comparable con un lugar de los ojos, que le dijo:\n",
      "\n",
      "-a lo que respondió que\n",
      "\n",
      "DIVERSITY:  1\n",
      "de lo que sucedió a leonera: don quijote\n",
      "\n",
      "   muy diocumpro en la cabeza, que los cuadrilleros andantes en la tal hijos, caballeros andantes en la memoria, sino\n",
      "palabras que pasadas de la orden de su numbre, mandayo a una manera por cosesisión, como puedo enviendo de coroas, y que, entre los trabajotes apartados, y el juicio, finca que se representa de lo que puede hacer la mar. el\n",
      "cura y aun encerrados por\n",
      "el \n",
      "\n",
      "DIVERSITY:  1.2\n",
      "de lo que sucedió a\n",
      "la cabeza.\n",
      "\n",
      "-y -ni encubre el labrador confesó por la jarr, y vieron\n",
      "en camila; y, ciéndole luego, pero no sé la miración, que\n",
      "yo sé qué mi mal desnudachén; y con el cabo le\n",
      "volviese mucho sus madres otras cuantas ejurcicios le\n",
      "odejas.\n",
      "\n",
      "-busca salió de aquellos sobre esta trabajo.\n",
      "\n",
      "-y, volviéndose, asimismo no pudienno valentísimamente quedaron ha quedaba, puesto en sus servi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = 'mire vuestra merced '\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "\n",
    "sentence = 'de lo que sucedió a'\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "\n",
    "sentence = 'de allí a poco come'\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "mire vuestra merced decís, y que se le pareció que en la cabeza de la cabeza, y el caballero del caballero de la cabeza, y los demás de los demás de los demás de los demás de la mano de la mujer de la mano de la mano, y aun a su señora dulcinea del toboso, y el cura que el caballero de la mano que le pareció que estaba en la misma cosa que en la mitad del caballero de la mano, se le dijo:\n",
    "\n",
    "-no sé -respondi\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "mire vuestra merced que no está en la cabeza. pero, en efeto, pues todo aquello que está en el mundo que cada uno debía\n",
    "de ser con la misma sierra parte de la muerte de la mano, con todo el mundo me la conoció, que tenía por el primero que están el rostro en las manos, y al señor don quijote -respondió sancho-, porque en la puerta de los sucesos del buen estado de la desencantada de los ojos.\n",
    "\n",
    "-¿qué mal \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "mire vuestra merced que, aunque, desdicho y tiempo viene, a\n",
    "cuya cabeza destos tres, los informaciones que dejara de serle, que me fueron? ¿admirado,\n",
    "\n",
    "y, creyendo que me va y enfermo. y esto que tienen entonces las requiebros que algunos limpios en un ampeoso como si improvisentes en sus insimulables y en los míos al que el honesto, en la mano de mucho premio y don quijote fingióno los dos o sabidores, y,\n",
    "acom\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "mire vuestra merced paso que no sa cluero-. subió, señor,\n",
    "le hubiera vuelto el duque, que pica, yo fue pose�ría de guardar cierto. para los demás, esperando la misma tragua debe de haberlas hallado su santa hijo ni debían para mí, porque yo hay de platar pre subir tú turba -dijo el deleitable-; otras porturas, sino como alabanzas y\n",
    "comedimientos puciese\n",
    "los días de lo que von mirado después de\n",
    "visión de\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de lo que sucedió a la mano de la mano de la mano de los ojos de la mano de la mano de la mano, y aun más que se le dijese la mano de la mano y en la mitad del caballero del caballero de la mano de la mano, y que el parte de la mano, y que en la mitad del caballero de la mano, sino a la puerta de la mano de la mano de la mano de los demás de la cabeza, y el cura y la cabeza de la mano y en la cabeza de la cabeza, \n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de lo que sucedió a camila, por ser tan alta de la mano y en voz brazo? ¿qué es lo que pudiera, señor don quijote que en las fermosuras que después que le sacarán con don quijote había de ser la duquesa, la cual no le había de ser muy buena como gausa, y si este deseo de ser mejor que en la cabeza está a los dos de la mano, y su amo no le puede dar a su casa por los manos de mano, diciendo:\n",
    "\n",
    "-pues, ¿qué \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de lo que sucedió a las más faltas cuentan haciendo:\n",
    "\n",
    "-�qcorrían por esto, bueno -respondió cómo está aquel mano traía furia por ella estajo\n",
    "más que andaba y fingión de sus apartieron de modo que no\n",
    "hay vasto qué hizo con galdáis que soy sin duda, duque ni en la más\n",
    "buena gran señora dulcinea; y así lo han don quijote y no bastaba junto a nuestra\n",
    "señora dulcinea, ahora\n",
    "la entienda, el aposento a\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de lo que sucedió al\n",
    "admiración, dijo:\n",
    "\n",
    "-eso me esplevo en razón encantada, y su venimo tiempo, que pasaba tan hirtoria. es\n",
    "\n",
    "don quijote, le dijo:\n",
    "\n",
    "y cuando jamás:\n",
    "  si ya entienden en mi padre desde aquí vean fuerza, como yo\n",
    "costa yo he oído decir, el rey\n",
    "\n",
    "vención que ésto, que lo més comenzó a vuestra merced colgaré por la orden y parece que\n",
    "pudieron semplarse. el cual, si de la mono.\n",
    "acudi�\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de allí a poco comenzó a su caballero andante, que en la más hermosa de la mano, y el cura que el mundo tenía con el cura y el de la mano de la cabeza de la mano de la mano de la mano de la mano de la cabeza, y a los demás de los cuatro de la mano de la mano de la mano y en la mitad del caballero de la cabeza, y le dijo:\n",
    "\n",
    "-¡oh sancho -dijo el cura-, que no se le habían de ser manos de los demás de los de la\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de allí a poco comenzó a la vida de la mano de\n",
    "arriba al mundo. pero, con todo eso, ha de ser el rostro de las linajes de su escudero. por el rey en el mundo de la industria que en su caballero andante; y, aunque se volvió a camila y de la mano de la muerte de mi cabeza? y así, como el tal caballero andante, que los demás juramentos, y con la mitad del toboso, y así, por decir que os son de allí a los deseos \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de allí a poco comenzó\n",
    "a dos crazos de entender que tratar la nueva al desde nuevas andantes de solos con los detros donde me hubieran\n",
    "de subir la sumiera y rabia la duquesa\n",
    "   a un hacer con un gato, que le\n",
    "dijo juntar la locura para su\n",
    "amo, para que quisieron decir alguna, vino\n",
    "todas aquellos demás caballeros. pero, sancho, tanto, viendo camila la primero tiene: prodición\n",
    "que yo le dio caminar las belloz\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de allí a poco comer. otros días se le pidía hablar de cerra\n",
    "   que el gate predice\n",
    "y otras puntas del cordel bestia.\n",
    "\n",
    "-yo no por eso, decaría sobre la venta, porque él saslos demás sin él que\n",
    "llevase a buscar de la suma, sancho milático, cuando posían los zogados. y si así, mi rendido a\n",
    "cuerpo, ni en llopar salió el baece-, que el hábísimo que viene por\n",
    "los tratas y tontos que sean, y el de los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras2_py36]",
   "language": "python",
   "name": "conda-env-keras2_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
