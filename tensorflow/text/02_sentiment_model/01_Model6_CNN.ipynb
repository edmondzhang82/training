{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment model with CNNs\n",
    "\n",
    " - Use Convolutions to create a sentiment model.\n",
    " - Based on: http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "data_path='/home/ubuntu/data/training/keras/aclImdb/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator of list of files in a folder and subfolders\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "def gen_find(filefilter, top):\n",
    "    for path, dirlist, filelist in os.walk(top):\n",
    "        for name in fnmatch.filter(filelist, filefilter):\n",
    "            yield os.path.join(path, name)\n",
    "\n",
    "def read_sentences(path):\n",
    "    sentences = []\n",
    "    sentences_list = gen_find(\"*.txt\", path)\n",
    "    for ff in sentences_list:\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    return sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read train sentences and create train target\n",
    "\n",
    "sentences_trn_pos = read_sentences(data_path+'train/pos/')\n",
    "sentences_trn_neg = read_sentences(data_path+'train/neg/')\n",
    "\n",
    "sentences_trn_ini = sentences_trn_pos + sentences_trn_neg\n",
    "print('max_document_length trn: ', max([len(x.split(\" \")) for x in sentences_trn_ini]))\n",
    "\n",
    "y_trn_ini = np.array([[1.,0.]]*len(sentences_trn_pos) + [[0.,1.]]*len(sentences_trn_neg), dtype=np.float32)\n",
    "\n",
    "print(y_trn_ini.shape)\n",
    "print(y_trn_ini)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffle train data\n",
    "from sklearn.utils import shuffle\n",
    "sentences_trn, y_trn = shuffle(sentences_trn_ini, y_trn_ini)\n",
    "\n",
    "print(y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read test sentences and create test target\n",
    "\n",
    "sentences_tst_pos = read_sentences(data_path+'test/pos/')\n",
    "sentences_tst_neg = read_sentences(data_path+'test/neg/')\n",
    "\n",
    "sentences_tst = sentences_tst_pos + sentences_tst_neg\n",
    "print('max_document_length tst: ', max([len(x.split(\" \")) for x in sentences_tst]))\n",
    "\n",
    "y_tst = np.array([[1.,0.]]*len(sentences_tst_pos) + [[0.,1.]]*len(sentences_tst_neg), dtype=np.float32)\n",
    "\n",
    "print(y_tst.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build vocabulary and transform sentences\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "sequence_length  =100\n",
    "\n",
    "# Train vocab and apply to train\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(sequence_length, min_frequency=10)\n",
    "X_trn = np.array(list(vocab_processor.fit_transform(sentences_trn)))\n",
    "\n",
    "# Apply trained vocab to test \n",
    "X_tst = np.array(list(vocab_processor.transform(sentences_tst)))\n",
    "\n",
    "# Size vocabulary\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "\n",
    "# Check results\n",
    "print('Vocab size: ', vocab_size)\n",
    "print('X trn shape: ', X_trn.shape)\n",
    "print('X tst shape: ', X_tst.shape)\n",
    "print('First sentence: ', X_trn[0])\n",
    "print('house id: ', vocab_processor.vocabulary_.get('house'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The model\n",
    " - Declare placeholders\n",
    " - Embedding layers\n",
    " - Convolutional and max pooling layers\n",
    " - Merge convolutions oputput\n",
    " - Dense layer to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "embedding_size = 128\n",
    "num_filters = 32\n",
    "filter_sizes = [3, 6, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start an interactive session\n",
    "gpu_options = tf.GPUOptions(allow_growth = True)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "input_x = tf.placeholder(tf.int32, shape=[None, sequence_length], name=\"input_x\")\n",
    "print(input_x)\n",
    "\n",
    "input_y = tf.placeholder(tf.int32, shape=[None, 2], name=\"input_y\")\n",
    "print(input_y)\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "with tf.name_scope(\"embedding\"):\n",
    "    W_embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W_embedding\")\n",
    "    embedded_chars = tf.nn.embedding_lookup(W_embedding, input_x)\n",
    "    print(embedded_chars)\n",
    "    \n",
    "    # Add an aditional dimension to match to the convolution requirements\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    print(embedded_chars_expanded)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a convolution + maxpool layer for each filter size\n",
    "\n",
    "def conv_layer(x, size_x=2, size_y=2, input_channels=1, output_channels=32):\n",
    "    W_conv = tf.Variable(tf.truncated_normal([size_x, size_y, input_channels, output_channels], stddev=0.1), name='W')\n",
    "    \n",
    "    b_conv = tf.Variable(tf.constant(0.1, shape=[output_channels]), name='b')\n",
    "    \n",
    "    conv_out = tf.nn.relu(tf.nn.conv2d(x, W_conv, strides=[1, 1, 1, 1], padding='VALID') + b_conv, name='conv')\n",
    "    \n",
    "    pooled = tf.nn.max_pool(conv_out, \n",
    "                            ksize=[1, sequence_length - filter_size + 1, 1, 1], \n",
    "                            strides=[1, 1, 1, 1], \n",
    "                            padding='VALID', \n",
    "                            name=\"pool\")\n",
    "    return pooled\n",
    "\n",
    "\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "        pooled = conv_layer(embedded_chars_expanded, size_x=filter_size, size_y=embedding_size, input_channels=1, output_channels=num_filters)\n",
    "        pooled_outputs.append(pooled)\n",
    "\n",
    "print(pooled_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine all the pooled features\n",
    "h_pool = tf.concat(3, pooled_outputs)\n",
    "print(h_pool)\n",
    "\n",
    "# Reshape to flat the tensor: f\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "print(h_pool_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add dropout\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Final (unnormalized) scores and predictions\n",
    "with tf.name_scope(\"output\"):\n",
    "    W = tf.get_variable(\"W\", shape=[num_filters_total, 2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[2]), name=\"b\")\n",
    "    \n",
    "    # scores = h_drop * W + b\n",
    "    scores = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\n",
    "    print(scores)\n",
    "    \n",
    "    # predictions: position of the max value of scores\n",
    "    predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the Mean of the cross-entropy loss in the batch\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(scores, input_y), name='loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy: percent of correct predictions\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, tf.argmax(input_y, 1)), \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training procces\n",
    " - Create a generator to create the batches of inputs and targets. Train & test.\n",
    " - Iterate over the data to train the model\n",
    "   - Each iteration over all the data is an epoch\n",
    "   - For each epoch iterate overt the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_iter(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for inputs (X) and targets (y) of batch_size size.\n",
    "    \"\"\"\n",
    "    data_size = len(X)\n",
    "    # Shuffle the data at each epoch\n",
    "    shuffle_indices = np.random.permutation(np.arange(data_size))    \n",
    "    shuffled_X = X[shuffle_indices]\n",
    "    shuffled_y = y[shuffle_indices]\n",
    "        \n",
    "    num_batches = int((data_size-1)/batch_size) + 1\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        yield shuffled_X[start_index:end_index], shuffled_y[start_index:end_index]\n",
    "        \n",
    "# Test the generator function      \n",
    "b_iter= batch_iter(X_trn, y_trn, 2)\n",
    "print(b_iter.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Inicialization.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train proccess parameters\n",
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "\n",
    "loss_trn_epoch = []\n",
    "loss_tst_epoch = []\n",
    "acc_trn_epoch = []\n",
    "acc_tst_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_trn = []\n",
    "    acc_trn = []\n",
    "    loss_tst = []\n",
    "    acc_tst = []\n",
    "    \n",
    "    # Train step\n",
    "    for x_batch, y_batch in batch_iter(X_trn, y_trn, batch_size):\n",
    "        train_step.run(feed_dict={input_x: x_batch, input_y: y_batch, dropout_keep_prob: 0.5})\n",
    "        loss_step, acc_step = sess.run([loss, accuracy], \n",
    "                                       feed_dict={input_x: x_batch, input_y: y_batch, dropout_keep_prob: 1})\n",
    "        loss_trn += [loss_step]\n",
    "        acc_trn += [acc_step]\n",
    "    \n",
    "    # Validation step\n",
    "    for x_batch_test, y_batch_test in batch_iter(X_tst, y_tst, batch_size):\n",
    "        loss_step, acc_step = sess.run([loss, accuracy], \n",
    "                                       feed_dict={input_x: x_batch_test, input_y: y_batch_test, dropout_keep_prob: 1})\n",
    "        loss_tst += [loss_step]\n",
    "        acc_tst += [acc_step]\n",
    "    \n",
    "    # Summary\n",
    "    print(epoch, np.mean(loss_trn), np.mean(acc_trn), np.mean(loss_tst), np.mean(acc_tst))\n",
    "    loss_trn_epoch += [np.mean(loss_trn)]\n",
    "    loss_tst_epoch += [np.mean(loss_tst)]\n",
    "    acc_trn_epoch += [np.mean(acc_trn)]\n",
    "    acc_tst_epoch += [np.mean(acc_tst)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(loss_trn_epoch)\n",
    "plt.plot(loss_tst_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.plot(acc_trn_epoch)\n",
    "plt.plot(acc_tst_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
