{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 Conchylicultor. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Load the cornell movie dialog corpus.\n",
    "Available from here:\n",
    "http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "\"\"\"\n",
    "\n",
    "class CornellData:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dirName):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dirName (string): directory where to load the corpus\n",
    "        \"\"\"\n",
    "        self.lines = {}\n",
    "        self.conversations = []\n",
    "\n",
    "        MOVIE_LINES_FIELDS = [\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n",
    "        MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n",
    "\n",
    "        self.lines = self.loadLines(dirName + \"movie_lines.txt\", MOVIE_LINES_FIELDS)\n",
    "        self.conversations = self.loadConversations(dirName + \"movie_conversations.txt\", MOVIE_CONVERSATIONS_FIELDS)\n",
    "\n",
    "        # TODO: Cleaner program (merge copy-paste) !!\n",
    "\n",
    "    def loadLines(self, fileName, fields):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fileName (str): file to load\n",
    "            field (set<str>): fields to extract\n",
    "        Return:\n",
    "            dict<dict<str>>: the extracted fields for each line\n",
    "        \"\"\"\n",
    "        lines = {}\n",
    "\n",
    "        with open(fileName, 'r', encoding='iso-8859-1') as f:  # TODO: Solve Iso encoding pb !\n",
    "            for line in f:\n",
    "                values = line.split(\" +++$+++ \")\n",
    "\n",
    "                # Extract fields\n",
    "                lineObj = {}\n",
    "                for i, field in enumerate(fields):\n",
    "                    lineObj[field] = values[i]\n",
    "\n",
    "                lines[lineObj['lineID']] = lineObj\n",
    "\n",
    "        return lines\n",
    "\n",
    "    def loadConversations(self, fileName, fields):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fileName (str): file to load\n",
    "            field (set<str>): fields to extract\n",
    "        Return:\n",
    "            dict<dict<str>>: the extracted fields for each line\n",
    "        \"\"\"\n",
    "        conversations = []\n",
    "\n",
    "        with open(fileName, 'r', encoding='iso-8859-1') as f:  # TODO: Solve Iso encoding pb !\n",
    "            for line in f:\n",
    "                values = line.split(\" +++$+++ \")\n",
    "\n",
    "                # Extract fields\n",
    "                convObj = {}\n",
    "                for i, field in enumerate(fields):\n",
    "                    convObj[field] = values[i]\n",
    "\n",
    "                lineIds = convObj[\"utteranceIDs\"][2:-3].split(\"', '\")\n",
    "\n",
    "                #print(convObj[\"utteranceIDs\"])\n",
    "                #for lineId in lineIds:\n",
    "                    #print(lineId, end=' ')\n",
    "                #print()\n",
    "\n",
    "                # Reassemble lines\n",
    "                convObj[\"lines\"] = []\n",
    "                for lineId in lineIds:\n",
    "                    convObj[\"lines\"].append(self.lines[lineId])\n",
    "\n",
    "                conversations.append(convObj)\n",
    "\n",
    "        return conversations\n",
    "\n",
    "    def getConversations(self):\n",
    "        return self.conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 Conchylicultor. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Loads the dialogue corpus, builds the vocabulary\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import nltk  # For tokenize\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import pickle  # Saving the data\n",
    "import math  # For float comparison\n",
    "import os  # Checking file existance\n",
    "import random\n",
    "\n",
    "#from chatbot.cornelldata import CornellData\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    \"\"\"Struct containing batches info\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.encoderSeqs = []\n",
    "        self.decoderSeqs = []\n",
    "        self.targetSeqs = []\n",
    "        self.weights = []\n",
    "\n",
    "\n",
    "class TextData:\n",
    "    \"\"\"Dataset class\n",
    "    Warning: No vocabulary limit\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"Load all conversations\n",
    "        Args:\n",
    "            args: parameters of the model\n",
    "        \"\"\"\n",
    "        # Model parameters\n",
    "        self.args = args\n",
    "\n",
    "        # Path variables\n",
    "        self.corpusDir = os.path.join(self.args.rootDir, 'data/cornell/')\n",
    "        self.samplesDir = os.path.join(self.args.rootDir, 'data/samples/')\n",
    "        self.samplesName = self._constructName()\n",
    "\n",
    "        self.padToken = -1  # Padding\n",
    "        self.goToken = -1  # Start of sequence\n",
    "        self.eosToken = -1  # End of sequence\n",
    "        self.unknownToken = -1  # Word dropped from vocabulary\n",
    "\n",
    "        self.trainingSamples = []  # 2d array containing each question and his answer [[input,target]]\n",
    "\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}  # For a rapid conversion\n",
    "\n",
    "        self.loadCorpus(self.samplesDir)\n",
    "\n",
    "        # Plot some stats:\n",
    "        print('Loaded: {} words, {} QA'.format(len(self.word2id), len(self.trainingSamples)))\n",
    "\n",
    "        if self.args.playDataset:\n",
    "            self.playDataset()\n",
    "\n",
    "    def _constructName(self):\n",
    "        \"\"\"Return the name of the dataset that the program should use with the current parameters.\n",
    "        Computer from the base name, the given tag (self.args.datasetTag) and the sentence length\n",
    "        \"\"\"\n",
    "        baseName = 'dataset'\n",
    "        if self.args.datasetTag:\n",
    "            baseName += '-' + self.args.datasetTag\n",
    "        return baseName + '-' + str(self.args.maxLength) + '.pkl'\n",
    "\n",
    "    def makeLighter(self, ratioDataset):\n",
    "        \"\"\"Only keep a small fraction of the dataset, given by the ratio\n",
    "        \"\"\"\n",
    "        if not math.isclose(ratioDataset, 1.0):\n",
    "            self.shuffle()  # Really ?\n",
    "            print('WARNING: Ratio feature not implemented !!!')\n",
    "        pass\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"Shuffle the training samples\n",
    "        \"\"\"\n",
    "        print(\"Shuffling the dataset...\")\n",
    "        random.shuffle(self.trainingSamples)\n",
    "\n",
    "    def _createBatch(self, samples):\n",
    "        \"\"\"Create a single batch from the list of sample. The batch size is automatically defined by the number of\n",
    "        samples given.\n",
    "        The inputs should already be inverted. The target should already have <go> and <eos>\n",
    "        Warning: This function should not make direct calls to args.batchSize !!!\n",
    "        Args:\n",
    "            samples (list<Obj>): a list of samples, each sample being on the form [input, target]\n",
    "        Return:\n",
    "            Batch: a batch object en\n",
    "        \"\"\"\n",
    "\n",
    "        batch = Batch()\n",
    "        batchSize = len(samples)\n",
    "\n",
    "        # Create the batch tensor\n",
    "        for i in range(batchSize):\n",
    "            # Unpack the sample\n",
    "            sample = samples[i]\n",
    "            if not self.args.test and self.args.watsonMode:  # Watson mode: invert question and answer\n",
    "                sample = list(reversed(sample))\n",
    "            batch.encoderSeqs.append(list(reversed(sample[0])))  # Reverse inputs (and not outputs), little trick as defined on the original seq2seq paper\n",
    "            batch.decoderSeqs.append([self.goToken] + sample[1] + [self.eosToken])  # Add the <go> and <eos> tokens\n",
    "            batch.targetSeqs.append(batch.decoderSeqs[-1][1:])  # Same as decoder, but shifted to the left (ignore the <go>)\n",
    "\n",
    "            # Long sentences should have been filtered during the dataset creation\n",
    "            assert len(batch.encoderSeqs[i]) <= self.args.maxLengthEnco\n",
    "            assert len(batch.decoderSeqs[i]) <= self.args.maxLengthDeco\n",
    "\n",
    "            # Add padding & define weight\n",
    "            batch.encoderSeqs[i]   = [self.padToken] * (self.args.maxLengthEnco  - len(batch.encoderSeqs[i])) + batch.encoderSeqs[i]  # Left padding for the input\n",
    "            batch.weights.append([1.0] * len(batch.targetSeqs[i]) + [0.0] * (self.args.maxLengthDeco - len(batch.targetSeqs[i])))\n",
    "            batch.decoderSeqs[i] = batch.decoderSeqs[i] + [self.padToken] * (self.args.maxLengthDeco - len(batch.decoderSeqs[i]))\n",
    "            batch.targetSeqs[i]  = batch.targetSeqs[i]  + [self.padToken] * (self.args.maxLengthDeco - len(batch.targetSeqs[i]))\n",
    "\n",
    "        # Simple hack to reshape the batch\n",
    "        encoderSeqsT = []  # Corrected orientation\n",
    "        for i in range(self.args.maxLengthEnco):\n",
    "            encoderSeqT = []\n",
    "            for j in range(batchSize):\n",
    "                encoderSeqT.append(batch.encoderSeqs[j][i])\n",
    "            encoderSeqsT.append(encoderSeqT)\n",
    "        batch.encoderSeqs = encoderSeqsT\n",
    "\n",
    "        decoderSeqsT = []\n",
    "        targetSeqsT = []\n",
    "        weightsT = []\n",
    "        for i in range(self.args.maxLengthDeco):\n",
    "            decoderSeqT = []\n",
    "            targetSeqT = []\n",
    "            weightT = []\n",
    "            for j in range(batchSize):\n",
    "                decoderSeqT.append(batch.decoderSeqs[j][i])\n",
    "                targetSeqT.append(batch.targetSeqs[j][i])\n",
    "                weightT.append(batch.weights[j][i])\n",
    "            decoderSeqsT.append(decoderSeqT)\n",
    "            targetSeqsT.append(targetSeqT)\n",
    "            weightsT.append(weightT)\n",
    "        batch.decoderSeqs = decoderSeqsT\n",
    "        batch.targetSeqs = targetSeqsT\n",
    "        batch.weights = weightsT\n",
    "\n",
    "        # # Debug\n",
    "        # self.printBatch(batch)  # Input inverted, padding should be correct\n",
    "        # print(self.sequence2str(samples[0][0]))\n",
    "        # print(self.sequence2str(samples[0][1]))  # Check we did not modified the original sample\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def getBatches(self):\n",
    "        \"\"\"Prepare the batches for the current epoch\n",
    "        Return:\n",
    "            list<Batch>: Get a list of the batches for the next epoch\n",
    "        \"\"\"\n",
    "        self.shuffle()\n",
    "\n",
    "        batches = []\n",
    "\n",
    "        def genNextSamples():\n",
    "            \"\"\" Generator over the mini-batch training samples\n",
    "            \"\"\"\n",
    "            for i in range(0, self.getSampleSize(), self.args.batchSize):\n",
    "                yield self.trainingSamples[i:min(i + self.args.batchSize, self.getSampleSize())]\n",
    "\n",
    "        for samples in genNextSamples():\n",
    "            batch = self._createBatch(samples)\n",
    "            batches.append(batch)\n",
    "        return batches\n",
    "\n",
    "    def getSampleSize(self):\n",
    "        \"\"\"Return the size of the dataset\n",
    "        Return:\n",
    "            int: Number of training samples\n",
    "        \"\"\"\n",
    "        return len(self.trainingSamples)\n",
    "\n",
    "    def getVocabularySize(self):\n",
    "        \"\"\"Return the number of words present in the dataset\n",
    "        Return:\n",
    "            int: Number of word on the loader corpus\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def loadCorpus(self, dirName):\n",
    "        \"\"\"Load/create the conversations data\n",
    "        Args:\n",
    "            dirName (str): The directory where to load/save the model\n",
    "        \"\"\"\n",
    "        datasetExist = False\n",
    "        if os.path.exists(os.path.join(dirName, self.samplesName)):\n",
    "            datasetExist = True\n",
    "\n",
    "        if not datasetExist:  # First time we load the database: creating all files\n",
    "            print('Training samples not found. Creating dataset...')\n",
    "            # Corpus creation\n",
    "            cornellData = CornellData(self.corpusDir)\n",
    "            self.createCorpus(cornellData.getConversations())\n",
    "\n",
    "            # Saving\n",
    "            print('Saving dataset...')\n",
    "            self.saveDataset(dirName)  # Saving tf samples\n",
    "        else:\n",
    "            print('Loading dataset from {}...'.format(dirName))\n",
    "            self.loadDataset(dirName)\n",
    "\n",
    "        assert self.padToken == 0\n",
    "\n",
    "    def saveDataset(self, dirName):\n",
    "        \"\"\"Save samples to file\n",
    "        Args:\n",
    "            dirName (str): The directory where to load/save the model\n",
    "        \"\"\"\n",
    "\n",
    "        with open(os.path.join(dirName, self.samplesName), 'wb') as handle:\n",
    "            data = {  # Warning: If adding something here, also modifying loadDataset\n",
    "                \"word2id\": self.word2id,\n",
    "                \"id2word\": self.id2word,\n",
    "                \"trainingSamples\": self.trainingSamples\n",
    "                }\n",
    "            pickle.dump(data, handle, -1)  # Using the highest protocol available\n",
    "\n",
    "    def loadDataset(self, dirName):\n",
    "        \"\"\"Load samples from file\n",
    "        Args:\n",
    "            dirName (str): The directory where to load the model\n",
    "        \"\"\"\n",
    "        with open(os.path.join(dirName, self.samplesName), 'rb') as handle:\n",
    "            data = pickle.load(handle)  # Warning: If adding something here, also modifying saveDataset\n",
    "            self.word2id = data[\"word2id\"]\n",
    "            self.id2word = data[\"id2word\"]\n",
    "            self.trainingSamples = data[\"trainingSamples\"]\n",
    "\n",
    "            self.padToken = self.word2id[\"<pad>\"]\n",
    "            self.goToken = self.word2id[\"<go>\"]\n",
    "            self.eosToken = self.word2id[\"<eos>\"]\n",
    "            self.unknownToken = self.word2id[\"<unknown>\"]  # Restore special words\n",
    "\n",
    "    def createCorpus(self, conversations):\n",
    "        \"\"\"Extract all data from the given vocabulary\n",
    "        \"\"\"\n",
    "        # Add standard tokens\n",
    "        self.padToken = self.getWordId(\"<pad>\")  # Padding (Warning: first things to add > id=0 !!)\n",
    "        self.goToken = self.getWordId(\"<go>\")  # Start of sequence\n",
    "        self.eosToken = self.getWordId(\"<eos>\")  # End of sequence\n",
    "        self.unknownToken = self.getWordId(\"<unknown>\")  # Word dropped from vocabulary\n",
    "\n",
    "        # Preprocessing data\n",
    "\n",
    "        for conversation in tqdm(conversations, desc=\"Extract conversations\"):\n",
    "            self.extractConversation(conversation)\n",
    "\n",
    "        # The dataset will be saved in the same order it has been extracted\n",
    "\n",
    "    def extractConversation(self, conversation):\n",
    "        \"\"\"Extract the sample lines from the conversations\n",
    "        Args:\n",
    "            conversation (Obj): a conversation object containing the lines to extract\n",
    "        \"\"\"\n",
    "\n",
    "        # Iterate over all the lines of the conversation\n",
    "        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n",
    "            inputLine  = conversation[\"lines\"][i]\n",
    "            targetLine = conversation[\"lines\"][i+1]\n",
    "\n",
    "            inputWords  = self.extractText(inputLine[\"text\"])\n",
    "            targetWords = self.extractText(targetLine[\"text\"], True)\n",
    "\n",
    "            if inputWords and targetWords:  # Filter wrong samples (if one of the list is empty)\n",
    "                self.trainingSamples.append([inputWords, targetWords])\n",
    "\n",
    "    def extractText(self, line, isTarget=False):\n",
    "        \"\"\"Extract the words from a sample lines\n",
    "        Args:\n",
    "            line (str): a line containing the text to extract\n",
    "            isTarget (bool): Define the question on the answer\n",
    "        Return:\n",
    "            list<int>: the list of the word ids of the sentence\n",
    "        \"\"\"\n",
    "        words = []\n",
    "\n",
    "        # Extract sentences\n",
    "        sentencesToken = nltk.sent_tokenize(line)\n",
    "\n",
    "        # We add sentence by sentence until we reach the maximum length\n",
    "        for i in range(len(sentencesToken)):\n",
    "            # If question: we only keep the last sentences\n",
    "            # If answer: we only keep the first sentences\n",
    "            if not isTarget:\n",
    "                i = len(sentencesToken)-1 - i\n",
    "\n",
    "            tokens = nltk.word_tokenize(sentencesToken[i])\n",
    "\n",
    "            # If the total length is not too big, we still can add one more sentence\n",
    "            if len(words) + len(tokens) <= self.args.maxLength:\n",
    "                tempWords = []\n",
    "                for token in tokens:\n",
    "                    tempWords.append(self.getWordId(token))  # Create the vocabulary and the training sentences\n",
    "\n",
    "                if isTarget:\n",
    "                    words = words + tempWords\n",
    "                else:\n",
    "                    words = tempWords + words\n",
    "            else:\n",
    "                break  # We reach the max length already\n",
    "\n",
    "        return words\n",
    "\n",
    "    def getWordId(self, word, create=True):\n",
    "        \"\"\"Get the id of the word (and add it to the dictionary if not existing). If the word does not exist and\n",
    "        create is set to False, the function will return the unknownToken value\n",
    "        Args:\n",
    "            word (str): word to add\n",
    "            create (Bool): if True and the word does not exist already, the world will be added\n",
    "        Return:\n",
    "            int: the id of the word created\n",
    "        \"\"\"\n",
    "        # Should we Keep only words with more than one occurrence ?\n",
    "\n",
    "        word = word.lower()  # Ignore case\n",
    "\n",
    "        # Get the id if the word already exist\n",
    "        wordId = self.word2id.get(word, -1)\n",
    "\n",
    "        # If not, we create a new entry\n",
    "        if wordId == -1:\n",
    "            if create:\n",
    "                wordId = len(self.word2id)\n",
    "                self.word2id[word] = wordId\n",
    "                self.id2word[wordId] = word\n",
    "            else:\n",
    "                wordId = self.unknownToken\n",
    "\n",
    "        return wordId\n",
    "\n",
    "    def printBatch(self, batch):\n",
    "        \"\"\"Print a complete batch, useful for debugging\n",
    "        Args:\n",
    "            batch (Batch): a batch object\n",
    "        \"\"\"\n",
    "        print('----- Print batch -----')\n",
    "        for i in range(len(batch.encoderSeqs[0])):  # Batch size\n",
    "            print('Encoder: {}'.format(self.batchSeq2str(batch.encoderSeqs, seqId=i)))\n",
    "            print('Decoder: {}'.format(self.batchSeq2str(batch.decoderSeqs, seqId=i)))\n",
    "            print('Targets: {}'.format(self.batchSeq2str(batch.targetSeqs, seqId=i)))\n",
    "            print('Weights: {}'.format(' '.join([str(weight) for weight in [batchWeight[i] for batchWeight in batch.weights]])))\n",
    "\n",
    "    def sequence2str(self, sequence, clean=False, reverse=False):\n",
    "        \"\"\"Convert a list of integer into a human readable string\n",
    "        Args:\n",
    "            sequence (list<int>): the sentence to print\n",
    "            clean (Bool): if set, remove the <go>, <pad> and <eos> tokens\n",
    "            reverse (Bool): for the input, option to restore the standard order\n",
    "        Return:\n",
    "            str: the sentence\n",
    "        \"\"\"\n",
    "\n",
    "        if not sequence:\n",
    "            return ''\n",
    "\n",
    "        if not clean:\n",
    "            return ' '.join([self.id2word[idx] for idx in sequence])\n",
    "\n",
    "        sentence = []\n",
    "        for wordId in sequence:\n",
    "            if wordId == self.eosToken:  # End of generated sentence\n",
    "                break\n",
    "            elif wordId != self.padToken and wordId != self.goToken:\n",
    "                sentence.append(self.id2word[wordId])\n",
    "\n",
    "        if reverse:  # Reverse means input so no <eos> (otherwise pb with previous early stop)\n",
    "            sentence.reverse()\n",
    "\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "    def batchSeq2str(self, batchSeq, seqId=0, **kwargs):\n",
    "        \"\"\"Convert a list of integer into a human readable string.\n",
    "        The difference between the previous function is that on a batch object, the values have been reorganized as\n",
    "        batch instead of sentence.\n",
    "        Args:\n",
    "            batchSeq (list<list<int>>): the sentence(s) to print\n",
    "            seqId (int): the position of the sequence inside the batch\n",
    "            kwargs: the formatting options( See sequence2str() )\n",
    "        Return:\n",
    "            str: the sentence\n",
    "        \"\"\"\n",
    "        sequence = []\n",
    "        for i in range(len(batchSeq)):  # Sequence length\n",
    "            sequence.append(batchSeq[i][seqId])\n",
    "        return self.sequence2str(sequence, **kwargs)\n",
    "\n",
    "    def sentence2enco(self, sentence):\n",
    "        \"\"\"Encode a sequence and return a batch as an input for the model\n",
    "        Return:\n",
    "            Batch: a batch object containing the sentence, or none if something went wrong\n",
    "        \"\"\"\n",
    "\n",
    "        if sentence == '':\n",
    "            return None\n",
    "\n",
    "        # First step: Divide the sentence in token\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        if len(tokens) > self.args.maxLength:\n",
    "            return None\n",
    "\n",
    "        # Second step: Convert the token in word ids\n",
    "        wordIds = []\n",
    "        for token in tokens:\n",
    "            wordIds.append(self.getWordId(token, create=False))  # Create the vocabulary and the training sentences\n",
    "\n",
    "        # Third step: creating the batch (add padding, reverse)\n",
    "        batch = self._createBatch([[wordIds, []]])  # Mono batch, no target output\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def deco2sentence(self, decoderOutputs):\n",
    "        \"\"\"Decode the output of the decoder and return a human friendly sentence\n",
    "        decoderOutputs (list<np.array>):\n",
    "        \"\"\"\n",
    "        sequence = []\n",
    "\n",
    "        # Choose the words with the highest prediction score\n",
    "        for out in decoderOutputs:\n",
    "            sequence.append(np.argmax(out))  # Adding each predicted word ids\n",
    "\n",
    "        return sequence  # We return the raw sentence. Let the caller do some cleaning eventually\n",
    "\n",
    "    def playDataset(self):\n",
    "        \"\"\"Print a random dialogue from the dataset\n",
    "        \"\"\"\n",
    "        print('Randomly play samples:')\n",
    "        for i in range(self.args.playDataset):\n",
    "            idSample = random.randint(0, len(self.trainingSamples))\n",
    "            print('Q: {}'.format(self.sequence2str(self.trainingSamples[idSample][0])))\n",
    "            print('A: {}'.format(self.sequence2str(self.trainingSamples[idSample][1])))\n",
    "            print()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 Conchylicultor. All Rights Reserved.\n",
    "# Modifications copyright (C) 2016 Carlos Segura\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Model to predict the next sentence given an input sequence\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#from chatbot.textdata import Batch\n",
    "\n",
    "\n",
    "class ProjectionOp:\n",
    "    \"\"\" Single layer perceptron\n",
    "    Project input tensor on the output dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, scope=None, dtype=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            shape: a tuple (input dim, output dim)\n",
    "            scope (str): encapsulate variables\n",
    "            dtype: the weights type\n",
    "        \"\"\"\n",
    "        assert len(shape) == 2\n",
    "\n",
    "        self.scope = scope\n",
    "\n",
    "        # Projection on the keyboard\n",
    "        with tf.variable_scope('weights_' + self.scope):\n",
    "            self.W = tf.get_variable(\n",
    "                'weights',\n",
    "                shape,\n",
    "                # initializer=tf.truncated_normal_initializer()  # TODO: Tune value (fct of input size: 1/sqrt(input_dim))\n",
    "                dtype=dtype\n",
    "            )\n",
    "            self.b = tf.get_variable(\n",
    "                'bias',\n",
    "                shape[1],\n",
    "                initializer=tf.constant_initializer(),\n",
    "                dtype=dtype\n",
    "            )\n",
    "\n",
    "    def getWeights(self):\n",
    "        \"\"\" Convenience method for some tf arguments\n",
    "        \"\"\"\n",
    "        return self.W, self.b\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\" Project the output of the decoder into the vocabulary space\n",
    "        Args:\n",
    "            X (tf.Tensor): input value\n",
    "        \"\"\"\n",
    "        with tf.name_scope(self.scope):\n",
    "            return tf.matmul(X, self.W) + self.b\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Implementation of a seq2seq model.\n",
    "    Architecture:\n",
    "        Encoder/decoder\n",
    "        2 LTSM layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, textData):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            args: parameters of the model\n",
    "            textData: the dataset object\n",
    "        \"\"\"\n",
    "        print(\"Model creation...\")\n",
    "\n",
    "        self.textData = textData  # Keep a reference on the dataset\n",
    "        self.args = args  # Keep track of the parameters of the model\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        # Placeholders\n",
    "        self.encoderInputs  = None\n",
    "        self.decoderInputs  = None  # Same that decoderTarget plus the <go>\n",
    "        self.decoderTargets = None\n",
    "        self.decoderWeights = None  # Adjust the learning to the target sentence size\n",
    "\n",
    "        # Main operators\n",
    "        self.lossFct = None\n",
    "        self.optOp = None\n",
    "        self.outputs = None  # Outputs of the network, list of probability for each words\n",
    "\n",
    "        # Construct the graphs\n",
    "        self.buildNetwork()\n",
    "\n",
    "    def buildNetwork(self):\n",
    "        \"\"\" Create the computational graph\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Create name_scopes (for better graph visualisation)\n",
    "        # TODO: Use buckets (better perfs)\n",
    "\n",
    "        # Parameters of sampled softmax (needed for attention mechanism and a large vocabulary size)\n",
    "        outputProjection = None\n",
    "        # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "        if 0 < self.args.softmaxSamples < self.textData.getVocabularySize():\n",
    "            outputProjection = ProjectionOp(\n",
    "                (self.args.hiddenSize, self.textData.getVocabularySize()),\n",
    "                scope='softmax_projection',\n",
    "                dtype=self.dtype\n",
    "            )\n",
    "\n",
    "            def sampledSoftmax(inputs, labels):\n",
    "                labels = tf.reshape(labels, [-1, 1])  # Add one dimension (nb of true classes, here 1)\n",
    "\n",
    "                # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "                # avoid numerical instabilities.\n",
    "                localWt     = tf.cast(tf.transpose(outputProjection.W), tf.float32)\n",
    "                localB      = tf.cast(outputProjection.b,               tf.float32)\n",
    "                localInputs = tf.cast(inputs,                           tf.float32)\n",
    "\n",
    "                return tf.cast(\n",
    "                    tf.nn.sampled_softmax_loss(\n",
    "                        localWt,  # Should have shape [num_classes, dim]\n",
    "                        localB,\n",
    "                        localInputs,\n",
    "                        labels,\n",
    "                        self.args.softmaxSamples,  # The number of classes to randomly sample per batch\n",
    "                        self.textData.getVocabularySize()),  # The number of classes\n",
    "                    self.dtype)\n",
    "\n",
    "        # Creation of the rnn cell\n",
    "        encoDecoCell = tf.nn.rnn_cell.BasicLSTMCell(self.args.hiddenSize, state_is_tuple=True)  # Or GRUCell, LSTMCell(args.hiddenSize)\n",
    "        #encoDecoCell = tf.nn.rnn_cell.DropoutWrapper(encoDecoCell, input_keep_prob=1.0, output_keep_prob=1.0)  # TODO: Custom values (WARNING: No dropout when testing !!!)\n",
    "        encoDecoCell = tf.nn.rnn_cell.MultiRNNCell([encoDecoCell] * self.args.numLayers, state_is_tuple=True)\n",
    "\n",
    "        # Network input (placeholders)\n",
    "\n",
    "        with tf.name_scope('placeholder_encoder'):\n",
    "            self.encoderInputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(self.args.maxLengthEnco)]  # Batch size * sequence length * input dim\n",
    "\n",
    "        with tf.name_scope('placeholder_decoder'):\n",
    "            self.decoderInputs  = [tf.placeholder(tf.int32,   [None, ], name='inputs') for _ in range(self.args.maxLengthDeco)]  # Same sentence length for input and output (Right ?)\n",
    "            self.decoderTargets = [tf.placeholder(tf.int32,   [None, ], name='targets') for _ in range(self.args.maxLengthDeco)]\n",
    "            self.decoderWeights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in range(self.args.maxLengthDeco)]\n",
    "\n",
    "        # Define the network\n",
    "        # Here we use an embedding model, it takes integer as input and convert them into word vector for\n",
    "        # better word representation\n",
    "        decoderOutputs, states = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "            self.encoderInputs,  # List<[batch=?, inputDim=1]>, list of size args.maxLength\n",
    "            self.decoderInputs,  # For training, we force the correct output (feed_previous=False)\n",
    "            encoDecoCell,\n",
    "            self.textData.getVocabularySize(),\n",
    "            self.textData.getVocabularySize(),  # Both encoder and decoder have the same number of class\n",
    "            embedding_size=self.args.embeddingSize,  # Dimension of each word\n",
    "            output_projection=outputProjection.getWeights() if outputProjection else None,\n",
    "            feed_previous=bool(self.args.test)  # When we test (self.args.test), we use previous output as next input (feed_previous)\n",
    "        )\n",
    "\n",
    "        # For testing only\n",
    "        if self.args.test:\n",
    "            if not outputProjection:\n",
    "                self.outputs = decoderOutputs\n",
    "            else:\n",
    "                self.outputs = [outputProjection(output) for output in decoderOutputs]\n",
    "            \n",
    "            # TODO: Attach a summary to visualize the output\n",
    "\n",
    "        # For training only\n",
    "        else:\n",
    "            # Finally, we define the loss function\n",
    "            self.lossFct = tf.nn.seq2seq.sequence_loss(\n",
    "                decoderOutputs,\n",
    "                self.decoderTargets,\n",
    "                self.decoderWeights,\n",
    "                self.textData.getVocabularySize(),\n",
    "                softmax_loss_function= sampledSoftmax if outputProjection else None  # If None, use default SoftMax\n",
    "            )\n",
    "            tf.scalar_summary('loss', self.lossFct)  # Keep track of the cost\n",
    "\n",
    "            # Initialize the optimizer\n",
    "            opt = tf.train.AdamOptimizer(\n",
    "                learning_rate=self.args.learningRate,\n",
    "                beta1=0.9,\n",
    "                beta2=0.999,\n",
    "                epsilon=1e-08\n",
    "            )\n",
    "            self.optOp = opt.minimize(self.lossFct)\n",
    "\n",
    "    def step(self, batch):\n",
    "        \"\"\" Forward/training step operation.\n",
    "        Does not perform run on itself but just return the operators to do so. Those have then to be run\n",
    "        Args:\n",
    "            batch (Batch): Input data on testing mode, input and target on output mode\n",
    "        Return:\n",
    "            (ops), dict: A tuple of the (training, loss) operators or (outputs,) in testing mode with the associated feed dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        # Feed the dictionary\n",
    "        feedDict = {}\n",
    "        ops = None\n",
    "\n",
    "        if not self.args.test:  # Training\n",
    "            for i in range(self.args.maxLengthEnco):\n",
    "                feedDict[self.encoderInputs[i]]  = batch.encoderSeqs[i]\n",
    "            for i in range(self.args.maxLengthDeco):\n",
    "                feedDict[self.decoderInputs[i]]  = batch.decoderSeqs[i]\n",
    "                feedDict[self.decoderTargets[i]] = batch.targetSeqs[i]\n",
    "                feedDict[self.decoderWeights[i]] = batch.weights[i]\n",
    "\n",
    "            ops = (self.optOp, self.lossFct)\n",
    "        else:  # Testing (batchSize == 1)\n",
    "            for i in range(self.args.maxLengthEnco):\n",
    "                feedDict[self.encoderInputs[i]]  = batch.encoderSeqs[i]\n",
    "            feedDict[self.decoderInputs[0]]  = [self.textData.goToken]\n",
    "\n",
    "            ops = (self.outputs,)\n",
    "\n",
    "        # Return one pass operator\n",
    "        return ops, feedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 Conchylicultor. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Main script. See README.md for more information\n",
    "Use python 3\n",
    "\"\"\"\n",
    "\n",
    "import argparse  # Command line parsing\n",
    "import configparser  # Saving the models parameters\n",
    "import datetime  # Chronometer\n",
    "import os  # Files management\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import tensorflow as tf\n",
    "\n",
    "#from chatbot.textdata import TextData\n",
    "#from chatbot.model import Model\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    \"\"\"\n",
    "    Main class which launch the training or testing mode\n",
    "    \"\"\"\n",
    "\n",
    "    class TestMode:\n",
    "        \"\"\" Simple structure representing the different testing modes\n",
    "        \"\"\"\n",
    "        ALL = 'all'\n",
    "        INTERACTIVE = 'interactive'  # The user can write his own questions\n",
    "        DAEMON = 'daemon'  # The chatbot runs on background and can regularly be called to predict something\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Model/dataset parameters\n",
    "        self.args = None\n",
    "\n",
    "        # Task specific object\n",
    "        self.textData = None  # Dataset\n",
    "        self.model = None  # Sequence to sequence model\n",
    "\n",
    "        # Tensorflow utilities for convenience saving/logging\n",
    "        self.writer = None\n",
    "        self.saver = None\n",
    "        self.modelDir = ''  # Where the model is saved\n",
    "        self.globStep = 0  # Represent the number of iteration for the current model\n",
    "\n",
    "        # TensorFlow main session (we keep track for the daemon)\n",
    "        self.sess = None\n",
    "\n",
    "        # Filename and directories constants\n",
    "        self.MODEL_DIR_BASE = 'save/model'\n",
    "        self.MODEL_NAME_BASE = 'model'\n",
    "        self.MODEL_EXT = '.ckpt'\n",
    "        self.CONFIG_FILENAME = 'params.ini'\n",
    "        self.CONFIG_VERSION = '0.3'\n",
    "        self.TEST_IN_NAME = 'data/test/samples.txt'\n",
    "        self.TEST_OUT_SUFFIX = '_predictions.txt'\n",
    "        self.SENTENCES_PREFIX = ['Q: ', 'A: ']\n",
    "\n",
    "    @staticmethod\n",
    "    def parseArgs(args):\n",
    "        \"\"\"\n",
    "        Parse the arguments from the given command line\n",
    "        Args:\n",
    "            args (list<str>): List of arguments to parse. If None, the default sys.argv will be parsed\n",
    "        \"\"\"\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "\n",
    "        # Global options\n",
    "        globalArgs = parser.add_argument_group('Global options')\n",
    "        globalArgs.add_argument('--test',\n",
    "                                nargs='?',\n",
    "                                choices=[Chatbot.TestMode.ALL, Chatbot.TestMode.INTERACTIVE, Chatbot.TestMode.DAEMON],\n",
    "                                const=Chatbot.TestMode.ALL, default=None,\n",
    "                                help='if present, launch the program try to answer all sentences from data/test/ with'\n",
    "                                     ' the defined model(s), in interactive mode, the user can wrote his own sentences,'\n",
    "                                     ' use daemon mode to integrate the chatbot in another program')\n",
    "        globalArgs.add_argument('--createDataset', action='store_true', help='if present, the program will only generate the dataset from the corpus (no training/testing)')\n",
    "        globalArgs.add_argument('--playDataset', type=int, nargs='?', const=10, default=None,  help='if set, the program  will randomly play some samples(can be use conjointly with createDataset if this is the only action you want to perform)')\n",
    "        globalArgs.add_argument('--reset', action='store_true', help='use this if you want to ignore the previous model present on the model directory (Warning: the model will be destroyed with all the folder content)')\n",
    "        globalArgs.add_argument('--verbose', action='store_true', help='When testing, will plot the outputs at the same time they are computed')\n",
    "        globalArgs.add_argument('--keepAll', action='store_true', help='If this option is set, all saved model will be keep (Warning: make sure you have enough free disk space or increase saveEvery)')  # TODO: Add an option to delimit the max size\n",
    "        globalArgs.add_argument('--modelTag', type=str, default=None, help='tag to differentiate which model to store/load')\n",
    "        globalArgs.add_argument('--rootDir', type=str, default=None, help='folder where to look for the models and data')\n",
    "        globalArgs.add_argument('--watsonMode', action='store_true', help='Inverse the questions and answer when training (the network try to guess the question)')\n",
    "        globalArgs.add_argument('--device', type=str, default=None, help='\\'gpu\\' or \\'cpu\\' (Warning: make sure you have enough free RAM), allow to choose on which hardware run the model')\n",
    "        globalArgs.add_argument('--seed', type=int, default=None, help='random seed for replication')\n",
    "\n",
    "        # Dataset options\n",
    "        datasetArgs = parser.add_argument_group('Dataset options')\n",
    "        datasetArgs.add_argument('--corpus', type=str, default='cornell', help='corpus on which extract the dataset. Only one corpus available right now (Cornell)')\n",
    "        datasetArgs.add_argument('--datasetTag', type=str, default=None, help='add a tag to the dataset (file where to load the vocabulary and the precomputed samples, not the original corpus). Useful to manage multiple versions')  # The samples are computed from the corpus if it does not exist already. There are saved in \\'data/samples/\\'\n",
    "        datasetArgs.add_argument('--ratioDataset', type=float, default=1.0, help='ratio of dataset used to avoid using the whole dataset')  # Not implemented, useless ?\n",
    "        datasetArgs.add_argument('--maxLength', type=int, default=10, help='maximum length of the sentence (for input and output), define number of maximum step of the RNN')\n",
    "\n",
    "        # Network options (Warning: if modifying something here, also make the change on save/loadParams() )\n",
    "        nnArgs = parser.add_argument_group('Network options', 'architecture related option')\n",
    "        nnArgs.add_argument('--hiddenSize', type=int, default=256, help='number of hidden units in each RNN cell')\n",
    "        nnArgs.add_argument('--numLayers', type=int, default=2, help='number of rnn layers')\n",
    "        nnArgs.add_argument('--embeddingSize', type=int, default=32, help='embedding size of the word representation')\n",
    "        nnArgs.add_argument('--softmaxSamples', type=int, default=0, help='Number of samples in the sampled softmax loss function. A value of 0 deactivates sampled softmax')\n",
    "        \n",
    "        # Training options\n",
    "        trainingArgs = parser.add_argument_group('Training options')\n",
    "        trainingArgs.add_argument('--numEpochs', type=int, default=30, help='maximum number of epochs to run')\n",
    "        trainingArgs.add_argument('--saveEvery', type=int, default=1000, help='nb of mini-batch step before creating a model checkpoint')\n",
    "        trainingArgs.add_argument('--batchSize', type=int, default=10, help='mini-batch size')\n",
    "        trainingArgs.add_argument('--learningRate', type=float, default=0.001, help='Learning rate')\n",
    "\n",
    "        return parser.parse_args(args)\n",
    "\n",
    "    def main(self, args=None):\n",
    "        \"\"\"\n",
    "        Launch the training and/or the interactive mode\n",
    "        \"\"\"\n",
    "        print('Welcome to DeepQA v0.1 !')\n",
    "        print()\n",
    "        print('TensorFlow detected: v{}'.format(tf.__version__))\n",
    "\n",
    "        # General initialisation\n",
    "\n",
    "        self.args = self.parseArgs(args)\n",
    "\n",
    "        if not self.args.rootDir:\n",
    "            self.args.rootDir = os.getcwd()  # Use the current working directory\n",
    "\n",
    "        #tf.logging.set_verbosity(tf.logging.INFO) # DEBUG, INFO, WARN (default), ERROR, or FATAL\n",
    "\n",
    "        self.loadModelParams()  # Update the self.modelDir and self.globStep, for now, not used when loading Model (but need to be called before _getSummaryName)\n",
    "\n",
    "        self.textData = TextData(self.args)\n",
    "        # TODO: Add a mode where we can force the input of the decoder // Try to visualize the predictions for\n",
    "        # each word of the vocabulary / decoder input\n",
    "        # TODO: For now, the model are trained for a specific dataset (because of the maxLength which define the\n",
    "        # vocabulary). Add a compatibility mode which allow to launch a model trained on a different vocabulary (\n",
    "        # remap the word2id/id2word variables).\n",
    "        if self.args.createDataset:\n",
    "            print('Dataset created! Thanks for using this program')\n",
    "            return  # No need to go further\n",
    "\n",
    "        with tf.device(self.getDevice()):\n",
    "            self.model = Model(self.args, self.textData)\n",
    "\n",
    "        # Saver/summaries\n",
    "        self.writer = tf.train.SummaryWriter(self._getSummaryName())\n",
    "        self.saver = tf.train.Saver(max_to_keep=200)  # Arbitrary limit ?\n",
    "\n",
    "        # TODO: Fixed seed (WARNING: If dataset shuffling, make sure to do that after saving the\n",
    "        # dataset, otherwise, all which cames after the shuffling won't be replicable when\n",
    "        # reloading the dataset). How to restore the seed after loading ??\n",
    "        # Also fix seed for random.shuffle (does it works globally for all files ?)\n",
    "\n",
    "        # Running session\n",
    "\n",
    "        self.sess = tf.Session()  # TODO: Replace all sess by self.sess (not necessary a good idea) ?\n",
    "\n",
    "        print('Initialize variables...')\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # Reload the model eventually (if it exist.), on testing mode, the models are not loaded here (but in predictTestset)\n",
    "        if self.args.test != Chatbot.TestMode.ALL:\n",
    "            self.managePreviousModel(self.sess)\n",
    "\n",
    "        if self.args.test:\n",
    "            # TODO: For testing, add a mode where instead taking the most likely output after the <go> token,\n",
    "            # takes the second or third so it generates new sentences for the same input. Difficult to implement,\n",
    "            # probably have to modify the TensorFlow source code\n",
    "            if self.args.test == Chatbot.TestMode.INTERACTIVE:\n",
    "                self.mainTestInteractive(self.sess)\n",
    "            elif self.args.test == Chatbot.TestMode.ALL:\n",
    "                print('Start predicting...')\n",
    "                self.predictTestset(self.sess)\n",
    "                print('All predictions done')\n",
    "            elif self.args.test == Chatbot.TestMode.DAEMON:\n",
    "                print('Daemon mode, running in background...')\n",
    "            else:\n",
    "                raise RuntimeError('Unknown test mode: {}'.format(self.args.test))  # Should never happen\n",
    "        else:\n",
    "            self.mainTrain(self.sess)\n",
    "\n",
    "        if self.args.test != Chatbot.TestMode.DAEMON:\n",
    "            self.sess.close()\n",
    "            print(\"The End! Thanks for using this program\")\n",
    "\n",
    "    def mainTrain(self, sess):\n",
    "        \"\"\" Training loop\n",
    "        Args:\n",
    "            sess: The current running session\n",
    "        \"\"\"\n",
    "\n",
    "        # Specific training dependent loading\n",
    "\n",
    "        self.textData.makeLighter(self.args.ratioDataset)  # Limit the number of training samples\n",
    "\n",
    "        mergedSummaries = tf.merge_all_summaries()  # Define the summary operator (Warning: Won't appear on the tensorboard graph)\n",
    "        if self.globStep == 0:  # Not restoring from previous run\n",
    "            self.writer.add_graph(sess.graph)  # First time only\n",
    "\n",
    "        # If restoring a model, restore the progression bar ? and current batch ?\n",
    "\n",
    "        print('Start training (press Ctrl+C to save and exit)...')\n",
    "\n",
    "        try:  # If the user exit while training, we still try to save the model\n",
    "            for e in range(self.args.numEpochs):\n",
    "\n",
    "                print()\n",
    "                print(\"----- Epoch {}/{} ; (lr={}) -----\".format(e+1, self.args.numEpochs, self.args.learningRate))\n",
    "\n",
    "                batches = self.textData.getBatches()\n",
    "\n",
    "                # TODO: Also update learning parameters eventually\n",
    "\n",
    "                tic = datetime.datetime.now()\n",
    "                for nextBatch in tqdm(batches, desc=\"Training\"):\n",
    "                    # Training pass\n",
    "                    ops, feedDict = self.model.step(nextBatch)\n",
    "                    assert len(ops) == 2  # training, loss\n",
    "                    _, loss, summary = sess.run(ops + (mergedSummaries,), feedDict)\n",
    "                    self.writer.add_summary(summary, self.globStep)\n",
    "                    self.globStep += 1\n",
    "\n",
    "                    # Checkpoint\n",
    "                    if self.globStep % self.args.saveEvery == 0:\n",
    "                        self._saveSession(sess)\n",
    "\n",
    "                toc = datetime.datetime.now()\n",
    "\n",
    "                print(\"Epoch finished in {}\".format(toc-tic))  # Warning: Will overflow if an epoch takes more than 24 hours, and the output isn't really nicer\n",
    "        except (KeyboardInterrupt, SystemExit):  # If the user press Ctrl+C while testing progress\n",
    "            print('Interruption detected, exiting the program...')\n",
    "\n",
    "        self._saveSession(sess)  # Ultimate saving before complete exit\n",
    "\n",
    "    def predictTestset(self, sess):\n",
    "        \"\"\" Try predicting the sentences from the samples.txt file.\n",
    "        The sentences are saved on the modelDir under the same name\n",
    "        Args:\n",
    "            sess: The current running session\n",
    "        \"\"\"\n",
    "\n",
    "        # Loading the file to predict\n",
    "        with open(os.path.join(self.args.rootDir, self.TEST_IN_NAME), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        modelList = self._getModelList()\n",
    "        if not modelList:\n",
    "            print('Warning: No model found in \\'{}\\'. Please train a model before trying to predict'.format(self.modelDir))\n",
    "            return\n",
    "\n",
    "        # Predicting for each model present in modelDir\n",
    "        for modelName in sorted(modelList):  # TODO: Natural sorting\n",
    "            print('Restoring previous model from {}'.format(modelName))\n",
    "            self.saver.restore(sess, modelName)\n",
    "            print('Testing...')\n",
    "\n",
    "            saveName = modelName[:-len(self.MODEL_EXT)] + self.TEST_OUT_SUFFIX  # We remove the model extension and add the prediction suffix\n",
    "            with open(saveName, 'w') as f:\n",
    "                nbIgnored = 0\n",
    "                for line in tqdm(lines, desc='Sentences'):\n",
    "                    question = line[:-1]  # Remove the endl character\n",
    "\n",
    "                    answer = self.singlePredict(question)\n",
    "                    if not answer:\n",
    "                        nbIgnored += 1\n",
    "                        continue  # Back to the beginning, try again\n",
    "\n",
    "                    predString = '{x[0]}{0}\\n{x[1]}{1}\\n\\n'.format(question, self.textData.sequence2str(answer, clean=True), x=self.SENTENCES_PREFIX)\n",
    "                    if self.args.verbose:\n",
    "                        tqdm.write(predString)\n",
    "                    f.write(predString)\n",
    "                print('Prediction finished, {}/{} sentences ignored (too long)'.format(nbIgnored, len(lines)))\n",
    "\n",
    "    def mainTestInteractive(self, sess):\n",
    "        \"\"\" Try predicting the sentences that the user will enter in the console\n",
    "        Args:\n",
    "            sess: The current running session\n",
    "        \"\"\"\n",
    "        # TODO: If verbose mode, also show similar sentences from the training set with the same words (include in mainTest also)\n",
    "        # TODO: Also show the top 10 most likely predictions for each predicted output (when verbose mode)\n",
    "        # TODO: Log the questions asked for latter re-use (merge with test/samples.txt)\n",
    "\n",
    "        print('Testing: Launch interactive mode:')\n",
    "        print('')\n",
    "        print('Welcome to the interactive mode, here you can ask to Deep Q&A the sentence you want. Don\\'t have high '\n",
    "              'expectation. Type \\'exit\\' or just press ENTER to quit the program. Have fun.')\n",
    "\n",
    "        while True:\n",
    "            question = input(self.SENTENCES_PREFIX[0])\n",
    "            if question == '' or question == 'exit':\n",
    "                break\n",
    "\n",
    "            questionSeq = []  # Will be contain the question as seen by the encoder\n",
    "            answer = self.singlePredict(question, questionSeq)\n",
    "            if not answer:\n",
    "                print('Warning: sentence too long, sorry. Maybe try a simpler sentence.')\n",
    "                continue  # Back to the beginning, try again\n",
    "\n",
    "            print('{}{}'.format(self.SENTENCES_PREFIX[1], self.textData.sequence2str(answer, clean=True)))\n",
    "\n",
    "            if self.args.verbose:\n",
    "                print(self.textData.batchSeq2str(questionSeq, clean=True, reverse=True))\n",
    "                print(self.textData.sequence2str(answer))\n",
    "\n",
    "            print()\n",
    "\n",
    "    def singlePredict(self, question, questionSeq=None):\n",
    "        \"\"\" Predict the sentence\n",
    "        Args:\n",
    "            question (str): the raw input sentence\n",
    "            questionSeq (List<int>): output argument. If given will contain the input batch sequence\n",
    "        Return:\n",
    "            list <int>: the word ids corresponding to the answer\n",
    "        \"\"\"\n",
    "        # Create the input batch\n",
    "        batch = self.textData.sentence2enco(question)\n",
    "        if not batch:\n",
    "            return None\n",
    "        if questionSeq is not None:  # If the caller want to have the real input\n",
    "            questionSeq.extend(batch.encoderSeqs)\n",
    "\n",
    "        # Run the model\n",
    "        ops, feedDict = self.model.step(batch)\n",
    "        output = self.sess.run(ops[0], feedDict)  # TODO: Summarize the output too (histogram, ...)\n",
    "        answer = self.textData.deco2sentence(output)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def daemonPredict(self, sentence):\n",
    "        \"\"\" Return the answer to a given sentence (same as singlePredict() but with additional cleaning)\n",
    "        Args:\n",
    "            sentence (str): the raw input sentence\n",
    "        Return:\n",
    "            str: the human readable sentence\n",
    "        \"\"\"\n",
    "        return self.textData.sequence2str(\n",
    "            self.singlePredict(sentence),\n",
    "            clean=True\n",
    "        )\n",
    "\n",
    "    def daemonClose(self):\n",
    "        \"\"\" A utility function to close the daemon when finish\n",
    "        \"\"\"\n",
    "        print('Exiting the daemon mode...')\n",
    "        self.sess.close()\n",
    "        print('Daemon closed.')\n",
    "\n",
    "    def managePreviousModel(self, sess):\n",
    "        \"\"\" Restore or reset the model, depending of the parameters\n",
    "        If the destination directory already contains some file, it will handle the conflict as following:\n",
    "         * If --reset is set, all present files will be removed (warning: no confirmation is asked) and the training\n",
    "         restart from scratch (globStep & cie reinitialized)\n",
    "         * Otherwise, it will depend of the directory content. If the directory contains:\n",
    "           * No model files (only summary logs): works as a reset (restart from scratch)\n",
    "           * Other model files, but modelName not found (surely keepAll option changed): raise error, the user should\n",
    "           decide by himself what to do\n",
    "           * The right model file (eventually some other): no problem, simply resume the training\n",
    "        In any case, the directory will exist as it has been created by the summary writer\n",
    "        Args:\n",
    "            sess: The current running session\n",
    "        \"\"\"\n",
    "\n",
    "        print('WARNING: ', end='')\n",
    "\n",
    "        modelName = self._getModelName()\n",
    "\n",
    "        if os.listdir(self.modelDir):\n",
    "            if self.args.reset:\n",
    "                print('Reset: Destroying previous model at {}'.format(self.modelDir))\n",
    "            # Analysing directory content\n",
    "            elif os.path.exists(modelName):  # Restore the model\n",
    "                print('Restoring previous model from {}'.format(modelName))\n",
    "                self.saver.restore(sess, modelName)  # Will crash when --reset is not activated and the model has not been saved yet\n",
    "                print('Model restored.')\n",
    "            elif self._getModelList():\n",
    "                print('Conflict with previous models.')\n",
    "                raise RuntimeError('Some models are already present in \\'{}\\'. You should check them first (or re-try with the keepAll flag)'.format(self.modelDir))\n",
    "            else:  # No other model to conflict with (probably summary files)\n",
    "                print('No previous model found, but some files found at {}. Cleaning...'.format(self.modelDir))  # Warning: No confirmation asked\n",
    "                self.args.reset = True\n",
    "\n",
    "            if self.args.reset:\n",
    "                fileList = [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir)]\n",
    "                for f in fileList:\n",
    "                    print('Removing {}'.format(f))\n",
    "                    os.remove(f)\n",
    "\n",
    "        else:\n",
    "            print('No previous model found, starting from clean directory: {}'.format(self.modelDir))\n",
    "\n",
    "    def _saveSession(self, sess):\n",
    "        \"\"\" Save the model parameters and the variables\n",
    "        Args:\n",
    "            sess: the current session\n",
    "        \"\"\"\n",
    "        tqdm.write('Checkpoint reached: saving model (don\\'t stop the run)...')\n",
    "        self.saveModelParams()\n",
    "        self.saver.save(sess, self._getModelName())  # TODO: Put a limit size (ex: 3GB for the modelDir)\n",
    "        tqdm.write('Model saved.')\n",
    "\n",
    "    def _getModelList(self):\n",
    "        \"\"\" Return the list of the model files inside the model directory\n",
    "        \"\"\"\n",
    "        return [os.path.join(self.modelDir, f) for f in os.listdir(self.modelDir) if f.endswith(self.MODEL_EXT)]\n",
    "\n",
    "    def loadModelParams(self):\n",
    "        \"\"\" Load the some values associated with the current model, like the current globStep value\n",
    "        For now, this function does not need to be called before loading the model (no parameters restored). However,\n",
    "        the modelDir name will be initialized here so it is required to call this function before managePreviousModel(),\n",
    "        _getModelName() or _getSummaryName()\n",
    "        Warning: if you modify this function, make sure the changes mirror saveModelParams, also check if the parameters\n",
    "        should be reset in managePreviousModel\n",
    "        \"\"\"\n",
    "        # Compute the current model path\n",
    "        self.modelDir = os.path.join(self.args.rootDir, self.MODEL_DIR_BASE)\n",
    "        if self.args.modelTag:\n",
    "            self.modelDir += '-' + self.args.modelTag\n",
    "\n",
    "        # If there is a previous model, restore some parameters\n",
    "        configName = os.path.join(self.modelDir, self.CONFIG_FILENAME)\n",
    "        if not self.args.reset and not self.args.createDataset and os.path.exists(configName):\n",
    "            # Loading\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read(configName)\n",
    "\n",
    "            # Check the version\n",
    "            currentVersion = config['General'].get('version')\n",
    "            if currentVersion != self.CONFIG_VERSION:\n",
    "                raise UserWarning('Present configuration version {0} does not match {1}. You can try manual changes on \\'{2}\\''.format(currentVersion, self.CONFIG_VERSION, configName))\n",
    "\n",
    "            # Restoring the the parameters\n",
    "            self.globStep = config['General'].getint('globStep')\n",
    "            self.args.maxLength = config['General'].getint('maxLength')  # We need to restore the model length because of the textData associated and the vocabulary size (TODO: Compatibility mode between different maxLength)\n",
    "            self.args.watsonMode = config['General'].getboolean('watsonMode')\n",
    "            #self.args.datasetTag = config['General'].get('datasetTag')\n",
    "\n",
    "            self.args.hiddenSize = config['Network'].getint('hiddenSize')\n",
    "            self.args.numLayers = config['Network'].getint('numLayers')\n",
    "            self.args.embeddingSize = config['Network'].getint('embeddingSize')\n",
    "            self.args.softmaxSamples = config['Network'].getint('softmaxSamples')\n",
    "\n",
    "            # No restoring for training params, batch size or other non model dependent parameters\n",
    "\n",
    "            # Show the restored params\n",
    "            print()\n",
    "            print('Warning: Restoring parameters:')\n",
    "            print('globStep: {}'.format(self.globStep))\n",
    "            print('maxLength: {}'.format(self.args.maxLength))\n",
    "            print('watsonMode: {}'.format(self.args.watsonMode))\n",
    "            print('hiddenSize: {}'.format(self.args.hiddenSize))\n",
    "            print('numLayers: {}'.format(self.args.numLayers))\n",
    "            print('embeddingSize: {}'.format(self.args.embeddingSize))\n",
    "            print('softmaxSamples: {}'.format(self.args.softmaxSamples))\n",
    "            print()\n",
    "\n",
    "        # For now, not arbitrary  independent maxLength between encoder and decoder\n",
    "        self.args.maxLengthEnco = self.args.maxLength\n",
    "        self.args.maxLengthDeco = self.args.maxLength + 2\n",
    "\n",
    "        if self.args.watsonMode:\n",
    "            self.SENTENCES_PREFIX.reverse()\n",
    "\n",
    "\n",
    "    def saveModelParams(self):\n",
    "        \"\"\" Save the params of the model, like the current globStep value\n",
    "        Warning: if you modify this function, make sure the changes mirror loadModelParams\n",
    "        \"\"\"\n",
    "        config = configparser.ConfigParser()\n",
    "        config['General'] = {}\n",
    "        config['General']['version']  = self.CONFIG_VERSION\n",
    "        config['General']['globStep']  = str(self.globStep)\n",
    "        config['General']['maxLength'] = str(self.args.maxLength)\n",
    "        config['General']['watsonMode'] = str(self.args.watsonMode)\n",
    "\n",
    "        config['Network'] = {}\n",
    "        config['Network']['hiddenSize'] = str(self.args.hiddenSize)\n",
    "        config['Network']['numLayers'] = str(self.args.numLayers)\n",
    "        config['Network']['embeddingSize'] = str(self.args.embeddingSize)\n",
    "        config['Network']['softmaxSamples'] = str(self.args.softmaxSamples)\n",
    "\n",
    "        # Keep track of the learning params (but without restoring them)\n",
    "        config['Training (won\\'t be restored)'] = {}\n",
    "        config['Training (won\\'t be restored)']['learningRate'] = str(self.args.learningRate)\n",
    "        config['Training (won\\'t be restored)']['batchSize'] = str(self.args.batchSize)\n",
    "\n",
    "        with open(os.path.join(self.modelDir, self.CONFIG_FILENAME), 'w') as configFile:\n",
    "            config.write(configFile)\n",
    "\n",
    "    def _getSummaryName(self):\n",
    "        \"\"\" Parse the argument to decide were to save the summary, at the same place that the model\n",
    "        The folder could already contain logs if we restore the training, those will be merged\n",
    "        Return:\n",
    "            str: The path and name of the summary\n",
    "        \"\"\"\n",
    "        return self.modelDir\n",
    "\n",
    "    def _getModelName(self):\n",
    "        \"\"\" Parse the argument to decide were to save/load the model\n",
    "        This function is called at each checkpoint and the first time the model is load. If keepAll option is set, the\n",
    "        globStep value will be included in the name.\n",
    "        Return:\n",
    "            str: The path and name were the model need to be saved\n",
    "        \"\"\"\n",
    "        modelName = os.path.join(self.modelDir, self.MODEL_NAME_BASE)\n",
    "        if self.args.keepAll:  # We do not erase the previously saved model by including the current step on the name\n",
    "            modelName += '-' + str(self.globStep)\n",
    "        return modelName + self.MODEL_EXT\n",
    "\n",
    "    def getDevice(self):\n",
    "        \"\"\" Parse the argument to decide on which device run the model\n",
    "        Return:\n",
    "            str: The name of the device on which run the program\n",
    "        \"\"\"\n",
    "        if self.args.device == 'cpu':\n",
    "            return '/cpu:0'\n",
    "        elif self.args.device == 'gpu':\n",
    "            return '/gpu:0'\n",
    "        elif self.args.device is None:  # No specified device (default)\n",
    "            return None\n",
    "        else:\n",
    "            print('Warning: Error in the device name: {}, use the default device'.format(self.args.device))\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
